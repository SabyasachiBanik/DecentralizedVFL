{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Processed Feature set"
      ],
      "metadata": {
        "id": "608ArUoG2Ep8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to load and preprocess the Titanic data\n",
        "def load_and_preprocess_titanic_data():\n",
        "    def _bin_age(age_series):\n",
        "        bins = [-np.inf, 10, 40, np.inf]\n",
        "        labels = [\"Child\", \"Adult\", \"Elderly\"]\n",
        "        return pd.cut(age_series, bins=bins, labels=labels, right=True).astype(str).replace(\"nan\", \"Unknown\")\n",
        "\n",
        "    def _extract_title(name_series):\n",
        "        titles = name_series.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
        "        rare_titles = {\n",
        "            \"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"\n",
        "        }\n",
        "        titles = titles.replace(list(rare_titles), \"Rare\")\n",
        "        titles = titles.replace({\"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\"})\n",
        "        return titles\n",
        "\n",
        "    def _create_features(df):\n",
        "        df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\")\n",
        "        df[\"Age\"] = _bin_age(df[\"Age\"])\n",
        "        df[\"Cabin\"] = df[\"Cabin\"].str[0].fillna(\"Unknown\")\n",
        "        df[\"Title\"] = _extract_title(df[\"Name\"])\n",
        "        df.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"], inplace=True)\n",
        "        return df\n",
        "\n",
        "    df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "    print(\"Original features:\")\n",
        "    print(df.columns.tolist())\n",
        "    processed_df = df.dropna(subset=[\"Embarked\", \"Fare\"]).copy()\n",
        "    processed_df = _create_features(processed_df)\n",
        "    print(\"Processed features:\")\n",
        "    print(processed_df.columns.tolist())\n",
        "    return processed_df\n",
        "\n",
        "# Loading and preprocessing the data\n",
        "processed_df = load_and_preprocess_titanic_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7yrea6atlAw",
        "outputId": "f3b61edc-a72f-42c1-ed7f-7fb6e847c65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original features:\n",
            "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
            "Processed features:\n",
            "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked', 'Title']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vertical Split after one-hot encoding"
      ],
      "metadata": {
        "id": "bWA2H4br4Y4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "seed = 55\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def load_titanic_data():\n",
        "    def _bin_age(age_series):\n",
        "        bins = [-np.inf, 10, 40, np.inf]\n",
        "        labels = [\"Child\", \"Adult\", \"Elderly\"]\n",
        "        return pd.cut(age_series, bins=bins, labels=labels, right=True).astype(str).replace(\"nan\", \"Unknown\")\n",
        "\n",
        "    def _extract_title(name_series):\n",
        "        titles = name_series.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
        "        rare_titles = {\n",
        "            \"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"\n",
        "        }\n",
        "        titles = titles.replace(list(rare_titles), \"Rare\")\n",
        "        titles = titles.replace({\"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\"})\n",
        "        return titles\n",
        "\n",
        "    def _create_features(df):\n",
        "        df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\")\n",
        "        df[\"Age\"] = _bin_age(df[\"Age\"])\n",
        "        df[\"Cabin\"] = df[\"Cabin\"].str[0].fillna(\"Unknown\")\n",
        "        df[\"Title\"] = _extract_title(df[\"Name\"])\n",
        "        df.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"], inplace=True)\n",
        "        return df\n",
        "\n",
        "    def vertical_partition_rotating(df, num_participants):\n",
        "        partitions = [[] for _ in range(num_participants)]\n",
        "        num_features = df.shape[1]\n",
        "\n",
        "        #  Round-Robin Vertical distribution of features after one-hot encoding\n",
        "        for i, feature in enumerate(df.columns):\n",
        "            participant = i % num_participants\n",
        "            partitions[participant].append(feature)\n",
        "\n",
        "        # Final feature set for each client after one-hot encoding\n",
        "        for i, partition in enumerate(partitions):\n",
        "            print(f\"Client {i + 1} Final Features: {partition}\")\n",
        "\n",
        "        partitioned_dfs = [df[features] for features in partitions]\n",
        "        return partitioned_dfs\n",
        "\n",
        "    def get_partitions_and_label():\n",
        "        df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "        processed_df = df.dropna(subset=[\"Embarked\", \"Fare\"]).copy()\n",
        "        processed_df = _create_features(processed_df)\n",
        "        processed_df = pd.get_dummies(processed_df)  # One-hot encode the entire dataset before splitting\n",
        "        labels = processed_df[\"Survived\"].values\n",
        "\n",
        "        # Train and test data split\n",
        "        train_df, test_df, y_train, y_test = train_test_split(processed_df, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "        # vertical_partitioning_rotating function call\n",
        "        train_partitions = vertical_partition_rotating(train_df.drop(columns=[\"Survived\"]), 3)\n",
        "        test_partitions = vertical_partition_rotating(test_df.drop(columns=[\"Survived\"]), 3)\n",
        "\n",
        "        # We need to ensuring each participant has access to labels\n",
        "        for i in range(len(train_partitions)):\n",
        "            train_partitions[i]['Survived'] = y_train\n",
        "\n",
        "        for i in range(len(test_partitions)):\n",
        "            test_partitions[i]['Survived'] = y_test\n",
        "\n",
        "        return train_partitions, test_partitions, y_train, y_test\n",
        "\n",
        "    # Partitions and Labels\n",
        "    train_partitions, test_partitions, y_train, y_test = get_partitions_and_label()\n",
        "\n",
        "    # Partitions to PyTorch datasets conversions\n",
        "    def create_tensor_datasets(partitions):\n",
        "        tensor_partitions = []\n",
        "        for partition in partitions:\n",
        "            partition = partition.apply(pd.to_numeric, errors='coerce')\n",
        "            partition = partition.fillna(0)\n",
        "\n",
        "            for col in partition.select_dtypes(include=['bool']).columns:\n",
        "                partition[col] = partition[col].astype(int)\n",
        "\n",
        "            features = partition.drop(columns=[\"Survived\"]).values\n",
        "            labels = partition[\"Survived\"].values.astype(np.int64)\n",
        "\n",
        "            tensor_partition = TensorDataset(torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long))\n",
        "            tensor_partitions.append(tensor_partition)\n",
        "        return tensor_partitions\n",
        "\n",
        "    train_tensor_partitions = create_tensor_datasets(train_partitions)\n",
        "    test_tensor_partitions = create_tensor_datasets(test_partitions)\n",
        "\n",
        "    return train_tensor_partitions, test_tensor_partitions, train_partitions, y_train, y_test\n",
        "\n",
        "# Global Model definition\n",
        "class GlobalModel(nn.Module):\n",
        "    def __init__(self, input_sizes, hidden_sizes, output_size):\n",
        "        super(GlobalModel, self).__init__()\n",
        "        self.segments = nn.ModuleList()\n",
        "        for input_size, hidden_size in zip(input_sizes, hidden_sizes):\n",
        "            layers = [nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, output_size)]\n",
        "            self.segments.append(nn.Sequential(*layers))\n",
        "\n",
        "    def forward(self, x, active_segments):\n",
        "        segment_outputs = []\n",
        "        start_index = 0\n",
        "        for i, segment in enumerate(self.segments):\n",
        "            end_index = start_index + input_sizes[i]\n",
        "            if i in active_segments:\n",
        "                segment_input = x[:, start_index:end_index] # We extract the segment of the input corresponding to the active participant's features only\n",
        "                segment_output = segment(segment_input)\n",
        "                segment_outputs.append(segment_output)\n",
        "            else:\n",
        "                segment_outputs.append(torch.zeros(x.size(0), output_size, device=x.device)) # Non-active segments are appended with zero\n",
        "            start_index = end_index\n",
        "        combined_output = torch.mean(torch.stack(segment_outputs), dim=0)\n",
        "        return combined_output\n",
        "\n",
        "# Dataset\n",
        "train_tensor_partitions, test_tensor_partitions, feature_partitions, y_train, y_test = load_titanic_data()\n",
        "\n",
        "# Input sizes after one-hot encoding\n",
        "input_sizes = [partition.shape[1] - 1 for partition in feature_partitions]  # minus 1 for 'Survived' column\n",
        "print(\"Input sizes:\", input_sizes)\n",
        "hidden_sizes = [10] * 3\n",
        "output_size = 2 # survive or dead?\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "models = [GlobalModel(input_sizes, hidden_sizes, output_size).to(device) for _ in range(3)]\n",
        "optimizers = [optim.Adam(model.parameters(), lr=0.001) for model in models]\n",
        "\n",
        "# Class weights due to imbalance class distribution\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Training function\n",
        "def train(model, device, train_loader, optimizer, epoch, input_sizes, participant_id):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        data = data.view(data.size(0), -1)\n",
        "        # Zero-padding non-responsible features\n",
        "        padded_data = torch.zeros(data.size(0), sum(input_sizes)).to(device)\n",
        "        start_index = sum(input_sizes[:participant_id])\n",
        "        end_index = start_index + input_sizes[participant_id]\n",
        "        padded_data[:, start_index:end_index] = data\n",
        "        optimizer.zero_grad()\n",
        "        output = model(padded_data, active_segments=[participant_id])\n",
        "        loss = nn.CrossEntropyLoss(weight=class_weights)(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #if batch_idx % 10 == 0:\n",
        "            #print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# Selective gradient exchange function\n",
        "def selective_exchange_gradients(models, input_sizes, hidden_sizes):\n",
        "    num_models = len(models)\n",
        "    param_indices = [0]\n",
        "    cumulative_index = 0\n",
        "    for i in range(len(hidden_sizes)):\n",
        "        cumulative_index += (input_sizes[i] * hidden_sizes[i]) + hidden_sizes[i]\n",
        "        param_indices.append(cumulative_index)\n",
        "\n",
        "    for seg in range(len(hidden_sizes)):\n",
        "        start = param_indices[seg]\n",
        "        end = param_indices[seg + 1]\n",
        "        for param_idx in range(start, end):\n",
        "            grads = []\n",
        "            for model in models:\n",
        "                model_params = list(model.parameters())\n",
        "                if param_idx < len(model_params) and model_params[param_idx].grad is not None:\n",
        "                    grads.append(model_params[param_idx].grad)\n",
        "            if grads:\n",
        "                avg_grad = torch.stack(grads).mean(dim=0)\n",
        "                for model in models:\n",
        "                    model_params = list(model.parameters())\n",
        "                    if param_idx < len(model_params):\n",
        "                        model_params[param_idx].grad = avg_grad.clone()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Model Training\n",
        "federated_rounds = 1000  # To match with their setup\n",
        "epochs_per_round = 1\n",
        "\n",
        "# Lists to store accuracy after each round\n",
        "vfl_accuracies = []\n",
        "\n",
        "def evaluate(models, device, test_loaders):\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        for batch_data in zip(*test_loaders):\n",
        "            data_list = []\n",
        "            target_list = []\n",
        "            for participant_id, (data, target) in enumerate(batch_data):\n",
        "                data_list.append(data)\n",
        "                target_list.append(target)\n",
        "\n",
        "            target = target_list[0].to(device)\n",
        "            for t in target_list:\n",
        "                assert torch.equal(t, target), \"Targets are not consistent across participants\"\n",
        "\n",
        "            data_combined = torch.cat(data_list, dim=1).to(device)\n",
        "\n",
        "\n",
        "            #print(\"data_combined size:\", data_combined.size())\n",
        "            padded_data = torch.zeros(data_combined.size(0), sum(input_sizes)).to(device)\n",
        "            #print(\"padded_data size before padding:\", padded_data.size())\n",
        "\n",
        "            start_index = 0\n",
        "            for participant_id in range(len(input_sizes)):\n",
        "                end_index = start_index + input_sizes[participant_id]\n",
        "                if end_index <= data_combined.size(1):\n",
        "                    #print(f\"Padded data: start_index={start_index}, end_index={end_index}, data_combined[:, {start_index}:{end_index}].size()={data_combined[:, start_index:end_index].size()}\")\n",
        "                    padded_data[:, start_index:end_index] = data_combined[:, start_index:end_index]\n",
        "                else:\n",
        "\n",
        "                    adjusted_end_index = data_combined.size(1)\n",
        "                    #print(f\"Padded data: start_index={start_index}, adjusted_end_index={adjusted_end_index}, data_combined[:, {start_index}:{adjusted_end_index}].size()={data_combined[:, start_index:adjusted_end_index].size()}\")\n",
        "                    padded_data[:, start_index:adjusted_end_index] = data_combined[:, start_index:adjusted_end_index]\n",
        "                start_index = end_index\n",
        "\n",
        "            #print(\"padded_data size after padding:\", padded_data.size())\n",
        "\n",
        "            outputs = torch.zeros(data_combined.size(0), 2, device=device)\n",
        "            for model in models:\n",
        "                output = model(padded_data, active_segments=list(range(len(model.segments))))\n",
        "                outputs += output\n",
        "            outputs /= len(models)\n",
        "            test_loss += nn.CrossEntropyLoss(reduction='sum')(outputs, target).item()\n",
        "            pred = outputs.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            all_preds.extend(pred.view(-1).cpu().numpy())\n",
        "            all_targets.extend(target.view(-1).cpu().numpy())\n",
        "\n",
        "        test_loss /= len(test_loaders[0].dataset)\n",
        "        accuracy = 100. * correct / len(test_loaders[0].dataset)\n",
        "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loaders[0].dataset)} ({accuracy:.0f}%)')\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "\n",
        "for federated_round in range(federated_rounds):\n",
        "    print(f\"Federated Round {federated_round + 1}/{federated_rounds}\")\n",
        "    for participant_id in range(3):\n",
        "        #print(f\"  Training Participant {participant_id + 1}\")\n",
        "        train_loader = DataLoader(train_tensor_partitions[participant_id], batch_size=64, shuffle=True)\n",
        "        for epoch in range(1, epochs_per_round + 1):\n",
        "            train(models[participant_id], device, train_loader, optimizers[participant_id], epoch, input_sizes, participant_id)\n",
        "\n",
        "    selective_exchange_gradients(models, input_sizes, hidden_sizes)\n",
        "\n",
        "    # Accurayc after each round\n",
        "    test_loaders = [DataLoader(test_tensor_partitions[i], batch_size=32, shuffle=False) for i in range(3)]\n",
        "    accuracy = evaluate(models, device, test_loaders)\n",
        "    vfl_accuracies.append(accuracy)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# VFL Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(federated_rounds), vfl_accuracies, label='VFL Accuracy')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('VFL Accuracy over Rounds')\n",
        "plt.ylim([0, 100])\n",
        "plt.legend()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1dFYrHJF4fbU",
        "outputId": "c627a18d-d997-434b-92ac-55f73c0b59be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1 Final Features: ['Pclass', 'Fare', 'Age_Adult', 'Age_Unknown', 'Cabin_C', 'Cabin_F', 'Cabin_Unknown', 'Embarked_S', 'Title_Mr']\n",
            "Client 2 Final Features: ['SibSp', 'Sex_female', 'Age_Child', 'Cabin_A', 'Cabin_D', 'Cabin_G', 'Embarked_C', 'Title_Master', 'Title_Mrs']\n",
            "Client 3 Final Features: ['Parch', 'Sex_male', 'Age_Elderly', 'Cabin_B', 'Cabin_E', 'Cabin_T', 'Embarked_Q', 'Title_Miss', 'Title_Rare']\n",
            "Client 1 Final Features: ['Pclass', 'Fare', 'Age_Adult', 'Age_Unknown', 'Cabin_C', 'Cabin_F', 'Cabin_Unknown', 'Embarked_S', 'Title_Mr']\n",
            "Client 2 Final Features: ['SibSp', 'Sex_female', 'Age_Child', 'Cabin_A', 'Cabin_D', 'Cabin_G', 'Embarked_C', 'Title_Master', 'Title_Mrs']\n",
            "Client 3 Final Features: ['Parch', 'Sex_male', 'Age_Elderly', 'Cabin_B', 'Cabin_E', 'Cabin_T', 'Embarked_Q', 'Title_Miss', 'Title_Rare']\n",
            "Input sizes: [9, 9, 9]\n",
            "Federated Round 1/1000\n",
            "Test set: Average loss: 0.7218, Accuracy: 63/178 (35%)\n",
            "Federated Round 2/1000\n",
            "Test set: Average loss: 0.7134, Accuracy: 69/178 (39%)\n",
            "Federated Round 3/1000\n",
            "Test set: Average loss: 0.7100, Accuracy: 69/178 (39%)\n",
            "Federated Round 4/1000\n",
            "Test set: Average loss: 0.7074, Accuracy: 69/178 (39%)\n",
            "Federated Round 5/1000\n",
            "Test set: Average loss: 0.7035, Accuracy: 69/178 (39%)\n",
            "Federated Round 6/1000\n",
            "Test set: Average loss: 0.6993, Accuracy: 69/178 (39%)\n",
            "Federated Round 7/1000\n",
            "Test set: Average loss: 0.6951, Accuracy: 70/178 (39%)\n",
            "Federated Round 8/1000\n",
            "Test set: Average loss: 0.6915, Accuracy: 70/178 (39%)\n",
            "Federated Round 9/1000\n",
            "Test set: Average loss: 0.6886, Accuracy: 70/178 (39%)\n",
            "Federated Round 10/1000\n",
            "Test set: Average loss: 0.6856, Accuracy: 70/178 (39%)\n",
            "Federated Round 11/1000\n",
            "Test set: Average loss: 0.6831, Accuracy: 70/178 (39%)\n",
            "Federated Round 12/1000\n",
            "Test set: Average loss: 0.6793, Accuracy: 72/178 (40%)\n",
            "Federated Round 13/1000\n",
            "Test set: Average loss: 0.6762, Accuracy: 81/178 (46%)\n",
            "Federated Round 14/1000\n",
            "Test set: Average loss: 0.6731, Accuracy: 97/178 (54%)\n",
            "Federated Round 15/1000\n",
            "Test set: Average loss: 0.6695, Accuracy: 109/178 (61%)\n",
            "Federated Round 16/1000\n",
            "Test set: Average loss: 0.6667, Accuracy: 120/178 (67%)\n",
            "Federated Round 17/1000\n",
            "Test set: Average loss: 0.6635, Accuracy: 122/178 (69%)\n",
            "Federated Round 18/1000\n",
            "Test set: Average loss: 0.6596, Accuracy: 125/178 (70%)\n",
            "Federated Round 19/1000\n",
            "Test set: Average loss: 0.6540, Accuracy: 125/178 (70%)\n",
            "Federated Round 20/1000\n",
            "Test set: Average loss: 0.6485, Accuracy: 126/178 (71%)\n",
            "Federated Round 21/1000\n",
            "Test set: Average loss: 0.6436, Accuracy: 126/178 (71%)\n",
            "Federated Round 22/1000\n",
            "Test set: Average loss: 0.6389, Accuracy: 128/178 (72%)\n",
            "Federated Round 23/1000\n",
            "Test set: Average loss: 0.6331, Accuracy: 128/178 (72%)\n",
            "Federated Round 24/1000\n",
            "Test set: Average loss: 0.6275, Accuracy: 133/178 (75%)\n",
            "Federated Round 25/1000\n",
            "Test set: Average loss: 0.6228, Accuracy: 134/178 (75%)\n",
            "Federated Round 26/1000\n",
            "Test set: Average loss: 0.6169, Accuracy: 135/178 (76%)\n",
            "Federated Round 27/1000\n",
            "Test set: Average loss: 0.6110, Accuracy: 134/178 (75%)\n",
            "Federated Round 28/1000\n",
            "Test set: Average loss: 0.6064, Accuracy: 134/178 (75%)\n",
            "Federated Round 29/1000\n",
            "Test set: Average loss: 0.6015, Accuracy: 133/178 (75%)\n",
            "Federated Round 30/1000\n",
            "Test set: Average loss: 0.5988, Accuracy: 132/178 (74%)\n",
            "Federated Round 31/1000\n",
            "Test set: Average loss: 0.5961, Accuracy: 133/178 (75%)\n",
            "Federated Round 32/1000\n",
            "Test set: Average loss: 0.5923, Accuracy: 134/178 (75%)\n",
            "Federated Round 33/1000\n",
            "Test set: Average loss: 0.5877, Accuracy: 134/178 (75%)\n",
            "Federated Round 34/1000\n",
            "Test set: Average loss: 0.5851, Accuracy: 135/178 (76%)\n",
            "Federated Round 35/1000\n",
            "Test set: Average loss: 0.5798, Accuracy: 136/178 (76%)\n",
            "Federated Round 36/1000\n",
            "Test set: Average loss: 0.5788, Accuracy: 137/178 (77%)\n",
            "Federated Round 37/1000\n",
            "Test set: Average loss: 0.5758, Accuracy: 136/178 (76%)\n",
            "Federated Round 38/1000\n",
            "Test set: Average loss: 0.5717, Accuracy: 137/178 (77%)\n",
            "Federated Round 39/1000\n",
            "Test set: Average loss: 0.5727, Accuracy: 136/178 (76%)\n",
            "Federated Round 40/1000\n",
            "Test set: Average loss: 0.5708, Accuracy: 137/178 (77%)\n",
            "Federated Round 41/1000\n",
            "Test set: Average loss: 0.5660, Accuracy: 138/178 (78%)\n",
            "Federated Round 42/1000\n",
            "Test set: Average loss: 0.5617, Accuracy: 138/178 (78%)\n",
            "Federated Round 43/1000\n",
            "Test set: Average loss: 0.5602, Accuracy: 138/178 (78%)\n",
            "Federated Round 44/1000\n",
            "Test set: Average loss: 0.5589, Accuracy: 139/178 (78%)\n",
            "Federated Round 45/1000\n",
            "Test set: Average loss: 0.5548, Accuracy: 141/178 (79%)\n",
            "Federated Round 46/1000\n",
            "Test set: Average loss: 0.5506, Accuracy: 141/178 (79%)\n",
            "Federated Round 47/1000\n",
            "Test set: Average loss: 0.5459, Accuracy: 141/178 (79%)\n",
            "Federated Round 48/1000\n",
            "Test set: Average loss: 0.5455, Accuracy: 142/178 (80%)\n",
            "Federated Round 49/1000\n",
            "Test set: Average loss: 0.5466, Accuracy: 142/178 (80%)\n",
            "Federated Round 50/1000\n",
            "Test set: Average loss: 0.5435, Accuracy: 142/178 (80%)\n",
            "Federated Round 51/1000\n",
            "Test set: Average loss: 0.5395, Accuracy: 142/178 (80%)\n",
            "Federated Round 52/1000\n",
            "Test set: Average loss: 0.5370, Accuracy: 142/178 (80%)\n",
            "Federated Round 53/1000\n",
            "Test set: Average loss: 0.5347, Accuracy: 141/178 (79%)\n",
            "Federated Round 54/1000\n",
            "Test set: Average loss: 0.5307, Accuracy: 143/178 (80%)\n",
            "Federated Round 55/1000\n",
            "Test set: Average loss: 0.5318, Accuracy: 141/178 (79%)\n",
            "Federated Round 56/1000\n",
            "Test set: Average loss: 0.5275, Accuracy: 142/178 (80%)\n",
            "Federated Round 57/1000\n",
            "Test set: Average loss: 0.5269, Accuracy: 141/178 (79%)\n",
            "Federated Round 58/1000\n",
            "Test set: Average loss: 0.5269, Accuracy: 141/178 (79%)\n",
            "Federated Round 59/1000\n",
            "Test set: Average loss: 0.5256, Accuracy: 141/178 (79%)\n",
            "Federated Round 60/1000\n",
            "Test set: Average loss: 0.5232, Accuracy: 142/178 (80%)\n",
            "Federated Round 61/1000\n",
            "Test set: Average loss: 0.5213, Accuracy: 142/178 (80%)\n",
            "Federated Round 62/1000\n",
            "Test set: Average loss: 0.5223, Accuracy: 142/178 (80%)\n",
            "Federated Round 63/1000\n",
            "Test set: Average loss: 0.5200, Accuracy: 143/178 (80%)\n",
            "Federated Round 64/1000\n",
            "Test set: Average loss: 0.5172, Accuracy: 143/178 (80%)\n",
            "Federated Round 65/1000\n",
            "Test set: Average loss: 0.5160, Accuracy: 143/178 (80%)\n",
            "Federated Round 66/1000\n",
            "Test set: Average loss: 0.5172, Accuracy: 143/178 (80%)\n",
            "Federated Round 67/1000\n",
            "Test set: Average loss: 0.5164, Accuracy: 143/178 (80%)\n",
            "Federated Round 68/1000\n",
            "Test set: Average loss: 0.5159, Accuracy: 142/178 (80%)\n",
            "Federated Round 69/1000\n",
            "Test set: Average loss: 0.5130, Accuracy: 143/178 (80%)\n",
            "Federated Round 70/1000\n",
            "Test set: Average loss: 0.5085, Accuracy: 143/178 (80%)\n",
            "Federated Round 71/1000\n",
            "Test set: Average loss: 0.5100, Accuracy: 143/178 (80%)\n",
            "Federated Round 72/1000\n",
            "Test set: Average loss: 0.5111, Accuracy: 143/178 (80%)\n",
            "Federated Round 73/1000\n",
            "Test set: Average loss: 0.5093, Accuracy: 143/178 (80%)\n",
            "Federated Round 74/1000\n",
            "Test set: Average loss: 0.5081, Accuracy: 143/178 (80%)\n",
            "Federated Round 75/1000\n",
            "Test set: Average loss: 0.5057, Accuracy: 143/178 (80%)\n",
            "Federated Round 76/1000\n",
            "Test set: Average loss: 0.5074, Accuracy: 143/178 (80%)\n",
            "Federated Round 77/1000\n",
            "Test set: Average loss: 0.5042, Accuracy: 143/178 (80%)\n",
            "Federated Round 78/1000\n",
            "Test set: Average loss: 0.5037, Accuracy: 143/178 (80%)\n",
            "Federated Round 79/1000\n",
            "Test set: Average loss: 0.5019, Accuracy: 143/178 (80%)\n",
            "Federated Round 80/1000\n",
            "Test set: Average loss: 0.5022, Accuracy: 143/178 (80%)\n",
            "Federated Round 81/1000\n",
            "Test set: Average loss: 0.5017, Accuracy: 143/178 (80%)\n",
            "Federated Round 82/1000\n",
            "Test set: Average loss: 0.5029, Accuracy: 143/178 (80%)\n",
            "Federated Round 83/1000\n",
            "Test set: Average loss: 0.4972, Accuracy: 144/178 (81%)\n",
            "Federated Round 84/1000\n",
            "Test set: Average loss: 0.5009, Accuracy: 143/178 (80%)\n",
            "Federated Round 85/1000\n",
            "Test set: Average loss: 0.5043, Accuracy: 143/178 (80%)\n",
            "Federated Round 86/1000\n",
            "Test set: Average loss: 0.5040, Accuracy: 143/178 (80%)\n",
            "Federated Round 87/1000\n",
            "Test set: Average loss: 0.5004, Accuracy: 143/178 (80%)\n",
            "Federated Round 88/1000\n",
            "Test set: Average loss: 0.5016, Accuracy: 143/178 (80%)\n",
            "Federated Round 89/1000\n",
            "Test set: Average loss: 0.5016, Accuracy: 143/178 (80%)\n",
            "Federated Round 90/1000\n",
            "Test set: Average loss: 0.4988, Accuracy: 143/178 (80%)\n",
            "Federated Round 91/1000\n",
            "Test set: Average loss: 0.5021, Accuracy: 143/178 (80%)\n",
            "Federated Round 92/1000\n",
            "Test set: Average loss: 0.5024, Accuracy: 143/178 (80%)\n",
            "Federated Round 93/1000\n",
            "Test set: Average loss: 0.4972, Accuracy: 143/178 (80%)\n",
            "Federated Round 94/1000\n",
            "Test set: Average loss: 0.4967, Accuracy: 143/178 (80%)\n",
            "Federated Round 95/1000\n",
            "Test set: Average loss: 0.4977, Accuracy: 143/178 (80%)\n",
            "Federated Round 96/1000\n",
            "Test set: Average loss: 0.4976, Accuracy: 143/178 (80%)\n",
            "Federated Round 97/1000\n",
            "Test set: Average loss: 0.4951, Accuracy: 143/178 (80%)\n",
            "Federated Round 98/1000\n",
            "Test set: Average loss: 0.4985, Accuracy: 143/178 (80%)\n",
            "Federated Round 99/1000\n",
            "Test set: Average loss: 0.4955, Accuracy: 143/178 (80%)\n",
            "Federated Round 100/1000\n",
            "Test set: Average loss: 0.4936, Accuracy: 144/178 (81%)\n",
            "Federated Round 101/1000\n",
            "Test set: Average loss: 0.4957, Accuracy: 144/178 (81%)\n",
            "Federated Round 102/1000\n",
            "Test set: Average loss: 0.4987, Accuracy: 143/178 (80%)\n",
            "Federated Round 103/1000\n",
            "Test set: Average loss: 0.4956, Accuracy: 144/178 (81%)\n",
            "Federated Round 104/1000\n",
            "Test set: Average loss: 0.4976, Accuracy: 143/178 (80%)\n",
            "Federated Round 105/1000\n",
            "Test set: Average loss: 0.4940, Accuracy: 144/178 (81%)\n",
            "Federated Round 106/1000\n",
            "Test set: Average loss: 0.4955, Accuracy: 143/178 (80%)\n",
            "Federated Round 107/1000\n",
            "Test set: Average loss: 0.4927, Accuracy: 144/178 (81%)\n",
            "Federated Round 108/1000\n",
            "Test set: Average loss: 0.4924, Accuracy: 144/178 (81%)\n",
            "Federated Round 109/1000\n",
            "Test set: Average loss: 0.4932, Accuracy: 144/178 (81%)\n",
            "Federated Round 110/1000\n",
            "Test set: Average loss: 0.4924, Accuracy: 144/178 (81%)\n",
            "Federated Round 111/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 144/178 (81%)\n",
            "Federated Round 112/1000\n",
            "Test set: Average loss: 0.4907, Accuracy: 144/178 (81%)\n",
            "Federated Round 113/1000\n",
            "Test set: Average loss: 0.4898, Accuracy: 144/178 (81%)\n",
            "Federated Round 114/1000\n",
            "Test set: Average loss: 0.4942, Accuracy: 144/178 (81%)\n",
            "Federated Round 115/1000\n",
            "Test set: Average loss: 0.4900, Accuracy: 144/178 (81%)\n",
            "Federated Round 116/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 144/178 (81%)\n",
            "Federated Round 117/1000\n",
            "Test set: Average loss: 0.4899, Accuracy: 144/178 (81%)\n",
            "Federated Round 118/1000\n",
            "Test set: Average loss: 0.4940, Accuracy: 144/178 (81%)\n",
            "Federated Round 119/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 144/178 (81%)\n",
            "Federated Round 120/1000\n",
            "Test set: Average loss: 0.4941, Accuracy: 144/178 (81%)\n",
            "Federated Round 121/1000\n",
            "Test set: Average loss: 0.4910, Accuracy: 144/178 (81%)\n",
            "Federated Round 122/1000\n",
            "Test set: Average loss: 0.4895, Accuracy: 144/178 (81%)\n",
            "Federated Round 123/1000\n",
            "Test set: Average loss: 0.4954, Accuracy: 144/178 (81%)\n",
            "Federated Round 124/1000\n",
            "Test set: Average loss: 0.4965, Accuracy: 144/178 (81%)\n",
            "Federated Round 125/1000\n",
            "Test set: Average loss: 0.4923, Accuracy: 144/178 (81%)\n",
            "Federated Round 126/1000\n",
            "Test set: Average loss: 0.4946, Accuracy: 144/178 (81%)\n",
            "Federated Round 127/1000\n",
            "Test set: Average loss: 0.4950, Accuracy: 144/178 (81%)\n",
            "Federated Round 128/1000\n",
            "Test set: Average loss: 0.4944, Accuracy: 144/178 (81%)\n",
            "Federated Round 129/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 144/178 (81%)\n",
            "Federated Round 130/1000\n",
            "Test set: Average loss: 0.4954, Accuracy: 143/178 (80%)\n",
            "Federated Round 131/1000\n",
            "Test set: Average loss: 0.4899, Accuracy: 144/178 (81%)\n",
            "Federated Round 132/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 144/178 (81%)\n",
            "Federated Round 133/1000\n",
            "Test set: Average loss: 0.4961, Accuracy: 144/178 (81%)\n",
            "Federated Round 134/1000\n",
            "Test set: Average loss: 0.4958, Accuracy: 144/178 (81%)\n",
            "Federated Round 135/1000\n",
            "Test set: Average loss: 0.4944, Accuracy: 144/178 (81%)\n",
            "Federated Round 136/1000\n",
            "Test set: Average loss: 0.4928, Accuracy: 144/178 (81%)\n",
            "Federated Round 137/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 144/178 (81%)\n",
            "Federated Round 138/1000\n",
            "Test set: Average loss: 0.4915, Accuracy: 144/178 (81%)\n",
            "Federated Round 139/1000\n",
            "Test set: Average loss: 0.4924, Accuracy: 144/178 (81%)\n",
            "Federated Round 140/1000\n",
            "Test set: Average loss: 0.4894, Accuracy: 144/178 (81%)\n",
            "Federated Round 141/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 144/178 (81%)\n",
            "Federated Round 142/1000\n",
            "Test set: Average loss: 0.4929, Accuracy: 144/178 (81%)\n",
            "Federated Round 143/1000\n",
            "Test set: Average loss: 0.4878, Accuracy: 144/178 (81%)\n",
            "Federated Round 144/1000\n",
            "Test set: Average loss: 0.4860, Accuracy: 144/178 (81%)\n",
            "Federated Round 145/1000\n",
            "Test set: Average loss: 0.4949, Accuracy: 143/178 (80%)\n",
            "Federated Round 146/1000\n",
            "Test set: Average loss: 0.4851, Accuracy: 145/178 (81%)\n",
            "Federated Round 147/1000\n",
            "Test set: Average loss: 0.4918, Accuracy: 144/178 (81%)\n",
            "Federated Round 148/1000\n",
            "Test set: Average loss: 0.4928, Accuracy: 144/178 (81%)\n",
            "Federated Round 149/1000\n",
            "Test set: Average loss: 0.4901, Accuracy: 144/178 (81%)\n",
            "Federated Round 150/1000\n",
            "Test set: Average loss: 0.4875, Accuracy: 144/178 (81%)\n",
            "Federated Round 151/1000\n",
            "Test set: Average loss: 0.4894, Accuracy: 144/178 (81%)\n",
            "Federated Round 152/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 144/178 (81%)\n",
            "Federated Round 153/1000\n",
            "Test set: Average loss: 0.4923, Accuracy: 144/178 (81%)\n",
            "Federated Round 154/1000\n",
            "Test set: Average loss: 0.4907, Accuracy: 144/178 (81%)\n",
            "Federated Round 155/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 144/178 (81%)\n",
            "Federated Round 156/1000\n",
            "Test set: Average loss: 0.4894, Accuracy: 144/178 (81%)\n",
            "Federated Round 157/1000\n",
            "Test set: Average loss: 0.4922, Accuracy: 144/178 (81%)\n",
            "Federated Round 158/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 144/178 (81%)\n",
            "Federated Round 159/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 144/178 (81%)\n",
            "Federated Round 160/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 144/178 (81%)\n",
            "Federated Round 161/1000\n",
            "Test set: Average loss: 0.4894, Accuracy: 144/178 (81%)\n",
            "Federated Round 162/1000\n",
            "Test set: Average loss: 0.4862, Accuracy: 145/178 (81%)\n",
            "Federated Round 163/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 144/178 (81%)\n",
            "Federated Round 164/1000\n",
            "Test set: Average loss: 0.4939, Accuracy: 144/178 (81%)\n",
            "Federated Round 165/1000\n",
            "Test set: Average loss: 0.4890, Accuracy: 144/178 (81%)\n",
            "Federated Round 166/1000\n",
            "Test set: Average loss: 0.4890, Accuracy: 144/178 (81%)\n",
            "Federated Round 167/1000\n",
            "Test set: Average loss: 0.4914, Accuracy: 144/178 (81%)\n",
            "Federated Round 168/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 144/178 (81%)\n",
            "Federated Round 169/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 144/178 (81%)\n",
            "Federated Round 170/1000\n",
            "Test set: Average loss: 0.4938, Accuracy: 144/178 (81%)\n",
            "Federated Round 171/1000\n",
            "Test set: Average loss: 0.4933, Accuracy: 144/178 (81%)\n",
            "Federated Round 172/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 144/178 (81%)\n",
            "Federated Round 173/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 144/178 (81%)\n",
            "Federated Round 174/1000\n",
            "Test set: Average loss: 0.4875, Accuracy: 144/178 (81%)\n",
            "Federated Round 175/1000\n",
            "Test set: Average loss: 0.4898, Accuracy: 144/178 (81%)\n",
            "Federated Round 176/1000\n",
            "Test set: Average loss: 0.4940, Accuracy: 144/178 (81%)\n",
            "Federated Round 177/1000\n",
            "Test set: Average loss: 0.4869, Accuracy: 143/178 (80%)\n",
            "Federated Round 178/1000\n",
            "Test set: Average loss: 0.4877, Accuracy: 144/178 (81%)\n",
            "Federated Round 179/1000\n",
            "Test set: Average loss: 0.4900, Accuracy: 144/178 (81%)\n",
            "Federated Round 180/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 144/178 (81%)\n",
            "Federated Round 181/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 144/178 (81%)\n",
            "Federated Round 182/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 144/178 (81%)\n",
            "Federated Round 183/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 144/178 (81%)\n",
            "Federated Round 184/1000\n",
            "Test set: Average loss: 0.4882, Accuracy: 144/178 (81%)\n",
            "Federated Round 185/1000\n",
            "Test set: Average loss: 0.4917, Accuracy: 144/178 (81%)\n",
            "Federated Round 186/1000\n",
            "Test set: Average loss: 0.4864, Accuracy: 144/178 (81%)\n",
            "Federated Round 187/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 144/178 (81%)\n",
            "Federated Round 188/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 144/178 (81%)\n",
            "Federated Round 189/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 144/178 (81%)\n",
            "Federated Round 190/1000\n",
            "Test set: Average loss: 0.4868, Accuracy: 145/178 (81%)\n",
            "Federated Round 191/1000\n",
            "Test set: Average loss: 0.4945, Accuracy: 143/178 (80%)\n",
            "Federated Round 192/1000\n",
            "Test set: Average loss: 0.4865, Accuracy: 144/178 (81%)\n",
            "Federated Round 193/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 144/178 (81%)\n",
            "Federated Round 194/1000\n",
            "Test set: Average loss: 0.4926, Accuracy: 144/178 (81%)\n",
            "Federated Round 195/1000\n",
            "Test set: Average loss: 0.4905, Accuracy: 144/178 (81%)\n",
            "Federated Round 196/1000\n",
            "Test set: Average loss: 0.4866, Accuracy: 144/178 (81%)\n",
            "Federated Round 197/1000\n",
            "Test set: Average loss: 0.4899, Accuracy: 144/178 (81%)\n",
            "Federated Round 198/1000\n",
            "Test set: Average loss: 0.4880, Accuracy: 144/178 (81%)\n",
            "Federated Round 199/1000\n",
            "Test set: Average loss: 0.4904, Accuracy: 144/178 (81%)\n",
            "Federated Round 200/1000\n",
            "Test set: Average loss: 0.4903, Accuracy: 144/178 (81%)\n",
            "Federated Round 201/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 144/178 (81%)\n",
            "Federated Round 202/1000\n",
            "Test set: Average loss: 0.4844, Accuracy: 144/178 (81%)\n",
            "Federated Round 203/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 144/178 (81%)\n",
            "Federated Round 204/1000\n",
            "Test set: Average loss: 0.4863, Accuracy: 144/178 (81%)\n",
            "Federated Round 205/1000\n",
            "Test set: Average loss: 0.4832, Accuracy: 144/178 (81%)\n",
            "Federated Round 206/1000\n",
            "Test set: Average loss: 0.4862, Accuracy: 144/178 (81%)\n",
            "Federated Round 207/1000\n",
            "Test set: Average loss: 0.4873, Accuracy: 144/178 (81%)\n",
            "Federated Round 208/1000\n",
            "Test set: Average loss: 0.4886, Accuracy: 144/178 (81%)\n",
            "Federated Round 209/1000\n",
            "Test set: Average loss: 0.4864, Accuracy: 144/178 (81%)\n",
            "Federated Round 210/1000\n",
            "Test set: Average loss: 0.4873, Accuracy: 144/178 (81%)\n",
            "Federated Round 211/1000\n",
            "Test set: Average loss: 0.4879, Accuracy: 144/178 (81%)\n",
            "Federated Round 212/1000\n",
            "Test set: Average loss: 0.4903, Accuracy: 144/178 (81%)\n",
            "Federated Round 213/1000\n",
            "Test set: Average loss: 0.4909, Accuracy: 144/178 (81%)\n",
            "Federated Round 214/1000\n",
            "Test set: Average loss: 0.4882, Accuracy: 144/178 (81%)\n",
            "Federated Round 215/1000\n",
            "Test set: Average loss: 0.4905, Accuracy: 144/178 (81%)\n",
            "Federated Round 216/1000\n",
            "Test set: Average loss: 0.4856, Accuracy: 145/178 (81%)\n",
            "Federated Round 217/1000\n",
            "Test set: Average loss: 0.4859, Accuracy: 145/178 (81%)\n",
            "Federated Round 218/1000\n",
            "Test set: Average loss: 0.4859, Accuracy: 145/178 (81%)\n",
            "Federated Round 219/1000\n",
            "Test set: Average loss: 0.4977, Accuracy: 144/178 (81%)\n",
            "Federated Round 220/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 144/178 (81%)\n",
            "Federated Round 221/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 144/178 (81%)\n",
            "Federated Round 222/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 144/178 (81%)\n",
            "Federated Round 223/1000\n",
            "Test set: Average loss: 0.4860, Accuracy: 145/178 (81%)\n",
            "Federated Round 224/1000\n",
            "Test set: Average loss: 0.4873, Accuracy: 144/178 (81%)\n",
            "Federated Round 225/1000\n",
            "Test set: Average loss: 0.4863, Accuracy: 144/178 (81%)\n",
            "Federated Round 226/1000\n",
            "Test set: Average loss: 0.4862, Accuracy: 144/178 (81%)\n",
            "Federated Round 227/1000\n",
            "Test set: Average loss: 0.4923, Accuracy: 145/178 (81%)\n",
            "Federated Round 228/1000\n",
            "Test set: Average loss: 0.4874, Accuracy: 145/178 (81%)\n",
            "Federated Round 229/1000\n",
            "Test set: Average loss: 0.4882, Accuracy: 145/178 (81%)\n",
            "Federated Round 230/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 144/178 (81%)\n",
            "Federated Round 231/1000\n",
            "Test set: Average loss: 0.4895, Accuracy: 145/178 (81%)\n",
            "Federated Round 232/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 144/178 (81%)\n",
            "Federated Round 233/1000\n",
            "Test set: Average loss: 0.4940, Accuracy: 145/178 (81%)\n",
            "Federated Round 234/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 144/178 (81%)\n",
            "Federated Round 235/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 144/178 (81%)\n",
            "Federated Round 236/1000\n",
            "Test set: Average loss: 0.4887, Accuracy: 144/178 (81%)\n",
            "Federated Round 237/1000\n",
            "Test set: Average loss: 0.4915, Accuracy: 145/178 (81%)\n",
            "Federated Round 238/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 144/178 (81%)\n",
            "Federated Round 239/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 145/178 (81%)\n",
            "Federated Round 240/1000\n",
            "Test set: Average loss: 0.4898, Accuracy: 145/178 (81%)\n",
            "Federated Round 241/1000\n",
            "Test set: Average loss: 0.4930, Accuracy: 145/178 (81%)\n",
            "Federated Round 242/1000\n",
            "Test set: Average loss: 0.4865, Accuracy: 145/178 (81%)\n",
            "Federated Round 243/1000\n",
            "Test set: Average loss: 0.4905, Accuracy: 144/178 (81%)\n",
            "Federated Round 244/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 144/178 (81%)\n",
            "Federated Round 245/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 145/178 (81%)\n",
            "Federated Round 246/1000\n",
            "Test set: Average loss: 0.4955, Accuracy: 145/178 (81%)\n",
            "Federated Round 247/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 144/178 (81%)\n",
            "Federated Round 248/1000\n",
            "Test set: Average loss: 0.4929, Accuracy: 145/178 (81%)\n",
            "Federated Round 249/1000\n",
            "Test set: Average loss: 0.4917, Accuracy: 145/178 (81%)\n",
            "Federated Round 250/1000\n",
            "Test set: Average loss: 0.4873, Accuracy: 145/178 (81%)\n",
            "Federated Round 251/1000\n",
            "Test set: Average loss: 0.4869, Accuracy: 145/178 (81%)\n",
            "Federated Round 252/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 145/178 (81%)\n",
            "Federated Round 253/1000\n",
            "Test set: Average loss: 0.4867, Accuracy: 145/178 (81%)\n",
            "Federated Round 254/1000\n",
            "Test set: Average loss: 0.4875, Accuracy: 145/178 (81%)\n",
            "Federated Round 255/1000\n",
            "Test set: Average loss: 0.4871, Accuracy: 145/178 (81%)\n",
            "Federated Round 256/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 145/178 (81%)\n",
            "Federated Round 257/1000\n",
            "Test set: Average loss: 0.4836, Accuracy: 146/178 (82%)\n",
            "Federated Round 258/1000\n",
            "Test set: Average loss: 0.4878, Accuracy: 145/178 (81%)\n",
            "Federated Round 259/1000\n",
            "Test set: Average loss: 0.4889, Accuracy: 145/178 (81%)\n",
            "Federated Round 260/1000\n",
            "Test set: Average loss: 0.4877, Accuracy: 145/178 (81%)\n",
            "Federated Round 261/1000\n",
            "Test set: Average loss: 0.4863, Accuracy: 145/178 (81%)\n",
            "Federated Round 262/1000\n",
            "Test set: Average loss: 0.4898, Accuracy: 145/178 (81%)\n",
            "Federated Round 263/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 145/178 (81%)\n",
            "Federated Round 264/1000\n",
            "Test set: Average loss: 0.4910, Accuracy: 145/178 (81%)\n",
            "Federated Round 265/1000\n",
            "Test set: Average loss: 0.4918, Accuracy: 145/178 (81%)\n",
            "Federated Round 266/1000\n",
            "Test set: Average loss: 0.4875, Accuracy: 146/178 (82%)\n",
            "Federated Round 267/1000\n",
            "Test set: Average loss: 0.4909, Accuracy: 145/178 (81%)\n",
            "Federated Round 268/1000\n",
            "Test set: Average loss: 0.4870, Accuracy: 146/178 (82%)\n",
            "Federated Round 269/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 145/178 (81%)\n",
            "Federated Round 270/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 145/178 (81%)\n",
            "Federated Round 271/1000\n",
            "Test set: Average loss: 0.4858, Accuracy: 146/178 (82%)\n",
            "Federated Round 272/1000\n",
            "Test set: Average loss: 0.4873, Accuracy: 145/178 (81%)\n",
            "Federated Round 273/1000\n",
            "Test set: Average loss: 0.4898, Accuracy: 145/178 (81%)\n",
            "Federated Round 274/1000\n",
            "Test set: Average loss: 0.4903, Accuracy: 145/178 (81%)\n",
            "Federated Round 275/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 145/178 (81%)\n",
            "Federated Round 276/1000\n",
            "Test set: Average loss: 0.4886, Accuracy: 145/178 (81%)\n",
            "Federated Round 277/1000\n",
            "Test set: Average loss: 0.4901, Accuracy: 145/178 (81%)\n",
            "Federated Round 278/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 145/178 (81%)\n",
            "Federated Round 279/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 145/178 (81%)\n",
            "Federated Round 280/1000\n",
            "Test set: Average loss: 0.4864, Accuracy: 146/178 (82%)\n",
            "Federated Round 281/1000\n",
            "Test set: Average loss: 0.4866, Accuracy: 145/178 (81%)\n",
            "Federated Round 282/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 145/178 (81%)\n",
            "Federated Round 283/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 145/178 (81%)\n",
            "Federated Round 284/1000\n",
            "Test set: Average loss: 0.4945, Accuracy: 145/178 (81%)\n",
            "Federated Round 285/1000\n",
            "Test set: Average loss: 0.4886, Accuracy: 146/178 (82%)\n",
            "Federated Round 286/1000\n",
            "Test set: Average loss: 0.4886, Accuracy: 146/178 (82%)\n",
            "Federated Round 287/1000\n",
            "Test set: Average loss: 0.4874, Accuracy: 146/178 (82%)\n",
            "Federated Round 288/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 146/178 (82%)\n",
            "Federated Round 289/1000\n",
            "Test set: Average loss: 0.4844, Accuracy: 147/178 (83%)\n",
            "Federated Round 290/1000\n",
            "Test set: Average loss: 0.4880, Accuracy: 146/178 (82%)\n",
            "Federated Round 291/1000\n",
            "Test set: Average loss: 0.4865, Accuracy: 146/178 (82%)\n",
            "Federated Round 292/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 146/178 (82%)\n",
            "Federated Round 293/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 146/178 (82%)\n",
            "Federated Round 294/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 146/178 (82%)\n",
            "Federated Round 295/1000\n",
            "Test set: Average loss: 0.4875, Accuracy: 146/178 (82%)\n",
            "Federated Round 296/1000\n",
            "Test set: Average loss: 0.4880, Accuracy: 146/178 (82%)\n",
            "Federated Round 297/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 145/178 (81%)\n",
            "Federated Round 298/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 145/178 (81%)\n",
            "Federated Round 299/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 145/178 (81%)\n",
            "Federated Round 300/1000\n",
            "Test set: Average loss: 0.4878, Accuracy: 145/178 (81%)\n",
            "Federated Round 301/1000\n",
            "Test set: Average loss: 0.4922, Accuracy: 145/178 (81%)\n",
            "Federated Round 302/1000\n",
            "Test set: Average loss: 0.4863, Accuracy: 146/178 (82%)\n",
            "Federated Round 303/1000\n",
            "Test set: Average loss: 0.4887, Accuracy: 145/178 (81%)\n",
            "Federated Round 304/1000\n",
            "Test set: Average loss: 0.4907, Accuracy: 145/178 (81%)\n",
            "Federated Round 305/1000\n",
            "Test set: Average loss: 0.4871, Accuracy: 147/178 (83%)\n",
            "Federated Round 306/1000\n",
            "Test set: Average loss: 0.4926, Accuracy: 145/178 (81%)\n",
            "Federated Round 307/1000\n",
            "Test set: Average loss: 0.4901, Accuracy: 145/178 (81%)\n",
            "Federated Round 308/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 145/178 (81%)\n",
            "Federated Round 309/1000\n",
            "Test set: Average loss: 0.4887, Accuracy: 145/178 (81%)\n",
            "Federated Round 310/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 145/178 (81%)\n",
            "Federated Round 311/1000\n",
            "Test set: Average loss: 0.4904, Accuracy: 145/178 (81%)\n",
            "Federated Round 312/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 145/178 (81%)\n",
            "Federated Round 313/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 145/178 (81%)\n",
            "Federated Round 314/1000\n",
            "Test set: Average loss: 0.4930, Accuracy: 144/178 (81%)\n",
            "Federated Round 315/1000\n",
            "Test set: Average loss: 0.4851, Accuracy: 147/178 (83%)\n",
            "Federated Round 316/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 145/178 (81%)\n",
            "Federated Round 317/1000\n",
            "Test set: Average loss: 0.4894, Accuracy: 146/178 (82%)\n",
            "Federated Round 318/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 146/178 (82%)\n",
            "Federated Round 319/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 146/178 (82%)\n",
            "Federated Round 320/1000\n",
            "Test set: Average loss: 0.4904, Accuracy: 146/178 (82%)\n",
            "Federated Round 321/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 146/178 (82%)\n",
            "Federated Round 322/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 145/178 (81%)\n",
            "Federated Round 323/1000\n",
            "Test set: Average loss: 0.4898, Accuracy: 146/178 (82%)\n",
            "Federated Round 324/1000\n",
            "Test set: Average loss: 0.4869, Accuracy: 146/178 (82%)\n",
            "Federated Round 325/1000\n",
            "Test set: Average loss: 0.4868, Accuracy: 146/178 (82%)\n",
            "Federated Round 326/1000\n",
            "Test set: Average loss: 0.4854, Accuracy: 145/178 (81%)\n",
            "Federated Round 327/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 146/178 (82%)\n",
            "Federated Round 328/1000\n",
            "Test set: Average loss: 0.4863, Accuracy: 147/178 (83%)\n",
            "Federated Round 329/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 146/178 (82%)\n",
            "Federated Round 330/1000\n",
            "Test set: Average loss: 0.4876, Accuracy: 146/178 (82%)\n",
            "Federated Round 331/1000\n",
            "Test set: Average loss: 0.4905, Accuracy: 145/178 (81%)\n",
            "Federated Round 332/1000\n",
            "Test set: Average loss: 0.4874, Accuracy: 146/178 (82%)\n",
            "Federated Round 333/1000\n",
            "Test set: Average loss: 0.4890, Accuracy: 146/178 (82%)\n",
            "Federated Round 334/1000\n",
            "Test set: Average loss: 0.4899, Accuracy: 145/178 (81%)\n",
            "Federated Round 335/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 146/178 (82%)\n",
            "Federated Round 336/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 146/178 (82%)\n",
            "Federated Round 337/1000\n",
            "Test set: Average loss: 0.4898, Accuracy: 146/178 (82%)\n",
            "Federated Round 338/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 146/178 (82%)\n",
            "Federated Round 339/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 146/178 (82%)\n",
            "Federated Round 340/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 145/178 (81%)\n",
            "Federated Round 341/1000\n",
            "Test set: Average loss: 0.4902, Accuracy: 145/178 (81%)\n",
            "Federated Round 342/1000\n",
            "Test set: Average loss: 0.4927, Accuracy: 144/178 (81%)\n",
            "Federated Round 343/1000\n",
            "Test set: Average loss: 0.4869, Accuracy: 146/178 (82%)\n",
            "Federated Round 344/1000\n",
            "Test set: Average loss: 0.4915, Accuracy: 144/178 (81%)\n",
            "Federated Round 345/1000\n",
            "Test set: Average loss: 0.4882, Accuracy: 146/178 (82%)\n",
            "Federated Round 346/1000\n",
            "Test set: Average loss: 0.4855, Accuracy: 146/178 (82%)\n",
            "Federated Round 347/1000\n",
            "Test set: Average loss: 0.4887, Accuracy: 145/178 (81%)\n",
            "Federated Round 348/1000\n",
            "Test set: Average loss: 0.4846, Accuracy: 145/178 (81%)\n",
            "Federated Round 349/1000\n",
            "Test set: Average loss: 0.4866, Accuracy: 145/178 (81%)\n",
            "Federated Round 350/1000\n",
            "Test set: Average loss: 0.4853, Accuracy: 146/178 (82%)\n",
            "Federated Round 351/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 145/178 (81%)\n",
            "Federated Round 352/1000\n",
            "Test set: Average loss: 0.4874, Accuracy: 146/178 (82%)\n",
            "Federated Round 353/1000\n",
            "Test set: Average loss: 0.4890, Accuracy: 144/178 (81%)\n",
            "Federated Round 354/1000\n",
            "Test set: Average loss: 0.4862, Accuracy: 146/178 (82%)\n",
            "Federated Round 355/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 145/178 (81%)\n",
            "Federated Round 356/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 146/178 (82%)\n",
            "Federated Round 357/1000\n",
            "Test set: Average loss: 0.4871, Accuracy: 146/178 (82%)\n",
            "Federated Round 358/1000\n",
            "Test set: Average loss: 0.4868, Accuracy: 146/178 (82%)\n",
            "Federated Round 359/1000\n",
            "Test set: Average loss: 0.4876, Accuracy: 146/178 (82%)\n",
            "Federated Round 360/1000\n",
            "Test set: Average loss: 0.4859, Accuracy: 146/178 (82%)\n",
            "Federated Round 361/1000\n",
            "Test set: Average loss: 0.4862, Accuracy: 145/178 (81%)\n",
            "Federated Round 362/1000\n",
            "Test set: Average loss: 0.4857, Accuracy: 145/178 (81%)\n",
            "Federated Round 363/1000\n",
            "Test set: Average loss: 0.4868, Accuracy: 144/178 (81%)\n",
            "Federated Round 364/1000\n",
            "Test set: Average loss: 0.4843, Accuracy: 145/178 (81%)\n",
            "Federated Round 365/1000\n",
            "Test set: Average loss: 0.4880, Accuracy: 145/178 (81%)\n",
            "Federated Round 366/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 145/178 (81%)\n",
            "Federated Round 367/1000\n",
            "Test set: Average loss: 0.4926, Accuracy: 145/178 (81%)\n",
            "Federated Round 368/1000\n",
            "Test set: Average loss: 0.4870, Accuracy: 145/178 (81%)\n",
            "Federated Round 369/1000\n",
            "Test set: Average loss: 0.4914, Accuracy: 145/178 (81%)\n",
            "Federated Round 370/1000\n",
            "Test set: Average loss: 0.4875, Accuracy: 144/178 (81%)\n",
            "Federated Round 371/1000\n",
            "Test set: Average loss: 0.4879, Accuracy: 145/178 (81%)\n",
            "Federated Round 372/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 145/178 (81%)\n",
            "Federated Round 373/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 145/178 (81%)\n",
            "Federated Round 374/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 145/178 (81%)\n",
            "Federated Round 375/1000\n",
            "Test set: Average loss: 0.4876, Accuracy: 144/178 (81%)\n",
            "Federated Round 376/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 144/178 (81%)\n",
            "Federated Round 377/1000\n",
            "Test set: Average loss: 0.4860, Accuracy: 144/178 (81%)\n",
            "Federated Round 378/1000\n",
            "Test set: Average loss: 0.4889, Accuracy: 144/178 (81%)\n",
            "Federated Round 379/1000\n",
            "Test set: Average loss: 0.4858, Accuracy: 144/178 (81%)\n",
            "Federated Round 380/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 145/178 (81%)\n",
            "Federated Round 381/1000\n",
            "Test set: Average loss: 0.4903, Accuracy: 145/178 (81%)\n",
            "Federated Round 382/1000\n",
            "Test set: Average loss: 0.4909, Accuracy: 144/178 (81%)\n",
            "Federated Round 383/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 144/178 (81%)\n",
            "Federated Round 384/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 144/178 (81%)\n",
            "Federated Round 385/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 144/178 (81%)\n",
            "Federated Round 386/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 144/178 (81%)\n",
            "Federated Round 387/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 144/178 (81%)\n",
            "Federated Round 388/1000\n",
            "Test set: Average loss: 0.4867, Accuracy: 146/178 (82%)\n",
            "Federated Round 389/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 144/178 (81%)\n",
            "Federated Round 390/1000\n",
            "Test set: Average loss: 0.4870, Accuracy: 146/178 (82%)\n",
            "Federated Round 391/1000\n",
            "Test set: Average loss: 0.4853, Accuracy: 146/178 (82%)\n",
            "Federated Round 392/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 144/178 (81%)\n",
            "Federated Round 393/1000\n",
            "Test set: Average loss: 0.4842, Accuracy: 147/178 (83%)\n",
            "Federated Round 394/1000\n",
            "Test set: Average loss: 0.4867, Accuracy: 146/178 (82%)\n",
            "Federated Round 395/1000\n",
            "Test set: Average loss: 0.4887, Accuracy: 145/178 (81%)\n",
            "Federated Round 396/1000\n",
            "Test set: Average loss: 0.4867, Accuracy: 146/178 (82%)\n",
            "Federated Round 397/1000\n",
            "Test set: Average loss: 0.4865, Accuracy: 146/178 (82%)\n",
            "Federated Round 398/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 146/178 (82%)\n",
            "Federated Round 399/1000\n",
            "Test set: Average loss: 0.4869, Accuracy: 146/178 (82%)\n",
            "Federated Round 400/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 146/178 (82%)\n",
            "Federated Round 401/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 146/178 (82%)\n",
            "Federated Round 402/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 146/178 (82%)\n",
            "Federated Round 403/1000\n",
            "Test set: Average loss: 0.4857, Accuracy: 146/178 (82%)\n",
            "Federated Round 404/1000\n",
            "Test set: Average loss: 0.4880, Accuracy: 146/178 (82%)\n",
            "Federated Round 405/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 146/178 (82%)\n",
            "Federated Round 406/1000\n",
            "Test set: Average loss: 0.4863, Accuracy: 146/178 (82%)\n",
            "Federated Round 407/1000\n",
            "Test set: Average loss: 0.4867, Accuracy: 146/178 (82%)\n",
            "Federated Round 408/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 146/178 (82%)\n",
            "Federated Round 409/1000\n",
            "Test set: Average loss: 0.4899, Accuracy: 145/178 (81%)\n",
            "Federated Round 410/1000\n",
            "Test set: Average loss: 0.4866, Accuracy: 145/178 (81%)\n",
            "Federated Round 411/1000\n",
            "Test set: Average loss: 0.4949, Accuracy: 142/178 (80%)\n",
            "Federated Round 412/1000\n",
            "Test set: Average loss: 0.4818, Accuracy: 147/178 (83%)\n",
            "Federated Round 413/1000\n",
            "Test set: Average loss: 0.4905, Accuracy: 145/178 (81%)\n",
            "Federated Round 414/1000\n",
            "Test set: Average loss: 0.4911, Accuracy: 144/178 (81%)\n",
            "Federated Round 415/1000\n",
            "Test set: Average loss: 0.4851, Accuracy: 146/178 (82%)\n",
            "Federated Round 416/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 144/178 (81%)\n",
            "Federated Round 417/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 145/178 (81%)\n",
            "Federated Round 418/1000\n",
            "Test set: Average loss: 0.4878, Accuracy: 145/178 (81%)\n",
            "Federated Round 419/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 144/178 (81%)\n",
            "Federated Round 420/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 144/178 (81%)\n",
            "Federated Round 421/1000\n",
            "Test set: Average loss: 0.4947, Accuracy: 142/178 (80%)\n",
            "Federated Round 422/1000\n",
            "Test set: Average loss: 0.4838, Accuracy: 146/178 (82%)\n",
            "Federated Round 423/1000\n",
            "Test set: Average loss: 0.4836, Accuracy: 146/178 (82%)\n",
            "Federated Round 424/1000\n",
            "Test set: Average loss: 0.4861, Accuracy: 145/178 (81%)\n",
            "Federated Round 425/1000\n",
            "Test set: Average loss: 0.4865, Accuracy: 145/178 (81%)\n",
            "Federated Round 426/1000\n",
            "Test set: Average loss: 0.4857, Accuracy: 145/178 (81%)\n",
            "Federated Round 427/1000\n",
            "Test set: Average loss: 0.4860, Accuracy: 145/178 (81%)\n",
            "Federated Round 428/1000\n",
            "Test set: Average loss: 0.4894, Accuracy: 145/178 (81%)\n",
            "Federated Round 429/1000\n",
            "Test set: Average loss: 0.4847, Accuracy: 145/178 (81%)\n",
            "Federated Round 430/1000\n",
            "Test set: Average loss: 0.4861, Accuracy: 145/178 (81%)\n",
            "Federated Round 431/1000\n",
            "Test set: Average loss: 0.4857, Accuracy: 144/178 (81%)\n",
            "Federated Round 432/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 145/178 (81%)\n",
            "Federated Round 433/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 145/178 (81%)\n",
            "Federated Round 434/1000\n",
            "Test set: Average loss: 0.4830, Accuracy: 144/178 (81%)\n",
            "Federated Round 435/1000\n",
            "Test set: Average loss: 0.4861, Accuracy: 145/178 (81%)\n",
            "Federated Round 436/1000\n",
            "Test set: Average loss: 0.4838, Accuracy: 145/178 (81%)\n",
            "Federated Round 437/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 144/178 (81%)\n",
            "Federated Round 438/1000\n",
            "Test set: Average loss: 0.4839, Accuracy: 145/178 (81%)\n",
            "Federated Round 439/1000\n",
            "Test set: Average loss: 0.4853, Accuracy: 145/178 (81%)\n",
            "Federated Round 440/1000\n",
            "Test set: Average loss: 0.4877, Accuracy: 145/178 (81%)\n",
            "Federated Round 441/1000\n",
            "Test set: Average loss: 0.4844, Accuracy: 145/178 (81%)\n",
            "Federated Round 442/1000\n",
            "Test set: Average loss: 0.4850, Accuracy: 144/178 (81%)\n",
            "Federated Round 443/1000\n",
            "Test set: Average loss: 0.4875, Accuracy: 145/178 (81%)\n",
            "Federated Round 444/1000\n",
            "Test set: Average loss: 0.4850, Accuracy: 145/178 (81%)\n",
            "Federated Round 445/1000\n",
            "Test set: Average loss: 0.4910, Accuracy: 145/178 (81%)\n",
            "Federated Round 446/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 145/178 (81%)\n",
            "Federated Round 447/1000\n",
            "Test set: Average loss: 0.4900, Accuracy: 145/178 (81%)\n",
            "Federated Round 448/1000\n",
            "Test set: Average loss: 0.4860, Accuracy: 145/178 (81%)\n",
            "Federated Round 449/1000\n",
            "Test set: Average loss: 0.4904, Accuracy: 145/178 (81%)\n",
            "Federated Round 450/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 145/178 (81%)\n",
            "Federated Round 451/1000\n",
            "Test set: Average loss: 0.4889, Accuracy: 145/178 (81%)\n",
            "Federated Round 452/1000\n",
            "Test set: Average loss: 0.4919, Accuracy: 145/178 (81%)\n",
            "Federated Round 453/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 145/178 (81%)\n",
            "Federated Round 454/1000\n",
            "Test set: Average loss: 0.4911, Accuracy: 145/178 (81%)\n",
            "Federated Round 455/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 144/178 (81%)\n",
            "Federated Round 456/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 144/178 (81%)\n",
            "Federated Round 457/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 144/178 (81%)\n",
            "Federated Round 458/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 145/178 (81%)\n",
            "Federated Round 459/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 144/178 (81%)\n",
            "Federated Round 460/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 144/178 (81%)\n",
            "Federated Round 461/1000\n",
            "Test set: Average loss: 0.4870, Accuracy: 144/178 (81%)\n",
            "Federated Round 462/1000\n",
            "Test set: Average loss: 0.4902, Accuracy: 145/178 (81%)\n",
            "Federated Round 463/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 144/178 (81%)\n",
            "Federated Round 464/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 144/178 (81%)\n",
            "Federated Round 465/1000\n",
            "Test set: Average loss: 0.4860, Accuracy: 144/178 (81%)\n",
            "Federated Round 466/1000\n",
            "Test set: Average loss: 0.4870, Accuracy: 144/178 (81%)\n",
            "Federated Round 467/1000\n",
            "Test set: Average loss: 0.4857, Accuracy: 144/178 (81%)\n",
            "Federated Round 468/1000\n",
            "Test set: Average loss: 0.4855, Accuracy: 144/178 (81%)\n",
            "Federated Round 469/1000\n",
            "Test set: Average loss: 0.4857, Accuracy: 144/178 (81%)\n",
            "Federated Round 470/1000\n",
            "Test set: Average loss: 0.4890, Accuracy: 144/178 (81%)\n",
            "Federated Round 471/1000\n",
            "Test set: Average loss: 0.4848, Accuracy: 144/178 (81%)\n",
            "Federated Round 472/1000\n",
            "Test set: Average loss: 0.4903, Accuracy: 144/178 (81%)\n",
            "Federated Round 473/1000\n",
            "Test set: Average loss: 0.4895, Accuracy: 144/178 (81%)\n",
            "Federated Round 474/1000\n",
            "Test set: Average loss: 0.4882, Accuracy: 144/178 (81%)\n",
            "Federated Round 475/1000\n",
            "Test set: Average loss: 0.4929, Accuracy: 145/178 (81%)\n",
            "Federated Round 476/1000\n",
            "Test set: Average loss: 0.4900, Accuracy: 145/178 (81%)\n",
            "Federated Round 477/1000\n",
            "Test set: Average loss: 0.4890, Accuracy: 144/178 (81%)\n",
            "Federated Round 478/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 144/178 (81%)\n",
            "Federated Round 479/1000\n",
            "Test set: Average loss: 0.4895, Accuracy: 144/178 (81%)\n",
            "Federated Round 480/1000\n",
            "Test set: Average loss: 0.4882, Accuracy: 144/178 (81%)\n",
            "Federated Round 481/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 144/178 (81%)\n",
            "Federated Round 482/1000\n",
            "Test set: Average loss: 0.4842, Accuracy: 144/178 (81%)\n",
            "Federated Round 483/1000\n",
            "Test set: Average loss: 0.4875, Accuracy: 144/178 (81%)\n",
            "Federated Round 484/1000\n",
            "Test set: Average loss: 0.4890, Accuracy: 144/178 (81%)\n",
            "Federated Round 485/1000\n",
            "Test set: Average loss: 0.4848, Accuracy: 144/178 (81%)\n",
            "Federated Round 486/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 145/178 (81%)\n",
            "Federated Round 487/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 145/178 (81%)\n",
            "Federated Round 488/1000\n",
            "Test set: Average loss: 0.4926, Accuracy: 145/178 (81%)\n",
            "Federated Round 489/1000\n",
            "Test set: Average loss: 0.4948, Accuracy: 145/178 (81%)\n",
            "Federated Round 490/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 144/178 (81%)\n",
            "Federated Round 491/1000\n",
            "Test set: Average loss: 0.4870, Accuracy: 144/178 (81%)\n",
            "Federated Round 492/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 144/178 (81%)\n",
            "Federated Round 493/1000\n",
            "Test set: Average loss: 0.4870, Accuracy: 144/178 (81%)\n",
            "Federated Round 494/1000\n",
            "Test set: Average loss: 0.4882, Accuracy: 144/178 (81%)\n",
            "Federated Round 495/1000\n",
            "Test set: Average loss: 0.4857, Accuracy: 144/178 (81%)\n",
            "Federated Round 496/1000\n",
            "Test set: Average loss: 0.4845, Accuracy: 144/178 (81%)\n",
            "Federated Round 497/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 143/178 (80%)\n",
            "Federated Round 498/1000\n",
            "Test set: Average loss: 0.4855, Accuracy: 144/178 (81%)\n",
            "Federated Round 499/1000\n",
            "Test set: Average loss: 0.4853, Accuracy: 144/178 (81%)\n",
            "Federated Round 500/1000\n",
            "Test set: Average loss: 0.4867, Accuracy: 144/178 (81%)\n",
            "Federated Round 501/1000\n",
            "Test set: Average loss: 0.4839, Accuracy: 144/178 (81%)\n",
            "Federated Round 502/1000\n",
            "Test set: Average loss: 0.4852, Accuracy: 144/178 (81%)\n",
            "Federated Round 503/1000\n",
            "Test set: Average loss: 0.4837, Accuracy: 144/178 (81%)\n",
            "Federated Round 504/1000\n",
            "Test set: Average loss: 0.4863, Accuracy: 144/178 (81%)\n",
            "Federated Round 505/1000\n",
            "Test set: Average loss: 0.4889, Accuracy: 144/178 (81%)\n",
            "Federated Round 506/1000\n",
            "Test set: Average loss: 0.4849, Accuracy: 144/178 (81%)\n",
            "Federated Round 507/1000\n",
            "Test set: Average loss: 0.4867, Accuracy: 144/178 (81%)\n",
            "Federated Round 508/1000\n",
            "Test set: Average loss: 0.4860, Accuracy: 144/178 (81%)\n",
            "Federated Round 509/1000\n",
            "Test set: Average loss: 0.4875, Accuracy: 144/178 (81%)\n",
            "Federated Round 510/1000\n",
            "Test set: Average loss: 0.4871, Accuracy: 144/178 (81%)\n",
            "Federated Round 511/1000\n",
            "Test set: Average loss: 0.4861, Accuracy: 144/178 (81%)\n",
            "Federated Round 512/1000\n",
            "Test set: Average loss: 0.4890, Accuracy: 144/178 (81%)\n",
            "Federated Round 513/1000\n",
            "Test set: Average loss: 0.4845, Accuracy: 144/178 (81%)\n",
            "Federated Round 514/1000\n",
            "Test set: Average loss: 0.4851, Accuracy: 144/178 (81%)\n",
            "Federated Round 515/1000\n",
            "Test set: Average loss: 0.4841, Accuracy: 144/178 (81%)\n",
            "Federated Round 516/1000\n",
            "Test set: Average loss: 0.4868, Accuracy: 144/178 (81%)\n",
            "Federated Round 517/1000\n",
            "Test set: Average loss: 0.4842, Accuracy: 144/178 (81%)\n",
            "Federated Round 518/1000\n",
            "Test set: Average loss: 0.4862, Accuracy: 144/178 (81%)\n",
            "Federated Round 519/1000\n",
            "Test set: Average loss: 0.4904, Accuracy: 144/178 (81%)\n",
            "Federated Round 520/1000\n",
            "Test set: Average loss: 0.4813, Accuracy: 145/178 (81%)\n",
            "Federated Round 521/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 144/178 (81%)\n",
            "Federated Round 522/1000\n",
            "Test set: Average loss: 0.4854, Accuracy: 144/178 (81%)\n",
            "Federated Round 523/1000\n",
            "Test set: Average loss: 0.4825, Accuracy: 145/178 (81%)\n",
            "Federated Round 524/1000\n",
            "Test set: Average loss: 0.4845, Accuracy: 144/178 (81%)\n",
            "Federated Round 525/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 144/178 (81%)\n",
            "Federated Round 526/1000\n",
            "Test set: Average loss: 0.4876, Accuracy: 144/178 (81%)\n",
            "Federated Round 527/1000\n",
            "Test set: Average loss: 0.4856, Accuracy: 144/178 (81%)\n",
            "Federated Round 528/1000\n",
            "Test set: Average loss: 0.4859, Accuracy: 144/178 (81%)\n",
            "Federated Round 529/1000\n",
            "Test set: Average loss: 0.4866, Accuracy: 144/178 (81%)\n",
            "Federated Round 530/1000\n",
            "Test set: Average loss: 0.4859, Accuracy: 144/178 (81%)\n",
            "Federated Round 531/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 144/178 (81%)\n",
            "Federated Round 532/1000\n",
            "Test set: Average loss: 0.4871, Accuracy: 144/178 (81%)\n",
            "Federated Round 533/1000\n",
            "Test set: Average loss: 0.4864, Accuracy: 144/178 (81%)\n",
            "Federated Round 534/1000\n",
            "Test set: Average loss: 0.4889, Accuracy: 144/178 (81%)\n",
            "Federated Round 535/1000\n",
            "Test set: Average loss: 0.4809, Accuracy: 145/178 (81%)\n",
            "Federated Round 536/1000\n",
            "Test set: Average loss: 0.4966, Accuracy: 143/178 (80%)\n",
            "Federated Round 537/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 143/178 (80%)\n",
            "Federated Round 538/1000\n",
            "Test set: Average loss: 0.4869, Accuracy: 144/178 (81%)\n",
            "Federated Round 539/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 144/178 (81%)\n",
            "Federated Round 540/1000\n",
            "Test set: Average loss: 0.4933, Accuracy: 143/178 (80%)\n",
            "Federated Round 541/1000\n",
            "Test set: Average loss: 0.4879, Accuracy: 144/178 (81%)\n",
            "Federated Round 542/1000\n",
            "Test set: Average loss: 0.4905, Accuracy: 143/178 (80%)\n",
            "Federated Round 543/1000\n",
            "Test set: Average loss: 0.4889, Accuracy: 144/178 (81%)\n",
            "Federated Round 544/1000\n",
            "Test set: Average loss: 0.4941, Accuracy: 142/178 (80%)\n",
            "Federated Round 545/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 144/178 (81%)\n",
            "Federated Round 546/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 144/178 (81%)\n",
            "Federated Round 547/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 144/178 (81%)\n",
            "Federated Round 548/1000\n",
            "Test set: Average loss: 0.4934, Accuracy: 143/178 (80%)\n",
            "Federated Round 549/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 144/178 (81%)\n",
            "Federated Round 550/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 144/178 (81%)\n",
            "Federated Round 551/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 144/178 (81%)\n",
            "Federated Round 552/1000\n",
            "Test set: Average loss: 0.4882, Accuracy: 144/178 (81%)\n",
            "Federated Round 553/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 143/178 (80%)\n",
            "Federated Round 554/1000\n",
            "Test set: Average loss: 0.4870, Accuracy: 144/178 (81%)\n",
            "Federated Round 555/1000\n",
            "Test set: Average loss: 0.5059, Accuracy: 138/178 (78%)\n",
            "Federated Round 556/1000\n",
            "Test set: Average loss: 0.4864, Accuracy: 144/178 (81%)\n",
            "Federated Round 557/1000\n",
            "Test set: Average loss: 0.4802, Accuracy: 144/178 (81%)\n",
            "Federated Round 558/1000\n",
            "Test set: Average loss: 0.4867, Accuracy: 144/178 (81%)\n",
            "Federated Round 559/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 143/178 (80%)\n",
            "Federated Round 560/1000\n",
            "Test set: Average loss: 0.4861, Accuracy: 144/178 (81%)\n",
            "Federated Round 561/1000\n",
            "Test set: Average loss: 0.4862, Accuracy: 144/178 (81%)\n",
            "Federated Round 562/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 142/178 (80%)\n",
            "Federated Round 563/1000\n",
            "Test set: Average loss: 0.4876, Accuracy: 142/178 (80%)\n",
            "Federated Round 564/1000\n",
            "Test set: Average loss: 0.4922, Accuracy: 143/178 (80%)\n",
            "Federated Round 565/1000\n",
            "Test set: Average loss: 0.4879, Accuracy: 143/178 (80%)\n",
            "Federated Round 566/1000\n",
            "Test set: Average loss: 0.4850, Accuracy: 144/178 (81%)\n",
            "Federated Round 567/1000\n",
            "Test set: Average loss: 0.4866, Accuracy: 143/178 (80%)\n",
            "Federated Round 568/1000\n",
            "Test set: Average loss: 0.4853, Accuracy: 144/178 (81%)\n",
            "Federated Round 569/1000\n",
            "Test set: Average loss: 0.4889, Accuracy: 143/178 (80%)\n",
            "Federated Round 570/1000\n",
            "Test set: Average loss: 0.4856, Accuracy: 143/178 (80%)\n",
            "Federated Round 571/1000\n",
            "Test set: Average loss: 0.4808, Accuracy: 145/178 (81%)\n",
            "Federated Round 572/1000\n",
            "Test set: Average loss: 0.4877, Accuracy: 143/178 (80%)\n",
            "Federated Round 573/1000\n",
            "Test set: Average loss: 0.4879, Accuracy: 143/178 (80%)\n",
            "Federated Round 574/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 143/178 (80%)\n",
            "Federated Round 575/1000\n",
            "Test set: Average loss: 0.4866, Accuracy: 144/178 (81%)\n",
            "Federated Round 576/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 143/178 (80%)\n",
            "Federated Round 577/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 143/178 (80%)\n",
            "Federated Round 578/1000\n",
            "Test set: Average loss: 0.4871, Accuracy: 143/178 (80%)\n",
            "Federated Round 579/1000\n",
            "Test set: Average loss: 0.4844, Accuracy: 144/178 (81%)\n",
            "Federated Round 580/1000\n",
            "Test set: Average loss: 0.4849, Accuracy: 144/178 (81%)\n",
            "Federated Round 581/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 143/178 (80%)\n",
            "Federated Round 582/1000\n",
            "Test set: Average loss: 0.4862, Accuracy: 144/178 (81%)\n",
            "Federated Round 583/1000\n",
            "Test set: Average loss: 0.4857, Accuracy: 144/178 (81%)\n",
            "Federated Round 584/1000\n",
            "Test set: Average loss: 0.4834, Accuracy: 144/178 (81%)\n",
            "Federated Round 585/1000\n",
            "Test set: Average loss: 0.4895, Accuracy: 144/178 (81%)\n",
            "Federated Round 586/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 145/178 (81%)\n",
            "Federated Round 587/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 145/178 (81%)\n",
            "Federated Round 588/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 145/178 (81%)\n",
            "Federated Round 589/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 145/178 (81%)\n",
            "Federated Round 590/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 145/178 (81%)\n",
            "Federated Round 591/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 143/178 (80%)\n",
            "Federated Round 592/1000\n",
            "Test set: Average loss: 0.4900, Accuracy: 144/178 (81%)\n",
            "Federated Round 593/1000\n",
            "Test set: Average loss: 0.4911, Accuracy: 143/178 (80%)\n",
            "Federated Round 594/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 144/178 (81%)\n",
            "Federated Round 595/1000\n",
            "Test set: Average loss: 0.4909, Accuracy: 145/178 (81%)\n",
            "Federated Round 596/1000\n",
            "Test set: Average loss: 0.4931, Accuracy: 143/178 (80%)\n",
            "Federated Round 597/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 144/178 (81%)\n",
            "Federated Round 598/1000\n",
            "Test set: Average loss: 0.4941, Accuracy: 143/178 (80%)\n",
            "Federated Round 599/1000\n",
            "Test set: Average loss: 0.4901, Accuracy: 144/178 (81%)\n",
            "Federated Round 600/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 144/178 (81%)\n",
            "Federated Round 601/1000\n",
            "Test set: Average loss: 0.4903, Accuracy: 143/178 (80%)\n",
            "Federated Round 602/1000\n",
            "Test set: Average loss: 0.4900, Accuracy: 144/178 (81%)\n",
            "Federated Round 603/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 144/178 (81%)\n",
            "Federated Round 604/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 144/178 (81%)\n",
            "Federated Round 605/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 144/178 (81%)\n",
            "Federated Round 606/1000\n",
            "Test set: Average loss: 0.4943, Accuracy: 142/178 (80%)\n",
            "Federated Round 607/1000\n",
            "Test set: Average loss: 0.4871, Accuracy: 144/178 (81%)\n",
            "Federated Round 608/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 144/178 (81%)\n",
            "Federated Round 609/1000\n",
            "Test set: Average loss: 0.4930, Accuracy: 142/178 (80%)\n",
            "Federated Round 610/1000\n",
            "Test set: Average loss: 0.4902, Accuracy: 143/178 (80%)\n",
            "Federated Round 611/1000\n",
            "Test set: Average loss: 0.4947, Accuracy: 141/178 (79%)\n",
            "Federated Round 612/1000\n",
            "Test set: Average loss: 0.4970, Accuracy: 142/178 (80%)\n",
            "Federated Round 613/1000\n",
            "Test set: Average loss: 0.4940, Accuracy: 141/178 (79%)\n",
            "Federated Round 614/1000\n",
            "Test set: Average loss: 0.4957, Accuracy: 142/178 (80%)\n",
            "Federated Round 615/1000\n",
            "Test set: Average loss: 0.4919, Accuracy: 141/178 (79%)\n",
            "Federated Round 616/1000\n",
            "Test set: Average loss: 0.4927, Accuracy: 141/178 (79%)\n",
            "Federated Round 617/1000\n",
            "Test set: Average loss: 0.4915, Accuracy: 141/178 (79%)\n",
            "Federated Round 618/1000\n",
            "Test set: Average loss: 0.4900, Accuracy: 144/178 (81%)\n",
            "Federated Round 619/1000\n",
            "Test set: Average loss: 0.4937, Accuracy: 141/178 (79%)\n",
            "Federated Round 620/1000\n",
            "Test set: Average loss: 0.4932, Accuracy: 141/178 (79%)\n",
            "Federated Round 621/1000\n",
            "Test set: Average loss: 0.4910, Accuracy: 143/178 (80%)\n",
            "Federated Round 622/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 142/178 (80%)\n",
            "Federated Round 623/1000\n",
            "Test set: Average loss: 0.4862, Accuracy: 144/178 (81%)\n",
            "Federated Round 624/1000\n",
            "Test set: Average loss: 0.4873, Accuracy: 144/178 (81%)\n",
            "Federated Round 625/1000\n",
            "Test set: Average loss: 0.4877, Accuracy: 144/178 (81%)\n",
            "Federated Round 626/1000\n",
            "Test set: Average loss: 0.4873, Accuracy: 144/178 (81%)\n",
            "Federated Round 627/1000\n",
            "Test set: Average loss: 0.4898, Accuracy: 142/178 (80%)\n",
            "Federated Round 628/1000\n",
            "Test set: Average loss: 0.4937, Accuracy: 143/178 (80%)\n",
            "Federated Round 629/1000\n",
            "Test set: Average loss: 0.4852, Accuracy: 144/178 (81%)\n",
            "Federated Round 630/1000\n",
            "Test set: Average loss: 0.4947, Accuracy: 143/178 (80%)\n",
            "Federated Round 631/1000\n",
            "Test set: Average loss: 0.4918, Accuracy: 142/178 (80%)\n",
            "Federated Round 632/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 144/178 (81%)\n",
            "Federated Round 633/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 144/178 (81%)\n",
            "Federated Round 634/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 143/178 (80%)\n",
            "Federated Round 635/1000\n",
            "Test set: Average loss: 0.4911, Accuracy: 142/178 (80%)\n",
            "Federated Round 636/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 142/178 (80%)\n",
            "Federated Round 637/1000\n",
            "Test set: Average loss: 0.4885, Accuracy: 143/178 (80%)\n",
            "Federated Round 638/1000\n",
            "Test set: Average loss: 0.4942, Accuracy: 143/178 (80%)\n",
            "Federated Round 639/1000\n",
            "Test set: Average loss: 0.4887, Accuracy: 143/178 (80%)\n",
            "Federated Round 640/1000\n",
            "Test set: Average loss: 0.4937, Accuracy: 142/178 (80%)\n",
            "Federated Round 641/1000\n",
            "Test set: Average loss: 0.4894, Accuracy: 143/178 (80%)\n",
            "Federated Round 642/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 143/178 (80%)\n",
            "Federated Round 643/1000\n",
            "Test set: Average loss: 0.4900, Accuracy: 142/178 (80%)\n",
            "Federated Round 644/1000\n",
            "Test set: Average loss: 0.4883, Accuracy: 143/178 (80%)\n",
            "Federated Round 645/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 142/178 (80%)\n",
            "Federated Round 646/1000\n",
            "Test set: Average loss: 0.4877, Accuracy: 143/178 (80%)\n",
            "Federated Round 647/1000\n",
            "Test set: Average loss: 0.4905, Accuracy: 143/178 (80%)\n",
            "Federated Round 648/1000\n",
            "Test set: Average loss: 0.4880, Accuracy: 143/178 (80%)\n",
            "Federated Round 649/1000\n",
            "Test set: Average loss: 0.4871, Accuracy: 144/178 (81%)\n",
            "Federated Round 650/1000\n",
            "Test set: Average loss: 0.4879, Accuracy: 144/178 (81%)\n",
            "Federated Round 651/1000\n",
            "Test set: Average loss: 0.4917, Accuracy: 142/178 (80%)\n",
            "Federated Round 652/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 141/178 (79%)\n",
            "Federated Round 653/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 142/178 (80%)\n",
            "Federated Round 654/1000\n",
            "Test set: Average loss: 0.4916, Accuracy: 141/178 (79%)\n",
            "Federated Round 655/1000\n",
            "Test set: Average loss: 0.4872, Accuracy: 143/178 (80%)\n",
            "Federated Round 656/1000\n",
            "Test set: Average loss: 0.4863, Accuracy: 143/178 (80%)\n",
            "Federated Round 657/1000\n",
            "Test set: Average loss: 0.4909, Accuracy: 141/178 (79%)\n",
            "Federated Round 658/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 141/178 (79%)\n",
            "Federated Round 659/1000\n",
            "Test set: Average loss: 0.4902, Accuracy: 142/178 (80%)\n",
            "Federated Round 660/1000\n",
            "Test set: Average loss: 0.4926, Accuracy: 141/178 (79%)\n",
            "Federated Round 661/1000\n",
            "Test set: Average loss: 0.4960, Accuracy: 142/178 (80%)\n",
            "Federated Round 662/1000\n",
            "Test set: Average loss: 0.4907, Accuracy: 142/178 (80%)\n",
            "Federated Round 663/1000\n",
            "Test set: Average loss: 0.4940, Accuracy: 141/178 (79%)\n",
            "Federated Round 664/1000\n",
            "Test set: Average loss: 0.4946, Accuracy: 141/178 (79%)\n",
            "Federated Round 665/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 142/178 (80%)\n",
            "Federated Round 666/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 141/178 (79%)\n",
            "Federated Round 667/1000\n",
            "Test set: Average loss: 0.4895, Accuracy: 142/178 (80%)\n",
            "Federated Round 668/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 142/178 (80%)\n",
            "Federated Round 669/1000\n",
            "Test set: Average loss: 0.4914, Accuracy: 142/178 (80%)\n",
            "Federated Round 670/1000\n",
            "Test set: Average loss: 0.4899, Accuracy: 143/178 (80%)\n",
            "Federated Round 671/1000\n",
            "Test set: Average loss: 0.4950, Accuracy: 142/178 (80%)\n",
            "Federated Round 672/1000\n",
            "Test set: Average loss: 0.4879, Accuracy: 143/178 (80%)\n",
            "Federated Round 673/1000\n",
            "Test set: Average loss: 0.4904, Accuracy: 143/178 (80%)\n",
            "Federated Round 674/1000\n",
            "Test set: Average loss: 0.4894, Accuracy: 143/178 (80%)\n",
            "Federated Round 675/1000\n",
            "Test set: Average loss: 0.4950, Accuracy: 142/178 (80%)\n",
            "Federated Round 676/1000\n",
            "Test set: Average loss: 0.4880, Accuracy: 143/178 (80%)\n",
            "Federated Round 677/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 143/178 (80%)\n",
            "Federated Round 678/1000\n",
            "Test set: Average loss: 0.4907, Accuracy: 141/178 (79%)\n",
            "Federated Round 679/1000\n",
            "Test set: Average loss: 0.4901, Accuracy: 142/178 (80%)\n",
            "Federated Round 680/1000\n",
            "Test set: Average loss: 0.4910, Accuracy: 141/178 (79%)\n",
            "Federated Round 681/1000\n",
            "Test set: Average loss: 0.4934, Accuracy: 142/178 (80%)\n",
            "Federated Round 682/1000\n",
            "Test set: Average loss: 0.4923, Accuracy: 142/178 (80%)\n",
            "Federated Round 683/1000\n",
            "Test set: Average loss: 0.4915, Accuracy: 142/178 (80%)\n",
            "Federated Round 684/1000\n",
            "Test set: Average loss: 0.4916, Accuracy: 142/178 (80%)\n",
            "Federated Round 685/1000\n",
            "Test set: Average loss: 0.4857, Accuracy: 144/178 (81%)\n",
            "Federated Round 686/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 142/178 (80%)\n",
            "Federated Round 687/1000\n",
            "Test set: Average loss: 0.4846, Accuracy: 143/178 (80%)\n",
            "Federated Round 688/1000\n",
            "Test set: Average loss: 0.4930, Accuracy: 142/178 (80%)\n",
            "Federated Round 689/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 142/178 (80%)\n",
            "Federated Round 690/1000\n",
            "Test set: Average loss: 0.4902, Accuracy: 141/178 (79%)\n",
            "Federated Round 691/1000\n",
            "Test set: Average loss: 0.4923, Accuracy: 141/178 (79%)\n",
            "Federated Round 692/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 142/178 (80%)\n",
            "Federated Round 693/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 142/178 (80%)\n",
            "Federated Round 694/1000\n",
            "Test set: Average loss: 0.4916, Accuracy: 141/178 (79%)\n",
            "Federated Round 695/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 141/178 (79%)\n",
            "Federated Round 696/1000\n",
            "Test set: Average loss: 0.4923, Accuracy: 142/178 (80%)\n",
            "Federated Round 697/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 142/178 (80%)\n",
            "Federated Round 698/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 141/178 (79%)\n",
            "Federated Round 699/1000\n",
            "Test set: Average loss: 0.4899, Accuracy: 141/178 (79%)\n",
            "Federated Round 700/1000\n",
            "Test set: Average loss: 0.4924, Accuracy: 141/178 (79%)\n",
            "Federated Round 701/1000\n",
            "Test set: Average loss: 0.4919, Accuracy: 141/178 (79%)\n",
            "Federated Round 702/1000\n",
            "Test set: Average loss: 0.4997, Accuracy: 141/178 (79%)\n",
            "Federated Round 703/1000\n",
            "Test set: Average loss: 0.4901, Accuracy: 142/178 (80%)\n",
            "Federated Round 704/1000\n",
            "Test set: Average loss: 0.4938, Accuracy: 141/178 (79%)\n",
            "Federated Round 705/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 142/178 (80%)\n",
            "Federated Round 706/1000\n",
            "Test set: Average loss: 0.4886, Accuracy: 143/178 (80%)\n",
            "Federated Round 707/1000\n",
            "Test set: Average loss: 0.4886, Accuracy: 143/178 (80%)\n",
            "Federated Round 708/1000\n",
            "Test set: Average loss: 0.4895, Accuracy: 143/178 (80%)\n",
            "Federated Round 709/1000\n",
            "Test set: Average loss: 0.4916, Accuracy: 143/178 (80%)\n",
            "Federated Round 710/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 143/178 (80%)\n",
            "Federated Round 711/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 143/178 (80%)\n",
            "Federated Round 712/1000\n",
            "Test set: Average loss: 0.4903, Accuracy: 142/178 (80%)\n",
            "Federated Round 713/1000\n",
            "Test set: Average loss: 0.4901, Accuracy: 143/178 (80%)\n",
            "Federated Round 714/1000\n",
            "Test set: Average loss: 0.4942, Accuracy: 143/178 (80%)\n",
            "Federated Round 715/1000\n",
            "Test set: Average loss: 0.4927, Accuracy: 144/178 (81%)\n",
            "Federated Round 716/1000\n",
            "Test set: Average loss: 0.4950, Accuracy: 143/178 (80%)\n",
            "Federated Round 717/1000\n",
            "Test set: Average loss: 0.4919, Accuracy: 142/178 (80%)\n",
            "Federated Round 718/1000\n",
            "Test set: Average loss: 0.4924, Accuracy: 142/178 (80%)\n",
            "Federated Round 719/1000\n",
            "Test set: Average loss: 0.4909, Accuracy: 143/178 (80%)\n",
            "Federated Round 720/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 143/178 (80%)\n",
            "Federated Round 721/1000\n",
            "Test set: Average loss: 0.4932, Accuracy: 142/178 (80%)\n",
            "Federated Round 722/1000\n",
            "Test set: Average loss: 0.4882, Accuracy: 143/178 (80%)\n",
            "Federated Round 723/1000\n",
            "Test set: Average loss: 0.4915, Accuracy: 142/178 (80%)\n",
            "Federated Round 724/1000\n",
            "Test set: Average loss: 0.4892, Accuracy: 143/178 (80%)\n",
            "Federated Round 725/1000\n",
            "Test set: Average loss: 0.4903, Accuracy: 142/178 (80%)\n",
            "Federated Round 726/1000\n",
            "Test set: Average loss: 0.4905, Accuracy: 142/178 (80%)\n",
            "Federated Round 727/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 143/178 (80%)\n",
            "Federated Round 728/1000\n",
            "Test set: Average loss: 0.4927, Accuracy: 142/178 (80%)\n",
            "Federated Round 729/1000\n",
            "Test set: Average loss: 0.4935, Accuracy: 143/178 (80%)\n",
            "Federated Round 730/1000\n",
            "Test set: Average loss: 0.4935, Accuracy: 144/178 (81%)\n",
            "Federated Round 731/1000\n",
            "Test set: Average loss: 0.4926, Accuracy: 142/178 (80%)\n",
            "Federated Round 732/1000\n",
            "Test set: Average loss: 0.4948, Accuracy: 142/178 (80%)\n",
            "Federated Round 733/1000\n",
            "Test set: Average loss: 0.4916, Accuracy: 142/178 (80%)\n",
            "Federated Round 734/1000\n",
            "Test set: Average loss: 0.4935, Accuracy: 142/178 (80%)\n",
            "Federated Round 735/1000\n",
            "Test set: Average loss: 0.4945, Accuracy: 142/178 (80%)\n",
            "Federated Round 736/1000\n",
            "Test set: Average loss: 0.4965, Accuracy: 142/178 (80%)\n",
            "Federated Round 737/1000\n",
            "Test set: Average loss: 0.4923, Accuracy: 142/178 (80%)\n",
            "Federated Round 738/1000\n",
            "Test set: Average loss: 0.4922, Accuracy: 142/178 (80%)\n",
            "Federated Round 739/1000\n",
            "Test set: Average loss: 0.4926, Accuracy: 142/178 (80%)\n",
            "Federated Round 740/1000\n",
            "Test set: Average loss: 0.4926, Accuracy: 142/178 (80%)\n",
            "Federated Round 741/1000\n",
            "Test set: Average loss: 0.4931, Accuracy: 142/178 (80%)\n",
            "Federated Round 742/1000\n",
            "Test set: Average loss: 0.4933, Accuracy: 142/178 (80%)\n",
            "Federated Round 743/1000\n",
            "Test set: Average loss: 0.4962, Accuracy: 143/178 (80%)\n",
            "Federated Round 744/1000\n",
            "Test set: Average loss: 0.4941, Accuracy: 143/178 (80%)\n",
            "Federated Round 745/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 144/178 (81%)\n",
            "Federated Round 746/1000\n",
            "Test set: Average loss: 0.4935, Accuracy: 142/178 (80%)\n",
            "Federated Round 747/1000\n",
            "Test set: Average loss: 0.4900, Accuracy: 144/178 (81%)\n",
            "Federated Round 748/1000\n",
            "Test set: Average loss: 0.4930, Accuracy: 142/178 (80%)\n",
            "Federated Round 749/1000\n",
            "Test set: Average loss: 0.4985, Accuracy: 142/178 (80%)\n",
            "Federated Round 750/1000\n",
            "Test set: Average loss: 0.4853, Accuracy: 144/178 (81%)\n",
            "Federated Round 751/1000\n",
            "Test set: Average loss: 0.4899, Accuracy: 143/178 (80%)\n",
            "Federated Round 752/1000\n",
            "Test set: Average loss: 0.4867, Accuracy: 144/178 (81%)\n",
            "Federated Round 753/1000\n",
            "Test set: Average loss: 0.4919, Accuracy: 142/178 (80%)\n",
            "Federated Round 754/1000\n",
            "Test set: Average loss: 0.4919, Accuracy: 142/178 (80%)\n",
            "Federated Round 755/1000\n",
            "Test set: Average loss: 0.4911, Accuracy: 143/178 (80%)\n",
            "Federated Round 756/1000\n",
            "Test set: Average loss: 0.4898, Accuracy: 144/178 (81%)\n",
            "Federated Round 757/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 145/178 (81%)\n",
            "Federated Round 758/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 142/178 (80%)\n",
            "Federated Round 759/1000\n",
            "Test set: Average loss: 0.4954, Accuracy: 142/178 (80%)\n",
            "Federated Round 760/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 145/178 (81%)\n",
            "Federated Round 761/1000\n",
            "Test set: Average loss: 0.4972, Accuracy: 142/178 (80%)\n",
            "Federated Round 762/1000\n",
            "Test set: Average loss: 0.4952, Accuracy: 142/178 (80%)\n",
            "Federated Round 763/1000\n",
            "Test set: Average loss: 0.4946, Accuracy: 142/178 (80%)\n",
            "Federated Round 764/1000\n",
            "Test set: Average loss: 0.4929, Accuracy: 142/178 (80%)\n",
            "Federated Round 765/1000\n",
            "Test set: Average loss: 0.4911, Accuracy: 143/178 (80%)\n",
            "Federated Round 766/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 142/178 (80%)\n",
            "Federated Round 767/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 141/178 (79%)\n",
            "Federated Round 768/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 142/178 (80%)\n",
            "Federated Round 769/1000\n",
            "Test set: Average loss: 0.4967, Accuracy: 142/178 (80%)\n",
            "Federated Round 770/1000\n",
            "Test set: Average loss: 0.4904, Accuracy: 143/178 (80%)\n",
            "Federated Round 771/1000\n",
            "Test set: Average loss: 0.4911, Accuracy: 143/178 (80%)\n",
            "Federated Round 772/1000\n",
            "Test set: Average loss: 0.4940, Accuracy: 142/178 (80%)\n",
            "Federated Round 773/1000\n",
            "Test set: Average loss: 0.4893, Accuracy: 144/178 (81%)\n",
            "Federated Round 774/1000\n",
            "Test set: Average loss: 0.4930, Accuracy: 141/178 (79%)\n",
            "Federated Round 775/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 142/178 (80%)\n",
            "Federated Round 776/1000\n",
            "Test set: Average loss: 0.4927, Accuracy: 141/178 (79%)\n",
            "Federated Round 777/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 141/178 (79%)\n",
            "Federated Round 778/1000\n",
            "Test set: Average loss: 0.4922, Accuracy: 141/178 (79%)\n",
            "Federated Round 779/1000\n",
            "Test set: Average loss: 0.4927, Accuracy: 142/178 (80%)\n",
            "Federated Round 780/1000\n",
            "Test set: Average loss: 0.4931, Accuracy: 142/178 (80%)\n",
            "Federated Round 781/1000\n",
            "Test set: Average loss: 0.4894, Accuracy: 144/178 (81%)\n",
            "Federated Round 782/1000\n",
            "Test set: Average loss: 0.5019, Accuracy: 142/178 (80%)\n",
            "Federated Round 783/1000\n",
            "Test set: Average loss: 0.4890, Accuracy: 144/178 (81%)\n",
            "Federated Round 784/1000\n",
            "Test set: Average loss: 0.4947, Accuracy: 142/178 (80%)\n",
            "Federated Round 785/1000\n",
            "Test set: Average loss: 0.4941, Accuracy: 142/178 (80%)\n",
            "Federated Round 786/1000\n",
            "Test set: Average loss: 0.4904, Accuracy: 145/178 (81%)\n",
            "Federated Round 787/1000\n",
            "Test set: Average loss: 0.4886, Accuracy: 145/178 (81%)\n",
            "Federated Round 788/1000\n",
            "Test set: Average loss: 0.4907, Accuracy: 144/178 (81%)\n",
            "Federated Round 789/1000\n",
            "Test set: Average loss: 0.4902, Accuracy: 143/178 (80%)\n",
            "Federated Round 790/1000\n",
            "Test set: Average loss: 0.4858, Accuracy: 145/178 (81%)\n",
            "Federated Round 791/1000\n",
            "Test set: Average loss: 0.4946, Accuracy: 143/178 (80%)\n",
            "Federated Round 792/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 143/178 (80%)\n",
            "Federated Round 793/1000\n",
            "Test set: Average loss: 0.4952, Accuracy: 143/178 (80%)\n",
            "Federated Round 794/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 142/178 (80%)\n",
            "Federated Round 795/1000\n",
            "Test set: Average loss: 0.4904, Accuracy: 143/178 (80%)\n",
            "Federated Round 796/1000\n",
            "Test set: Average loss: 0.4902, Accuracy: 143/178 (80%)\n",
            "Federated Round 797/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 142/178 (80%)\n",
            "Federated Round 798/1000\n",
            "Test set: Average loss: 0.4890, Accuracy: 144/178 (81%)\n",
            "Federated Round 799/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 144/178 (81%)\n",
            "Federated Round 800/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 143/178 (80%)\n",
            "Federated Round 801/1000\n",
            "Test set: Average loss: 0.4884, Accuracy: 144/178 (81%)\n",
            "Federated Round 802/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 143/178 (80%)\n",
            "Federated Round 803/1000\n",
            "Test set: Average loss: 0.4917, Accuracy: 143/178 (80%)\n",
            "Federated Round 804/1000\n",
            "Test set: Average loss: 0.4939, Accuracy: 142/178 (80%)\n",
            "Federated Round 805/1000\n",
            "Test set: Average loss: 0.4909, Accuracy: 144/178 (81%)\n",
            "Federated Round 806/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 144/178 (81%)\n",
            "Federated Round 807/1000\n",
            "Test set: Average loss: 0.4922, Accuracy: 143/178 (80%)\n",
            "Federated Round 808/1000\n",
            "Test set: Average loss: 0.4934, Accuracy: 144/178 (81%)\n",
            "Federated Round 809/1000\n",
            "Test set: Average loss: 0.4911, Accuracy: 144/178 (81%)\n",
            "Federated Round 810/1000\n",
            "Test set: Average loss: 0.4979, Accuracy: 143/178 (80%)\n",
            "Federated Round 811/1000\n",
            "Test set: Average loss: 0.4886, Accuracy: 144/178 (81%)\n",
            "Federated Round 812/1000\n",
            "Test set: Average loss: 0.4918, Accuracy: 143/178 (80%)\n",
            "Federated Round 813/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 143/178 (80%)\n",
            "Federated Round 814/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 143/178 (80%)\n",
            "Federated Round 815/1000\n",
            "Test set: Average loss: 0.4905, Accuracy: 144/178 (81%)\n",
            "Federated Round 816/1000\n",
            "Test set: Average loss: 0.4917, Accuracy: 143/178 (80%)\n",
            "Federated Round 817/1000\n",
            "Test set: Average loss: 0.4899, Accuracy: 144/178 (81%)\n",
            "Federated Round 818/1000\n",
            "Test set: Average loss: 0.4903, Accuracy: 143/178 (80%)\n",
            "Federated Round 819/1000\n",
            "Test set: Average loss: 0.4901, Accuracy: 143/178 (80%)\n",
            "Federated Round 820/1000\n",
            "Test set: Average loss: 0.4877, Accuracy: 145/178 (81%)\n",
            "Federated Round 821/1000\n",
            "Test set: Average loss: 0.4943, Accuracy: 143/178 (80%)\n",
            "Federated Round 822/1000\n",
            "Test set: Average loss: 0.4881, Accuracy: 145/178 (81%)\n",
            "Federated Round 823/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 143/178 (80%)\n",
            "Federated Round 824/1000\n",
            "Test set: Average loss: 0.4871, Accuracy: 144/178 (81%)\n",
            "Federated Round 825/1000\n",
            "Test set: Average loss: 0.4886, Accuracy: 144/178 (81%)\n",
            "Federated Round 826/1000\n",
            "Test set: Average loss: 0.4876, Accuracy: 145/178 (81%)\n",
            "Federated Round 827/1000\n",
            "Test set: Average loss: 0.4927, Accuracy: 142/178 (80%)\n",
            "Federated Round 828/1000\n",
            "Test set: Average loss: 0.4983, Accuracy: 143/178 (80%)\n",
            "Federated Round 829/1000\n",
            "Test set: Average loss: 0.4943, Accuracy: 143/178 (80%)\n",
            "Federated Round 830/1000\n",
            "Test set: Average loss: 0.4922, Accuracy: 143/178 (80%)\n",
            "Federated Round 831/1000\n",
            "Test set: Average loss: 0.4922, Accuracy: 143/178 (80%)\n",
            "Federated Round 832/1000\n",
            "Test set: Average loss: 0.4924, Accuracy: 143/178 (80%)\n",
            "Federated Round 833/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 143/178 (80%)\n",
            "Federated Round 834/1000\n",
            "Test set: Average loss: 0.4907, Accuracy: 142/178 (80%)\n",
            "Federated Round 835/1000\n",
            "Test set: Average loss: 0.4944, Accuracy: 143/178 (80%)\n",
            "Federated Round 836/1000\n",
            "Test set: Average loss: 0.4936, Accuracy: 143/178 (80%)\n",
            "Federated Round 837/1000\n",
            "Test set: Average loss: 0.4922, Accuracy: 142/178 (80%)\n",
            "Federated Round 838/1000\n",
            "Test set: Average loss: 0.4950, Accuracy: 143/178 (80%)\n",
            "Federated Round 839/1000\n",
            "Test set: Average loss: 0.4930, Accuracy: 143/178 (80%)\n",
            "Federated Round 840/1000\n",
            "Test set: Average loss: 0.4903, Accuracy: 143/178 (80%)\n",
            "Federated Round 841/1000\n",
            "Test set: Average loss: 0.4902, Accuracy: 143/178 (80%)\n",
            "Federated Round 842/1000\n",
            "Test set: Average loss: 0.4917, Accuracy: 142/178 (80%)\n",
            "Federated Round 843/1000\n",
            "Test set: Average loss: 0.4935, Accuracy: 143/178 (80%)\n",
            "Federated Round 844/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 143/178 (80%)\n",
            "Federated Round 845/1000\n",
            "Test set: Average loss: 0.4924, Accuracy: 142/178 (80%)\n",
            "Federated Round 846/1000\n",
            "Test set: Average loss: 0.4904, Accuracy: 142/178 (80%)\n",
            "Federated Round 847/1000\n",
            "Test set: Average loss: 0.4940, Accuracy: 143/178 (80%)\n",
            "Federated Round 848/1000\n",
            "Test set: Average loss: 0.5077, Accuracy: 139/178 (78%)\n",
            "Federated Round 849/1000\n",
            "Test set: Average loss: 0.4864, Accuracy: 144/178 (81%)\n",
            "Federated Round 850/1000\n",
            "Test set: Average loss: 0.4923, Accuracy: 143/178 (80%)\n",
            "Federated Round 851/1000\n",
            "Test set: Average loss: 0.4943, Accuracy: 143/178 (80%)\n",
            "Federated Round 852/1000\n",
            "Test set: Average loss: 0.4933, Accuracy: 143/178 (80%)\n",
            "Federated Round 853/1000\n",
            "Test set: Average loss: 0.4945, Accuracy: 143/178 (80%)\n",
            "Federated Round 854/1000\n",
            "Test set: Average loss: 0.4929, Accuracy: 142/178 (80%)\n",
            "Federated Round 855/1000\n",
            "Test set: Average loss: 0.4936, Accuracy: 142/178 (80%)\n",
            "Federated Round 856/1000\n",
            "Test set: Average loss: 0.4916, Accuracy: 142/178 (80%)\n",
            "Federated Round 857/1000\n",
            "Test set: Average loss: 0.4912, Accuracy: 143/178 (80%)\n",
            "Federated Round 858/1000\n",
            "Test set: Average loss: 0.4910, Accuracy: 143/178 (80%)\n",
            "Federated Round 859/1000\n",
            "Test set: Average loss: 0.4909, Accuracy: 143/178 (80%)\n",
            "Federated Round 860/1000\n",
            "Test set: Average loss: 0.4915, Accuracy: 143/178 (80%)\n",
            "Federated Round 861/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 144/178 (81%)\n",
            "Federated Round 862/1000\n",
            "Test set: Average loss: 0.4933, Accuracy: 143/178 (80%)\n",
            "Federated Round 863/1000\n",
            "Test set: Average loss: 0.4895, Accuracy: 144/178 (81%)\n",
            "Federated Round 864/1000\n",
            "Test set: Average loss: 0.4911, Accuracy: 144/178 (81%)\n",
            "Federated Round 865/1000\n",
            "Test set: Average loss: 0.4929, Accuracy: 143/178 (80%)\n",
            "Federated Round 866/1000\n",
            "Test set: Average loss: 0.4917, Accuracy: 144/178 (81%)\n",
            "Federated Round 867/1000\n",
            "Test set: Average loss: 0.4910, Accuracy: 144/178 (81%)\n",
            "Federated Round 868/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 144/178 (81%)\n",
            "Federated Round 869/1000\n",
            "Test set: Average loss: 0.4946, Accuracy: 144/178 (81%)\n",
            "Federated Round 870/1000\n",
            "Test set: Average loss: 0.4930, Accuracy: 143/178 (80%)\n",
            "Federated Round 871/1000\n",
            "Test set: Average loss: 0.4938, Accuracy: 143/178 (80%)\n",
            "Federated Round 872/1000\n",
            "Test set: Average loss: 0.4931, Accuracy: 143/178 (80%)\n",
            "Federated Round 873/1000\n",
            "Test set: Average loss: 0.4917, Accuracy: 144/178 (81%)\n",
            "Federated Round 874/1000\n",
            "Test set: Average loss: 0.4933, Accuracy: 143/178 (80%)\n",
            "Federated Round 875/1000\n",
            "Test set: Average loss: 0.4950, Accuracy: 143/178 (80%)\n",
            "Federated Round 876/1000\n",
            "Test set: Average loss: 0.4954, Accuracy: 143/178 (80%)\n",
            "Federated Round 877/1000\n",
            "Test set: Average loss: 0.4901, Accuracy: 144/178 (81%)\n",
            "Federated Round 878/1000\n",
            "Test set: Average loss: 0.4910, Accuracy: 144/178 (81%)\n",
            "Federated Round 879/1000\n",
            "Test set: Average loss: 0.4896, Accuracy: 144/178 (81%)\n",
            "Federated Round 880/1000\n",
            "Test set: Average loss: 0.4888, Accuracy: 144/178 (81%)\n",
            "Federated Round 881/1000\n",
            "Test set: Average loss: 0.4916, Accuracy: 143/178 (80%)\n",
            "Federated Round 882/1000\n",
            "Test set: Average loss: 0.4899, Accuracy: 145/178 (81%)\n",
            "Federated Round 883/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 145/178 (81%)\n",
            "Federated Round 884/1000\n",
            "Test set: Average loss: 0.4924, Accuracy: 143/178 (80%)\n",
            "Federated Round 885/1000\n",
            "Test set: Average loss: 0.4895, Accuracy: 145/178 (81%)\n",
            "Federated Round 886/1000\n",
            "Test set: Average loss: 0.4929, Accuracy: 144/178 (81%)\n",
            "Federated Round 887/1000\n",
            "Test set: Average loss: 0.4911, Accuracy: 143/178 (80%)\n",
            "Federated Round 888/1000\n",
            "Test set: Average loss: 0.4914, Accuracy: 143/178 (80%)\n",
            "Federated Round 889/1000\n",
            "Test set: Average loss: 0.4933, Accuracy: 143/178 (80%)\n",
            "Federated Round 890/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 143/178 (80%)\n",
            "Federated Round 891/1000\n",
            "Test set: Average loss: 0.4939, Accuracy: 143/178 (80%)\n",
            "Federated Round 892/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 143/178 (80%)\n",
            "Federated Round 893/1000\n",
            "Test set: Average loss: 0.4936, Accuracy: 144/178 (81%)\n",
            "Federated Round 894/1000\n",
            "Test set: Average loss: 0.4976, Accuracy: 143/178 (80%)\n",
            "Federated Round 895/1000\n",
            "Test set: Average loss: 0.4909, Accuracy: 144/178 (81%)\n",
            "Federated Round 896/1000\n",
            "Test set: Average loss: 0.4906, Accuracy: 143/178 (80%)\n",
            "Federated Round 897/1000\n",
            "Test set: Average loss: 0.4954, Accuracy: 144/178 (81%)\n",
            "Federated Round 898/1000\n",
            "Test set: Average loss: 0.4916, Accuracy: 143/178 (80%)\n",
            "Federated Round 899/1000\n",
            "Test set: Average loss: 0.4933, Accuracy: 143/178 (80%)\n",
            "Federated Round 900/1000\n",
            "Test set: Average loss: 0.4932, Accuracy: 143/178 (80%)\n",
            "Federated Round 901/1000\n",
            "Test set: Average loss: 0.4938, Accuracy: 143/178 (80%)\n",
            "Federated Round 902/1000\n",
            "Test set: Average loss: 0.4950, Accuracy: 145/178 (81%)\n",
            "Federated Round 903/1000\n",
            "Test set: Average loss: 0.4947, Accuracy: 145/178 (81%)\n",
            "Federated Round 904/1000\n",
            "Test set: Average loss: 0.4944, Accuracy: 144/178 (81%)\n",
            "Federated Round 905/1000\n",
            "Test set: Average loss: 0.4945, Accuracy: 144/178 (81%)\n",
            "Federated Round 906/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 144/178 (81%)\n",
            "Federated Round 907/1000\n",
            "Test set: Average loss: 0.4930, Accuracy: 144/178 (81%)\n",
            "Federated Round 908/1000\n",
            "Test set: Average loss: 0.4956, Accuracy: 144/178 (81%)\n",
            "Federated Round 909/1000\n",
            "Test set: Average loss: 0.4918, Accuracy: 143/178 (80%)\n",
            "Federated Round 910/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 143/178 (80%)\n",
            "Federated Round 911/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 144/178 (81%)\n",
            "Federated Round 912/1000\n",
            "Test set: Average loss: 0.4910, Accuracy: 143/178 (80%)\n",
            "Federated Round 913/1000\n",
            "Test set: Average loss: 0.4876, Accuracy: 144/178 (81%)\n",
            "Federated Round 914/1000\n",
            "Test set: Average loss: 0.4946, Accuracy: 144/178 (81%)\n",
            "Federated Round 915/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 144/178 (81%)\n",
            "Federated Round 916/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 143/178 (80%)\n",
            "Federated Round 917/1000\n",
            "Test set: Average loss: 0.4934, Accuracy: 143/178 (80%)\n",
            "Federated Round 918/1000\n",
            "Test set: Average loss: 0.4952, Accuracy: 143/178 (80%)\n",
            "Federated Round 919/1000\n",
            "Test set: Average loss: 0.4968, Accuracy: 143/178 (80%)\n",
            "Federated Round 920/1000\n",
            "Test set: Average loss: 0.4970, Accuracy: 143/178 (80%)\n",
            "Federated Round 921/1000\n",
            "Test set: Average loss: 0.4967, Accuracy: 143/178 (80%)\n",
            "Federated Round 922/1000\n",
            "Test set: Average loss: 0.4965, Accuracy: 143/178 (80%)\n",
            "Federated Round 923/1000\n",
            "Test set: Average loss: 0.5015, Accuracy: 144/178 (81%)\n",
            "Federated Round 924/1000\n",
            "Test set: Average loss: 0.4930, Accuracy: 142/178 (80%)\n",
            "Federated Round 925/1000\n",
            "Test set: Average loss: 0.4974, Accuracy: 143/178 (80%)\n",
            "Federated Round 926/1000\n",
            "Test set: Average loss: 0.4891, Accuracy: 144/178 (81%)\n",
            "Federated Round 927/1000\n",
            "Test set: Average loss: 0.4935, Accuracy: 144/178 (81%)\n",
            "Federated Round 928/1000\n",
            "Test set: Average loss: 0.4908, Accuracy: 143/178 (80%)\n",
            "Federated Round 929/1000\n",
            "Test set: Average loss: 0.4932, Accuracy: 144/178 (81%)\n",
            "Federated Round 930/1000\n",
            "Test set: Average loss: 0.4932, Accuracy: 144/178 (81%)\n",
            "Federated Round 931/1000\n",
            "Test set: Average loss: 0.4932, Accuracy: 143/178 (80%)\n",
            "Federated Round 932/1000\n",
            "Test set: Average loss: 0.4951, Accuracy: 144/178 (81%)\n",
            "Federated Round 933/1000\n",
            "Test set: Average loss: 0.4920, Accuracy: 143/178 (80%)\n",
            "Federated Round 934/1000\n",
            "Test set: Average loss: 0.4921, Accuracy: 143/178 (80%)\n",
            "Federated Round 935/1000\n",
            "Test set: Average loss: 0.4910, Accuracy: 144/178 (81%)\n",
            "Federated Round 936/1000\n",
            "Test set: Average loss: 0.4925, Accuracy: 143/178 (80%)\n",
            "Federated Round 937/1000\n",
            "Test set: Average loss: 0.4962, Accuracy: 144/178 (81%)\n",
            "Federated Round 938/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 143/178 (80%)\n",
            "Federated Round 939/1000\n",
            "Test set: Average loss: 0.4985, Accuracy: 143/178 (80%)\n",
            "Federated Round 940/1000\n",
            "Test set: Average loss: 0.4949, Accuracy: 144/178 (81%)\n",
            "Federated Round 941/1000\n",
            "Test set: Average loss: 0.4957, Accuracy: 144/178 (81%)\n",
            "Federated Round 942/1000\n",
            "Test set: Average loss: 0.4975, Accuracy: 143/178 (80%)\n",
            "Federated Round 943/1000\n",
            "Test set: Average loss: 0.4988, Accuracy: 143/178 (80%)\n",
            "Federated Round 944/1000\n",
            "Test set: Average loss: 0.4988, Accuracy: 143/178 (80%)\n",
            "Federated Round 945/1000\n",
            "Test set: Average loss: 0.4942, Accuracy: 143/178 (80%)\n",
            "Federated Round 946/1000\n",
            "Test set: Average loss: 0.5003, Accuracy: 143/178 (80%)\n",
            "Federated Round 947/1000\n",
            "Test set: Average loss: 0.4928, Accuracy: 143/178 (80%)\n",
            "Federated Round 948/1000\n",
            "Test set: Average loss: 0.4950, Accuracy: 143/178 (80%)\n",
            "Federated Round 949/1000\n",
            "Test set: Average loss: 0.4938, Accuracy: 144/178 (81%)\n",
            "Federated Round 950/1000\n",
            "Test set: Average loss: 0.5169, Accuracy: 139/178 (78%)\n",
            "Federated Round 951/1000\n",
            "Test set: Average loss: 0.4919, Accuracy: 144/178 (81%)\n",
            "Federated Round 952/1000\n",
            "Test set: Average loss: 0.4870, Accuracy: 144/178 (81%)\n",
            "Federated Round 953/1000\n",
            "Test set: Average loss: 0.4914, Accuracy: 144/178 (81%)\n",
            "Federated Round 954/1000\n",
            "Test set: Average loss: 0.4928, Accuracy: 144/178 (81%)\n",
            "Federated Round 955/1000\n",
            "Test set: Average loss: 0.4897, Accuracy: 143/178 (80%)\n",
            "Federated Round 956/1000\n",
            "Test set: Average loss: 0.4932, Accuracy: 143/178 (80%)\n",
            "Federated Round 957/1000\n",
            "Test set: Average loss: 0.4889, Accuracy: 144/178 (81%)\n",
            "Federated Round 958/1000\n",
            "Test set: Average loss: 0.4963, Accuracy: 143/178 (80%)\n",
            "Federated Round 959/1000\n",
            "Test set: Average loss: 0.5024, Accuracy: 144/178 (81%)\n",
            "Federated Round 960/1000\n",
            "Test set: Average loss: 0.4961, Accuracy: 143/178 (80%)\n",
            "Federated Round 961/1000\n",
            "Test set: Average loss: 0.4998, Accuracy: 144/178 (81%)\n",
            "Federated Round 962/1000\n",
            "Test set: Average loss: 0.4971, Accuracy: 143/178 (80%)\n",
            "Federated Round 963/1000\n",
            "Test set: Average loss: 0.4993, Accuracy: 144/178 (81%)\n",
            "Federated Round 964/1000\n",
            "Test set: Average loss: 0.4969, Accuracy: 143/178 (80%)\n",
            "Federated Round 965/1000\n",
            "Test set: Average loss: 0.5001, Accuracy: 143/178 (80%)\n",
            "Federated Round 966/1000\n",
            "Test set: Average loss: 0.4919, Accuracy: 144/178 (81%)\n",
            "Federated Round 967/1000\n",
            "Test set: Average loss: 0.4961, Accuracy: 143/178 (80%)\n",
            "Federated Round 968/1000\n",
            "Test set: Average loss: 0.4991, Accuracy: 143/178 (80%)\n",
            "Federated Round 969/1000\n",
            "Test set: Average loss: 0.4958, Accuracy: 143/178 (80%)\n",
            "Federated Round 970/1000\n",
            "Test set: Average loss: 0.4988, Accuracy: 143/178 (80%)\n",
            "Federated Round 971/1000\n",
            "Test set: Average loss: 0.4952, Accuracy: 143/178 (80%)\n",
            "Federated Round 972/1000\n",
            "Test set: Average loss: 0.4917, Accuracy: 143/178 (80%)\n",
            "Federated Round 973/1000\n",
            "Test set: Average loss: 0.4913, Accuracy: 143/178 (80%)\n",
            "Federated Round 974/1000\n",
            "Test set: Average loss: 0.4917, Accuracy: 143/178 (80%)\n",
            "Federated Round 975/1000\n",
            "Test set: Average loss: 0.4953, Accuracy: 143/178 (80%)\n",
            "Federated Round 976/1000\n",
            "Test set: Average loss: 0.4953, Accuracy: 143/178 (80%)\n",
            "Federated Round 977/1000\n",
            "Test set: Average loss: 0.4968, Accuracy: 143/178 (80%)\n",
            "Federated Round 978/1000\n",
            "Test set: Average loss: 0.4984, Accuracy: 143/178 (80%)\n",
            "Federated Round 979/1000\n",
            "Test set: Average loss: 0.4940, Accuracy: 143/178 (80%)\n",
            "Federated Round 980/1000\n",
            "Test set: Average loss: 0.4995, Accuracy: 143/178 (80%)\n",
            "Federated Round 981/1000\n",
            "Test set: Average loss: 0.4934, Accuracy: 142/178 (80%)\n",
            "Federated Round 982/1000\n",
            "Test set: Average loss: 0.4979, Accuracy: 143/178 (80%)\n",
            "Federated Round 983/1000\n",
            "Test set: Average loss: 0.4967, Accuracy: 143/178 (80%)\n",
            "Federated Round 984/1000\n",
            "Test set: Average loss: 0.5001, Accuracy: 143/178 (80%)\n",
            "Federated Round 985/1000\n",
            "Test set: Average loss: 0.4983, Accuracy: 143/178 (80%)\n",
            "Federated Round 986/1000\n",
            "Test set: Average loss: 0.4954, Accuracy: 143/178 (80%)\n",
            "Federated Round 987/1000\n",
            "Test set: Average loss: 0.4957, Accuracy: 143/178 (80%)\n",
            "Federated Round 988/1000\n",
            "Test set: Average loss: 0.4943, Accuracy: 142/178 (80%)\n",
            "Federated Round 989/1000\n",
            "Test set: Average loss: 0.4949, Accuracy: 142/178 (80%)\n",
            "Federated Round 990/1000\n",
            "Test set: Average loss: 0.4951, Accuracy: 142/178 (80%)\n",
            "Federated Round 991/1000\n",
            "Test set: Average loss: 0.4965, Accuracy: 143/178 (80%)\n",
            "Federated Round 992/1000\n",
            "Test set: Average loss: 0.4962, Accuracy: 143/178 (80%)\n",
            "Federated Round 993/1000\n",
            "Test set: Average loss: 0.4955, Accuracy: 143/178 (80%)\n",
            "Federated Round 994/1000\n",
            "Test set: Average loss: 0.4966, Accuracy: 143/178 (80%)\n",
            "Federated Round 995/1000\n",
            "Test set: Average loss: 0.4955, Accuracy: 143/178 (80%)\n",
            "Federated Round 996/1000\n",
            "Test set: Average loss: 0.4950, Accuracy: 143/178 (80%)\n",
            "Federated Round 997/1000\n",
            "Test set: Average loss: 0.4983, Accuracy: 143/178 (80%)\n",
            "Federated Round 998/1000\n",
            "Test set: Average loss: 0.4971, Accuracy: 143/178 (80%)\n",
            "Federated Round 999/1000\n",
            "Test set: Average loss: 0.4996, Accuracy: 143/178 (80%)\n",
            "Federated Round 1000/1000\n",
            "Test set: Average loss: 0.4995, Accuracy: 143/178 (80%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7d605017f940>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vertical Split before one-hot encoding\n"
      ],
      "metadata": {
        "id": "b49xMYlX2RAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "seed = 55\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def load_titanic_data():\n",
        "    def _bin_age(age_series):\n",
        "        bins = [-np.inf, 10, 40, np.inf]\n",
        "        labels = [\"Child\", \"Adult\", \"Elderly\"]\n",
        "        return pd.cut(age_series, bins=bins, labels=labels, right=True).astype(str).replace(\"nan\", \"Unknown\")\n",
        "\n",
        "    def _extract_title(name_series):\n",
        "        titles = name_series.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
        "        rare_titles = {\n",
        "            \"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"\n",
        "        }\n",
        "        titles = titles.replace(list(rare_titles), \"Rare\")\n",
        "        titles = titles.replace({\"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\"})\n",
        "        return titles\n",
        "\n",
        "    def _create_features(df):\n",
        "        df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\")\n",
        "        df[\"Age\"] = _bin_age(df[\"Age\"])\n",
        "        df[\"Cabin\"] = df[\"Cabin\"].str[0].fillna(\"Unknown\")\n",
        "        df[\"Title\"] = _extract_title(df[\"Name\"])\n",
        "        df.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"], inplace=True)\n",
        "        return df\n",
        "\n",
        "    def vertical_partition_rotating(df, num_participants):\n",
        "        partitions = [[] for _ in range(num_participants)]\n",
        "        num_features = df.shape[1]\n",
        "\n",
        "        # Now Round-Robin Vertical distribution of features before one-hot encoding\n",
        "        for i, feature in enumerate(df.columns):\n",
        "            participant = i % num_participants\n",
        "            partitions[participant].append(feature)\n",
        "\n",
        "        # Raw features distribution\n",
        "        for i, partition in enumerate(partitions):\n",
        "            print(f\"Client {i + 1} Raw Features: {partition}\")\n",
        "\n",
        "        # One-hot encoding each partition independently\n",
        "        partitioned_dfs = [pd.get_dummies(df[features]) for features in partitions]\n",
        "\n",
        "        # Final feature set for each client after one-hot encoding\n",
        "        for i, partition in enumerate(partitioned_dfs):\n",
        "            print(f\"Client {i + 1} Final Features after One-Hot Encoding: {partition.columns.tolist()}\")\n",
        "\n",
        "        return partitioned_dfs\n",
        "\n",
        "    def get_partitions_and_label():\n",
        "        df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "        processed_df = df.dropna(subset=[\"Embarked\", \"Fare\"]).copy()\n",
        "        processed_df = _create_features(processed_df)\n",
        "        labels = processed_df[\"Survived\"].values\n",
        "\n",
        "        # Train and test data split\n",
        "        train_df, test_df, y_train, y_test = train_test_split(processed_df, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Vertical_partitioning_rotating function\n",
        "        train_partitions = vertical_partition_rotating(train_df.drop(columns=[\"Survived\"]), 3)\n",
        "        test_partitions = vertical_partition_rotating(test_df.drop(columns=[\"Survived\"]), 3)\n",
        "\n",
        "        # Ensuring each participant has access to labels\n",
        "        for i in range(len(train_partitions)):\n",
        "            train_partitions[i]['Survived'] = y_train\n",
        "\n",
        "        for i in range(len(test_partitions)):\n",
        "            test_partitions[i]['Survived'] = y_test\n",
        "\n",
        "        return train_partitions, test_partitions, y_train, y_test\n",
        "\n",
        "    # Partitions and Labels\n",
        "    train_partitions, test_partitions, y_train, y_test = get_partitions_and_label()\n",
        "\n",
        "    # Partitions to PyTorch datasets conversions\n",
        "    def create_tensor_datasets(partitions):\n",
        "        tensor_partitions = []\n",
        "        for partition in partitions:\n",
        "            partition = partition.apply(pd.to_numeric, errors='coerce')\n",
        "            partition = partition.fillna(0)\n",
        "\n",
        "            for col in partition.select_dtypes(include=['bool']).columns:\n",
        "                partition[col] = partition[col].astype(int)\n",
        "\n",
        "            features = partition.drop(columns=[\"Survived\"]).values\n",
        "            labels = partition[\"Survived\"].values.astype(np.int64)\n",
        "\n",
        "            tensor_partition = TensorDataset(torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long))\n",
        "            tensor_partitions.append(tensor_partition)\n",
        "        return tensor_partitions\n",
        "\n",
        "    train_tensor_partitions = create_tensor_datasets(train_partitions)\n",
        "    test_tensor_partitions = create_tensor_datasets(test_partitions)\n",
        "\n",
        "    return train_tensor_partitions, test_tensor_partitions, train_partitions, y_train, y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Global Model definition like before\n",
        "class GlobalModel(nn.Module):\n",
        "    def __init__(self, input_sizes, hidden_sizes, output_size):\n",
        "        super(GlobalModel, self).__init__()\n",
        "        self.segments = nn.ModuleList()\n",
        "        for input_size, hidden_size in zip(input_sizes, hidden_sizes):\n",
        "            layers = [nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, output_size)]\n",
        "            self.segments.append(nn.Sequential(*layers))\n",
        "\n",
        "    def forward(self, x, active_segments):\n",
        "        segment_outputs = []\n",
        "        start_index = 0\n",
        "        for i, segment in enumerate(self.segments):\n",
        "            end_index = start_index + input_sizes[i]\n",
        "            if i in active_segments:\n",
        "                segment_input = x[:, start_index:end_index]  # We extract the segment of the input corresponding to the active participant's features only\n",
        "                segment_output = segment(segment_input)\n",
        "                segment_outputs.append(segment_output)\n",
        "            else:\n",
        "                segment_outputs.append(torch.zeros(x.size(0), output_size, device=x.device)) # Non-active segments are appended with zero\n",
        "            start_index = end_index\n",
        "        combined_output = torch.mean(torch.stack(segment_outputs), dim=0)\n",
        "        return combined_output\n",
        "\n",
        "# Dataset\n",
        "train_tensor_partitions, test_tensor_partitions, feature_partitions, y_train, y_test = load_titanic_data()\n",
        "\n",
        "# Input sizes after one-hot encoding\n",
        "input_sizes = [partition.shape[1] - 1 for partition in feature_partitions]  # minus 1 for 'Survived' column\n",
        "print(\"Input sizes:\", input_sizes)\n",
        "hidden_sizes = [10] * 3\n",
        "output_size = 2 # survive or dead?\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "models = [GlobalModel(input_sizes, hidden_sizes, output_size).to(device) for _ in range(3)]\n",
        "optimizers = [optim.Adam(model.parameters(), lr=0.01) for model in models]\n",
        "\n",
        "# Class weights due to imbalance class distribution\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train(model, device, train_loader, optimizer, epoch, input_sizes, participant_id):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        data = data.view(data.size(0), -1)\n",
        "        # Zero-padding non-responsible features\n",
        "        padded_data = torch.zeros(data.size(0), sum(input_sizes)).to(device)\n",
        "        start_index = sum(input_sizes[:participant_id])\n",
        "        end_index = start_index + input_sizes[participant_id]\n",
        "        padded_data[:, start_index:end_index] = data\n",
        "        optimizer.zero_grad()\n",
        "        output = model(padded_data, active_segments=[participant_id])\n",
        "        loss = nn.CrossEntropyLoss(weight=class_weights)(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #if batch_idx % 10 == 0:\n",
        "            #print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "\n",
        "\n",
        "# Selective gradient exchange function\n",
        "def selective_exchange_gradients(models, input_sizes, hidden_sizes):\n",
        "    num_models = len(models)\n",
        "    param_indices = [0]\n",
        "    cumulative_index = 0\n",
        "    for i in range(len(hidden_sizes)):\n",
        "        cumulative_index += (input_sizes[i] * hidden_sizes[i]) + hidden_sizes[i]\n",
        "        param_indices.append(cumulative_index)\n",
        "\n",
        "    for seg in range(len(hidden_sizes)):\n",
        "        start = param_indices[seg]\n",
        "        end = param_indices[seg + 1]\n",
        "        for param_idx in range(start, end):\n",
        "            grads = []\n",
        "            for model in models:\n",
        "                model_params = list(model.parameters())\n",
        "                if param_idx < len(model_params) and model_params[param_idx].grad is not None:\n",
        "                    grads.append(model_params[param_idx].grad)\n",
        "            if grads:\n",
        "                avg_grad = torch.stack(grads).mean(dim=0)\n",
        "                for model in models:\n",
        "                    model_params = list(model.parameters())\n",
        "                    if param_idx < len(model_params):\n",
        "                        model_params[param_idx].grad = avg_grad.clone()\n",
        "\n",
        "# Model Training\n",
        "federated_rounds = 1000  # To match with their setup\n",
        "epochs_per_round = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Lists to store accuracy after each round\n",
        "vfl_accuracies = []\n",
        "\n",
        "def evaluate(models, device, test_loaders):\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        for batch_data in zip(*test_loaders):\n",
        "            data_list = []\n",
        "            target_list = []\n",
        "            for participant_id, (data, target) in enumerate(batch_data):\n",
        "                data_list.append(data)\n",
        "                target_list.append(target)\n",
        "\n",
        "            target = target_list[0].to(device)\n",
        "            for t in target_list:\n",
        "                assert torch.equal(t, target), \"Targets are not consistent across participants\"\n",
        "\n",
        "            data_combined = torch.cat(data_list, dim=1).to(device)\n",
        "\n",
        "\n",
        "            #print(\"data_combined size:\", data_combined.size())\n",
        "            padded_data = torch.zeros(data_combined.size(0), sum(input_sizes)).to(device)\n",
        "            #print(\"padded_data size before padding:\", padded_data.size())\n",
        "\n",
        "            start_index = 0\n",
        "            for participant_id in range(len(input_sizes)):\n",
        "                end_index = start_index + input_sizes[participant_id]\n",
        "                if end_index <= data_combined.size(1):\n",
        "                    #print(f\"Padded data: start_index={start_index}, end_index={end_index}, data_combined[:, {start_index}:{end_index}].size()={data_combined[:, start_index:end_index].size()}\")\n",
        "                    padded_data[:, start_index:end_index] = data_combined[:, start_index:end_index]\n",
        "                else:\n",
        "\n",
        "                    adjusted_end_index = data_combined.size(1)\n",
        "                    #print(f\"Padded data: start_index={start_index}, adjusted_end_index={adjusted_end_index}, data_combined[:, {start_index}:{adjusted_end_index}].size()={data_combined[:, start_index:adjusted_end_index].size()}\")\n",
        "                    padded_data[:, start_index:adjusted_end_index] = data_combined[:, start_index:adjusted_end_index]\n",
        "                start_index = end_index\n",
        "\n",
        "            #print(\"padded_data size after padding:\", padded_data.size())\n",
        "\n",
        "            outputs = torch.zeros(data_combined.size(0), 2, device=device)\n",
        "            for model in models:\n",
        "                output = model(padded_data, active_segments=list(range(len(model.segments))))\n",
        "                outputs += output\n",
        "            outputs /= len(models)\n",
        "            test_loss += nn.CrossEntropyLoss(reduction='sum')(outputs, target).item()\n",
        "            pred = outputs.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            all_preds.extend(pred.view(-1).cpu().numpy())\n",
        "            all_targets.extend(target.view(-1).cpu().numpy())\n",
        "\n",
        "        test_loss /= len(test_loaders[0].dataset)\n",
        "        accuracy = 100. * correct / len(test_loaders[0].dataset)\n",
        "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loaders[0].dataset)} ({accuracy:.0f}%)')\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "# Rest of the code remains the same as previous scenario\n",
        "\n",
        "\n",
        "for federated_round in range(federated_rounds):\n",
        "    print(f\"Federated Round {federated_round + 1}/{federated_rounds}\")\n",
        "    for participant_id in range(3):\n",
        "        #print(f\"  Training Participant {participant_id + 1}\")\n",
        "        train_loader = DataLoader(train_tensor_partitions[participant_id], batch_size=64, shuffle=True)\n",
        "        for epoch in range(1, epochs_per_round + 1):\n",
        "            train(models[participant_id], device, train_loader, optimizers[participant_id], epoch, input_sizes, participant_id)\n",
        "\n",
        "    selective_exchange_gradients(models, input_sizes, hidden_sizes)\n",
        "\n",
        "    # Accuracy after each round\n",
        "    test_loaders = [DataLoader(test_tensor_partitions[i], batch_size=32, shuffle=False) for i in range(3)]\n",
        "    accuracy = evaluate(models, device, test_loaders)\n",
        "    vfl_accuracies.append(accuracy)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# VFL Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(federated_rounds), vfl_accuracies, label='VFL Accuracy')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('VFL Accuracy over Rounds')\n",
        "plt.ylim([0, 100])\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jSKSwdj8tpmZ",
        "outputId": "64d688e8-ff07-471f-b855-d4c2cf38d945"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1 Raw Features: ['Pclass', 'SibSp', 'Cabin']\n",
            "Client 2 Raw Features: ['Sex', 'Parch', 'Embarked']\n",
            "Client 3 Raw Features: ['Age', 'Fare', 'Title']\n",
            "Client 1 Final Features after One-Hot Encoding: ['Pclass', 'SibSp', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_Unknown']\n",
            "Client 2 Final Features after One-Hot Encoding: ['Parch', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
            "Client 3 Final Features after One-Hot Encoding: ['Fare', 'Age_Adult', 'Age_Child', 'Age_Elderly', 'Age_Unknown', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Rare']\n",
            "Client 1 Raw Features: ['Pclass', 'SibSp', 'Cabin']\n",
            "Client 2 Raw Features: ['Sex', 'Parch', 'Embarked']\n",
            "Client 3 Raw Features: ['Age', 'Fare', 'Title']\n",
            "Client 1 Final Features after One-Hot Encoding: ['Pclass', 'SibSp', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_Unknown']\n",
            "Client 2 Final Features after One-Hot Encoding: ['Parch', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
            "Client 3 Final Features after One-Hot Encoding: ['Fare', 'Age_Adult', 'Age_Child', 'Age_Elderly', 'Age_Unknown', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Rare']\n",
            "Input sizes: [11, 6, 10]\n",
            "Federated Round 1/1000\n",
            "Test set: Average loss: 1.0073, Accuracy: 108/178 (61%)\n",
            "Federated Round 2/1000\n",
            "Test set: Average loss: 1.1772, Accuracy: 108/178 (61%)\n",
            "Federated Round 3/1000\n",
            "Test set: Average loss: 1.4695, Accuracy: 108/178 (61%)\n",
            "Federated Round 4/1000\n",
            "Test set: Average loss: 1.7929, Accuracy: 108/178 (61%)\n",
            "Federated Round 5/1000\n",
            "Test set: Average loss: 2.0879, Accuracy: 108/178 (61%)\n",
            "Federated Round 6/1000\n",
            "Test set: Average loss: 2.3397, Accuracy: 108/178 (61%)\n",
            "Federated Round 7/1000\n",
            "Test set: Average loss: 2.0852, Accuracy: 108/178 (61%)\n",
            "Federated Round 8/1000\n",
            "Test set: Average loss: 2.2149, Accuracy: 108/178 (61%)\n",
            "Federated Round 9/1000\n",
            "Test set: Average loss: 2.1737, Accuracy: 108/178 (61%)\n",
            "Federated Round 10/1000\n",
            "Test set: Average loss: 2.1034, Accuracy: 104/178 (58%)\n",
            "Federated Round 11/1000\n",
            "Test set: Average loss: 1.9186, Accuracy: 104/178 (58%)\n",
            "Federated Round 12/1000\n",
            "Test set: Average loss: 1.8575, Accuracy: 104/178 (58%)\n",
            "Federated Round 13/1000\n",
            "Test set: Average loss: 1.9733, Accuracy: 104/178 (58%)\n",
            "Federated Round 14/1000\n",
            "Test set: Average loss: 1.8962, Accuracy: 104/178 (58%)\n",
            "Federated Round 15/1000\n",
            "Test set: Average loss: 1.7218, Accuracy: 93/178 (52%)\n",
            "Federated Round 16/1000\n",
            "Test set: Average loss: 1.7242, Accuracy: 93/178 (52%)\n",
            "Federated Round 17/1000\n",
            "Test set: Average loss: 2.0927, Accuracy: 104/178 (58%)\n",
            "Federated Round 18/1000\n",
            "Test set: Average loss: 1.9116, Accuracy: 104/178 (58%)\n",
            "Federated Round 19/1000\n",
            "Test set: Average loss: 1.5355, Accuracy: 77/178 (43%)\n",
            "Federated Round 20/1000\n",
            "Test set: Average loss: 1.8420, Accuracy: 103/178 (58%)\n",
            "Federated Round 21/1000\n",
            "Test set: Average loss: 1.7357, Accuracy: 102/178 (57%)\n",
            "Federated Round 22/1000\n",
            "Test set: Average loss: 1.9253, Accuracy: 104/178 (58%)\n",
            "Federated Round 23/1000\n",
            "Test set: Average loss: 2.1757, Accuracy: 106/178 (60%)\n",
            "Federated Round 24/1000\n",
            "Test set: Average loss: 2.0258, Accuracy: 104/178 (58%)\n",
            "Federated Round 25/1000\n",
            "Test set: Average loss: 1.8640, Accuracy: 104/178 (58%)\n",
            "Federated Round 26/1000\n",
            "Test set: Average loss: 1.8730, Accuracy: 104/178 (58%)\n",
            "Federated Round 27/1000\n",
            "Test set: Average loss: 1.9347, Accuracy: 106/178 (60%)\n",
            "Federated Round 28/1000\n",
            "Test set: Average loss: 1.9604, Accuracy: 105/178 (59%)\n",
            "Federated Round 29/1000\n",
            "Test set: Average loss: 2.0909, Accuracy: 106/178 (60%)\n",
            "Federated Round 30/1000\n",
            "Test set: Average loss: 2.3218, Accuracy: 108/178 (61%)\n",
            "Federated Round 31/1000\n",
            "Test set: Average loss: 2.0690, Accuracy: 106/178 (60%)\n",
            "Federated Round 32/1000\n",
            "Test set: Average loss: 2.2329, Accuracy: 108/178 (61%)\n",
            "Federated Round 33/1000\n",
            "Test set: Average loss: 2.3971, Accuracy: 108/178 (61%)\n",
            "Federated Round 34/1000\n",
            "Test set: Average loss: 2.1499, Accuracy: 108/178 (61%)\n",
            "Federated Round 35/1000\n",
            "Test set: Average loss: 1.8852, Accuracy: 108/178 (61%)\n",
            "Federated Round 36/1000\n",
            "Test set: Average loss: 2.0128, Accuracy: 108/178 (61%)\n",
            "Federated Round 37/1000\n",
            "Test set: Average loss: 2.2252, Accuracy: 108/178 (61%)\n",
            "Federated Round 38/1000\n",
            "Test set: Average loss: 2.2398, Accuracy: 108/178 (61%)\n",
            "Federated Round 39/1000\n",
            "Test set: Average loss: 2.0087, Accuracy: 108/178 (61%)\n",
            "Federated Round 40/1000\n",
            "Test set: Average loss: 1.9782, Accuracy: 108/178 (61%)\n",
            "Federated Round 41/1000\n",
            "Test set: Average loss: 2.1035, Accuracy: 108/178 (61%)\n",
            "Federated Round 42/1000\n",
            "Test set: Average loss: 2.1117, Accuracy: 108/178 (61%)\n",
            "Federated Round 43/1000\n",
            "Test set: Average loss: 1.9107, Accuracy: 108/178 (61%)\n",
            "Federated Round 44/1000\n",
            "Test set: Average loss: 2.0316, Accuracy: 108/178 (61%)\n",
            "Federated Round 45/1000\n",
            "Test set: Average loss: 2.3321, Accuracy: 108/178 (61%)\n",
            "Federated Round 46/1000\n",
            "Test set: Average loss: 2.4495, Accuracy: 108/178 (61%)\n",
            "Federated Round 47/1000\n",
            "Test set: Average loss: 2.1803, Accuracy: 108/178 (61%)\n",
            "Federated Round 48/1000\n",
            "Test set: Average loss: 1.9752, Accuracy: 108/178 (61%)\n",
            "Federated Round 49/1000\n",
            "Test set: Average loss: 1.9862, Accuracy: 108/178 (61%)\n",
            "Federated Round 50/1000\n",
            "Test set: Average loss: 2.4221, Accuracy: 108/178 (61%)\n",
            "Federated Round 51/1000\n",
            "Test set: Average loss: 2.0974, Accuracy: 108/178 (61%)\n",
            "Federated Round 52/1000\n",
            "Test set: Average loss: 2.0011, Accuracy: 107/178 (60%)\n",
            "Federated Round 53/1000\n",
            "Test set: Average loss: 1.8974, Accuracy: 107/178 (60%)\n",
            "Federated Round 54/1000\n",
            "Test set: Average loss: 2.0388, Accuracy: 108/178 (61%)\n",
            "Federated Round 55/1000\n",
            "Test set: Average loss: 1.8257, Accuracy: 107/178 (60%)\n",
            "Federated Round 56/1000\n",
            "Test set: Average loss: 1.7992, Accuracy: 107/178 (60%)\n",
            "Federated Round 57/1000\n",
            "Test set: Average loss: 1.9495, Accuracy: 108/178 (61%)\n",
            "Federated Round 58/1000\n",
            "Test set: Average loss: 2.0425, Accuracy: 108/178 (61%)\n",
            "Federated Round 59/1000\n",
            "Test set: Average loss: 1.9277, Accuracy: 108/178 (61%)\n",
            "Federated Round 60/1000\n",
            "Test set: Average loss: 1.9449, Accuracy: 108/178 (61%)\n",
            "Federated Round 61/1000\n",
            "Test set: Average loss: 1.7038, Accuracy: 106/178 (60%)\n",
            "Federated Round 62/1000\n",
            "Test set: Average loss: 1.7980, Accuracy: 106/178 (60%)\n",
            "Federated Round 63/1000\n",
            "Test set: Average loss: 1.9778, Accuracy: 108/178 (61%)\n",
            "Federated Round 64/1000\n",
            "Test set: Average loss: 1.9686, Accuracy: 108/178 (61%)\n",
            "Federated Round 65/1000\n",
            "Test set: Average loss: 1.8145, Accuracy: 106/178 (60%)\n",
            "Federated Round 66/1000\n",
            "Test set: Average loss: 1.4949, Accuracy: 106/178 (60%)\n",
            "Federated Round 67/1000\n",
            "Test set: Average loss: 1.9891, Accuracy: 108/178 (61%)\n",
            "Federated Round 68/1000\n",
            "Test set: Average loss: 1.9380, Accuracy: 108/178 (61%)\n",
            "Federated Round 69/1000\n",
            "Test set: Average loss: 1.9120, Accuracy: 108/178 (61%)\n",
            "Federated Round 70/1000\n",
            "Test set: Average loss: 1.8295, Accuracy: 108/178 (61%)\n",
            "Federated Round 71/1000\n",
            "Test set: Average loss: 1.7319, Accuracy: 106/178 (60%)\n",
            "Federated Round 72/1000\n",
            "Test set: Average loss: 1.5654, Accuracy: 106/178 (60%)\n",
            "Federated Round 73/1000\n",
            "Test set: Average loss: 1.7644, Accuracy: 108/178 (61%)\n",
            "Federated Round 74/1000\n",
            "Test set: Average loss: 1.7527, Accuracy: 108/178 (61%)\n",
            "Federated Round 75/1000\n",
            "Test set: Average loss: 1.8023, Accuracy: 108/178 (61%)\n",
            "Federated Round 76/1000\n",
            "Test set: Average loss: 1.7591, Accuracy: 109/178 (61%)\n",
            "Federated Round 77/1000\n",
            "Test set: Average loss: 1.8335, Accuracy: 108/178 (61%)\n",
            "Federated Round 78/1000\n",
            "Test set: Average loss: 2.1305, Accuracy: 109/178 (61%)\n",
            "Federated Round 79/1000\n",
            "Test set: Average loss: 1.9188, Accuracy: 107/178 (60%)\n",
            "Federated Round 80/1000\n",
            "Test set: Average loss: 1.6297, Accuracy: 107/178 (60%)\n",
            "Federated Round 81/1000\n",
            "Test set: Average loss: 1.7765, Accuracy: 108/178 (61%)\n",
            "Federated Round 82/1000\n",
            "Test set: Average loss: 1.4728, Accuracy: 108/178 (61%)\n",
            "Federated Round 83/1000\n",
            "Test set: Average loss: 1.6379, Accuracy: 109/178 (61%)\n",
            "Federated Round 84/1000\n",
            "Test set: Average loss: 1.8199, Accuracy: 108/178 (61%)\n",
            "Federated Round 85/1000\n",
            "Test set: Average loss: 1.5728, Accuracy: 109/178 (61%)\n",
            "Federated Round 86/1000\n",
            "Test set: Average loss: 1.6554, Accuracy: 110/178 (62%)\n",
            "Federated Round 87/1000\n",
            "Test set: Average loss: 1.4844, Accuracy: 109/178 (61%)\n",
            "Federated Round 88/1000\n",
            "Test set: Average loss: 1.8134, Accuracy: 108/178 (61%)\n",
            "Federated Round 89/1000\n",
            "Test set: Average loss: 1.4627, Accuracy: 108/178 (61%)\n",
            "Federated Round 90/1000\n",
            "Test set: Average loss: 1.5166, Accuracy: 109/178 (61%)\n",
            "Federated Round 91/1000\n",
            "Test set: Average loss: 1.4218, Accuracy: 108/178 (61%)\n",
            "Federated Round 92/1000\n",
            "Test set: Average loss: 1.3385, Accuracy: 108/178 (61%)\n",
            "Federated Round 93/1000\n",
            "Test set: Average loss: 1.3605, Accuracy: 108/178 (61%)\n",
            "Federated Round 94/1000\n",
            "Test set: Average loss: 1.3226, Accuracy: 108/178 (61%)\n",
            "Federated Round 95/1000\n",
            "Test set: Average loss: 1.5493, Accuracy: 109/178 (61%)\n",
            "Federated Round 96/1000\n",
            "Test set: Average loss: 1.4207, Accuracy: 108/178 (61%)\n",
            "Federated Round 97/1000\n",
            "Test set: Average loss: 1.4770, Accuracy: 110/178 (62%)\n",
            "Federated Round 98/1000\n",
            "Test set: Average loss: 1.2166, Accuracy: 107/178 (60%)\n",
            "Federated Round 99/1000\n",
            "Test set: Average loss: 1.3055, Accuracy: 108/178 (61%)\n",
            "Federated Round 100/1000\n",
            "Test set: Average loss: 1.3337, Accuracy: 108/178 (61%)\n",
            "Federated Round 101/1000\n",
            "Test set: Average loss: 1.2137, Accuracy: 108/178 (61%)\n",
            "Federated Round 102/1000\n",
            "Test set: Average loss: 1.1848, Accuracy: 107/178 (60%)\n",
            "Federated Round 103/1000\n",
            "Test set: Average loss: 1.2659, Accuracy: 108/178 (61%)\n",
            "Federated Round 104/1000\n",
            "Test set: Average loss: 1.4093, Accuracy: 108/178 (61%)\n",
            "Federated Round 105/1000\n",
            "Test set: Average loss: 1.4122, Accuracy: 108/178 (61%)\n",
            "Federated Round 106/1000\n",
            "Test set: Average loss: 1.1050, Accuracy: 106/178 (60%)\n",
            "Federated Round 107/1000\n",
            "Test set: Average loss: 1.2863, Accuracy: 108/178 (61%)\n",
            "Federated Round 108/1000\n",
            "Test set: Average loss: 1.0166, Accuracy: 107/178 (60%)\n",
            "Federated Round 109/1000\n",
            "Test set: Average loss: 0.9498, Accuracy: 106/178 (60%)\n",
            "Federated Round 110/1000\n",
            "Test set: Average loss: 1.0942, Accuracy: 106/178 (60%)\n",
            "Federated Round 111/1000\n",
            "Test set: Average loss: 1.2331, Accuracy: 107/178 (60%)\n",
            "Federated Round 112/1000\n",
            "Test set: Average loss: 1.5821, Accuracy: 109/178 (61%)\n",
            "Federated Round 113/1000\n",
            "Test set: Average loss: 1.3659, Accuracy: 108/178 (61%)\n",
            "Federated Round 114/1000\n",
            "Test set: Average loss: 1.3049, Accuracy: 108/178 (61%)\n",
            "Federated Round 115/1000\n",
            "Test set: Average loss: 1.1404, Accuracy: 106/178 (60%)\n",
            "Federated Round 116/1000\n",
            "Test set: Average loss: 1.1625, Accuracy: 106/178 (60%)\n",
            "Federated Round 117/1000\n",
            "Test set: Average loss: 1.1460, Accuracy: 106/178 (60%)\n",
            "Federated Round 118/1000\n",
            "Test set: Average loss: 1.3248, Accuracy: 107/178 (60%)\n",
            "Federated Round 119/1000\n",
            "Test set: Average loss: 1.3856, Accuracy: 107/178 (60%)\n",
            "Federated Round 120/1000\n",
            "Test set: Average loss: 1.5785, Accuracy: 109/178 (61%)\n",
            "Federated Round 121/1000\n",
            "Test set: Average loss: 1.4372, Accuracy: 108/178 (61%)\n",
            "Federated Round 122/1000\n",
            "Test set: Average loss: 1.4194, Accuracy: 107/178 (60%)\n",
            "Federated Round 123/1000\n",
            "Test set: Average loss: 1.4819, Accuracy: 107/178 (60%)\n",
            "Federated Round 124/1000\n",
            "Test set: Average loss: 1.3808, Accuracy: 107/178 (60%)\n",
            "Federated Round 125/1000\n",
            "Test set: Average loss: 1.5768, Accuracy: 108/178 (61%)\n",
            "Federated Round 126/1000\n",
            "Test set: Average loss: 1.0293, Accuracy: 107/178 (60%)\n",
            "Federated Round 127/1000\n",
            "Test set: Average loss: 1.2584, Accuracy: 107/178 (60%)\n",
            "Federated Round 128/1000\n",
            "Test set: Average loss: 1.3847, Accuracy: 107/178 (60%)\n",
            "Federated Round 129/1000\n",
            "Test set: Average loss: 1.4652, Accuracy: 108/178 (61%)\n",
            "Federated Round 130/1000\n",
            "Test set: Average loss: 1.2187, Accuracy: 107/178 (60%)\n",
            "Federated Round 131/1000\n",
            "Test set: Average loss: 1.4208, Accuracy: 107/178 (60%)\n",
            "Federated Round 132/1000\n",
            "Test set: Average loss: 1.2159, Accuracy: 107/178 (60%)\n",
            "Federated Round 133/1000\n",
            "Test set: Average loss: 1.1691, Accuracy: 107/178 (60%)\n",
            "Federated Round 134/1000\n",
            "Test set: Average loss: 0.9969, Accuracy: 103/178 (58%)\n",
            "Federated Round 135/1000\n",
            "Test set: Average loss: 0.9755, Accuracy: 107/178 (60%)\n",
            "Federated Round 136/1000\n",
            "Test set: Average loss: 1.2182, Accuracy: 107/178 (60%)\n",
            "Federated Round 137/1000\n",
            "Test set: Average loss: 0.8714, Accuracy: 106/178 (60%)\n",
            "Federated Round 138/1000\n",
            "Test set: Average loss: 0.7691, Accuracy: 111/178 (62%)\n",
            "Federated Round 139/1000\n",
            "Test set: Average loss: 0.8778, Accuracy: 106/178 (60%)\n",
            "Federated Round 140/1000\n",
            "Test set: Average loss: 0.9547, Accuracy: 105/178 (59%)\n",
            "Federated Round 141/1000\n",
            "Test set: Average loss: 0.7520, Accuracy: 118/178 (66%)\n",
            "Federated Round 142/1000\n",
            "Test set: Average loss: 0.7929, Accuracy: 109/178 (61%)\n",
            "Federated Round 143/1000\n",
            "Test set: Average loss: 1.1364, Accuracy: 107/178 (60%)\n",
            "Federated Round 144/1000\n",
            "Test set: Average loss: 1.3291, Accuracy: 108/178 (61%)\n",
            "Federated Round 145/1000\n",
            "Test set: Average loss: 1.0570, Accuracy: 106/178 (60%)\n",
            "Federated Round 146/1000\n",
            "Test set: Average loss: 0.8826, Accuracy: 108/178 (61%)\n",
            "Federated Round 147/1000\n",
            "Test set: Average loss: 0.9946, Accuracy: 106/178 (60%)\n",
            "Federated Round 148/1000\n",
            "Test set: Average loss: 0.7452, Accuracy: 118/178 (66%)\n",
            "Federated Round 149/1000\n",
            "Test set: Average loss: 0.7589, Accuracy: 115/178 (65%)\n",
            "Federated Round 150/1000\n",
            "Test set: Average loss: 0.8077, Accuracy: 107/178 (60%)\n",
            "Federated Round 151/1000\n",
            "Test set: Average loss: 0.8643, Accuracy: 106/178 (60%)\n",
            "Federated Round 152/1000\n",
            "Test set: Average loss: 0.8268, Accuracy: 106/178 (60%)\n",
            "Federated Round 153/1000\n",
            "Test set: Average loss: 0.7504, Accuracy: 116/178 (65%)\n",
            "Federated Round 154/1000\n",
            "Test set: Average loss: 0.7511, Accuracy: 117/178 (66%)\n",
            "Federated Round 155/1000\n",
            "Test set: Average loss: 0.7718, Accuracy: 114/178 (64%)\n",
            "Federated Round 156/1000\n",
            "Test set: Average loss: 0.9219, Accuracy: 105/178 (59%)\n",
            "Federated Round 157/1000\n",
            "Test set: Average loss: 1.1428, Accuracy: 107/178 (60%)\n",
            "Federated Round 158/1000\n",
            "Test set: Average loss: 0.8692, Accuracy: 109/178 (61%)\n",
            "Federated Round 159/1000\n",
            "Test set: Average loss: 0.8734, Accuracy: 109/178 (61%)\n",
            "Federated Round 160/1000\n",
            "Test set: Average loss: 0.8713, Accuracy: 108/178 (61%)\n",
            "Federated Round 161/1000\n",
            "Test set: Average loss: 0.8014, Accuracy: 107/178 (60%)\n",
            "Federated Round 162/1000\n",
            "Test set: Average loss: 0.8332, Accuracy: 107/178 (60%)\n",
            "Federated Round 163/1000\n",
            "Test set: Average loss: 0.7493, Accuracy: 116/178 (65%)\n",
            "Federated Round 164/1000\n",
            "Test set: Average loss: 0.7887, Accuracy: 111/178 (62%)\n",
            "Federated Round 165/1000\n",
            "Test set: Average loss: 0.8091, Accuracy: 107/178 (60%)\n",
            "Federated Round 166/1000\n",
            "Test set: Average loss: 0.7464, Accuracy: 119/178 (67%)\n",
            "Federated Round 167/1000\n",
            "Test set: Average loss: 0.9173, Accuracy: 106/178 (60%)\n",
            "Federated Round 168/1000\n",
            "Test set: Average loss: 1.0431, Accuracy: 106/178 (60%)\n",
            "Federated Round 169/1000\n",
            "Test set: Average loss: 0.8406, Accuracy: 108/178 (61%)\n",
            "Federated Round 170/1000\n",
            "Test set: Average loss: 0.7443, Accuracy: 115/178 (65%)\n",
            "Federated Round 171/1000\n",
            "Test set: Average loss: 0.9096, Accuracy: 105/178 (59%)\n",
            "Federated Round 172/1000\n",
            "Test set: Average loss: 1.0658, Accuracy: 107/178 (60%)\n",
            "Federated Round 173/1000\n",
            "Test set: Average loss: 0.7438, Accuracy: 120/178 (67%)\n",
            "Federated Round 174/1000\n",
            "Test set: Average loss: 0.7505, Accuracy: 121/178 (68%)\n",
            "Federated Round 175/1000\n",
            "Test set: Average loss: 0.7449, Accuracy: 119/178 (67%)\n",
            "Federated Round 176/1000\n",
            "Test set: Average loss: 0.7681, Accuracy: 117/178 (66%)\n",
            "Federated Round 177/1000\n",
            "Test set: Average loss: 0.8877, Accuracy: 106/178 (60%)\n",
            "Federated Round 178/1000\n",
            "Test set: Average loss: 0.8066, Accuracy: 107/178 (60%)\n",
            "Federated Round 179/1000\n",
            "Test set: Average loss: 0.9356, Accuracy: 106/178 (60%)\n",
            "Federated Round 180/1000\n",
            "Test set: Average loss: 0.7824, Accuracy: 112/178 (63%)\n",
            "Federated Round 181/1000\n",
            "Test set: Average loss: 0.7441, Accuracy: 121/178 (68%)\n",
            "Federated Round 182/1000\n",
            "Test set: Average loss: 0.7681, Accuracy: 117/178 (66%)\n",
            "Federated Round 183/1000\n",
            "Test set: Average loss: 0.8646, Accuracy: 105/178 (59%)\n",
            "Federated Round 184/1000\n",
            "Test set: Average loss: 0.7618, Accuracy: 120/178 (67%)\n",
            "Federated Round 185/1000\n",
            "Test set: Average loss: 0.7529, Accuracy: 121/178 (68%)\n",
            "Federated Round 186/1000\n",
            "Test set: Average loss: 0.7874, Accuracy: 114/178 (64%)\n",
            "Federated Round 187/1000\n",
            "Test set: Average loss: 1.0019, Accuracy: 107/178 (60%)\n",
            "Federated Round 188/1000\n",
            "Test set: Average loss: 0.9190, Accuracy: 107/178 (60%)\n",
            "Federated Round 189/1000\n",
            "Test set: Average loss: 0.7473, Accuracy: 121/178 (68%)\n",
            "Federated Round 190/1000\n",
            "Test set: Average loss: 0.7548, Accuracy: 116/178 (65%)\n",
            "Federated Round 191/1000\n",
            "Test set: Average loss: 0.9544, Accuracy: 107/178 (60%)\n",
            "Federated Round 192/1000\n",
            "Test set: Average loss: 0.7600, Accuracy: 116/178 (65%)\n",
            "Federated Round 193/1000\n",
            "Test set: Average loss: 0.7685, Accuracy: 116/178 (65%)\n",
            "Federated Round 194/1000\n",
            "Test set: Average loss: 0.7670, Accuracy: 117/178 (66%)\n",
            "Federated Round 195/1000\n",
            "Test set: Average loss: 0.8522, Accuracy: 107/178 (60%)\n",
            "Federated Round 196/1000\n",
            "Test set: Average loss: 0.9288, Accuracy: 105/178 (59%)\n",
            "Federated Round 197/1000\n",
            "Test set: Average loss: 0.7702, Accuracy: 118/178 (66%)\n",
            "Federated Round 198/1000\n",
            "Test set: Average loss: 0.9678, Accuracy: 107/178 (60%)\n",
            "Federated Round 199/1000\n",
            "Test set: Average loss: 0.9182, Accuracy: 107/178 (60%)\n",
            "Federated Round 200/1000\n",
            "Test set: Average loss: 0.8804, Accuracy: 106/178 (60%)\n",
            "Federated Round 201/1000\n",
            "Test set: Average loss: 0.7680, Accuracy: 118/178 (66%)\n",
            "Federated Round 202/1000\n",
            "Test set: Average loss: 0.7566, Accuracy: 123/178 (69%)\n",
            "Federated Round 203/1000\n",
            "Test set: Average loss: 0.7694, Accuracy: 119/178 (67%)\n",
            "Federated Round 204/1000\n",
            "Test set: Average loss: 0.9909, Accuracy: 107/178 (60%)\n",
            "Federated Round 205/1000\n",
            "Test set: Average loss: 0.7553, Accuracy: 119/178 (67%)\n",
            "Federated Round 206/1000\n",
            "Test set: Average loss: 0.7683, Accuracy: 118/178 (66%)\n",
            "Federated Round 207/1000\n",
            "Test set: Average loss: 0.7541, Accuracy: 124/178 (70%)\n",
            "Federated Round 208/1000\n",
            "Test set: Average loss: 0.7539, Accuracy: 119/178 (67%)\n",
            "Federated Round 209/1000\n",
            "Test set: Average loss: 0.7488, Accuracy: 118/178 (66%)\n",
            "Federated Round 210/1000\n",
            "Test set: Average loss: 0.7723, Accuracy: 118/178 (66%)\n",
            "Federated Round 211/1000\n",
            "Test set: Average loss: 0.7733, Accuracy: 117/178 (66%)\n",
            "Federated Round 212/1000\n",
            "Test set: Average loss: 0.7457, Accuracy: 121/178 (68%)\n",
            "Federated Round 213/1000\n",
            "Test set: Average loss: 0.7487, Accuracy: 124/178 (70%)\n",
            "Federated Round 214/1000\n",
            "Test set: Average loss: 0.7855, Accuracy: 119/178 (67%)\n",
            "Federated Round 215/1000\n",
            "Test set: Average loss: 0.7661, Accuracy: 122/178 (69%)\n",
            "Federated Round 216/1000\n",
            "Test set: Average loss: 0.7828, Accuracy: 120/178 (67%)\n",
            "Federated Round 217/1000\n",
            "Test set: Average loss: 0.7617, Accuracy: 123/178 (69%)\n",
            "Federated Round 218/1000\n",
            "Test set: Average loss: 0.7859, Accuracy: 118/178 (66%)\n",
            "Federated Round 219/1000\n",
            "Test set: Average loss: 0.7852, Accuracy: 118/178 (66%)\n",
            "Federated Round 220/1000\n",
            "Test set: Average loss: 0.7953, Accuracy: 115/178 (65%)\n",
            "Federated Round 221/1000\n",
            "Test set: Average loss: 0.8328, Accuracy: 117/178 (66%)\n",
            "Federated Round 222/1000\n",
            "Test set: Average loss: 0.8087, Accuracy: 119/178 (67%)\n",
            "Federated Round 223/1000\n",
            "Test set: Average loss: 0.8010, Accuracy: 118/178 (66%)\n",
            "Federated Round 224/1000\n",
            "Test set: Average loss: 0.7668, Accuracy: 122/178 (69%)\n",
            "Federated Round 225/1000\n",
            "Test set: Average loss: 0.8824, Accuracy: 116/178 (65%)\n",
            "Federated Round 226/1000\n",
            "Test set: Average loss: 0.8732, Accuracy: 116/178 (65%)\n",
            "Federated Round 227/1000\n",
            "Test set: Average loss: 0.8734, Accuracy: 116/178 (65%)\n",
            "Federated Round 228/1000\n",
            "Test set: Average loss: 0.8003, Accuracy: 115/178 (65%)\n",
            "Federated Round 229/1000\n",
            "Test set: Average loss: 0.8416, Accuracy: 116/178 (65%)\n",
            "Federated Round 230/1000\n",
            "Test set: Average loss: 0.8844, Accuracy: 115/178 (65%)\n",
            "Federated Round 231/1000\n",
            "Test set: Average loss: 0.8722, Accuracy: 114/178 (64%)\n",
            "Federated Round 232/1000\n",
            "Test set: Average loss: 0.8264, Accuracy: 115/178 (65%)\n",
            "Federated Round 233/1000\n",
            "Test set: Average loss: 0.8045, Accuracy: 115/178 (65%)\n",
            "Federated Round 234/1000\n",
            "Test set: Average loss: 0.8096, Accuracy: 115/178 (65%)\n",
            "Federated Round 235/1000\n",
            "Test set: Average loss: 0.8070, Accuracy: 115/178 (65%)\n",
            "Federated Round 236/1000\n",
            "Test set: Average loss: 0.8090, Accuracy: 115/178 (65%)\n",
            "Federated Round 237/1000\n",
            "Test set: Average loss: 0.8555, Accuracy: 114/178 (64%)\n",
            "Federated Round 238/1000\n",
            "Test set: Average loss: 0.8413, Accuracy: 114/178 (64%)\n",
            "Federated Round 239/1000\n",
            "Test set: Average loss: 0.8298, Accuracy: 115/178 (65%)\n",
            "Federated Round 240/1000\n",
            "Test set: Average loss: 0.8065, Accuracy: 115/178 (65%)\n",
            "Federated Round 241/1000\n",
            "Test set: Average loss: 0.7823, Accuracy: 117/178 (66%)\n",
            "Federated Round 242/1000\n",
            "Test set: Average loss: 0.8158, Accuracy: 115/178 (65%)\n",
            "Federated Round 243/1000\n",
            "Test set: Average loss: 0.8239, Accuracy: 115/178 (65%)\n",
            "Federated Round 244/1000\n",
            "Test set: Average loss: 0.7754, Accuracy: 118/178 (66%)\n",
            "Federated Round 245/1000\n",
            "Test set: Average loss: 0.7785, Accuracy: 117/178 (66%)\n",
            "Federated Round 246/1000\n",
            "Test set: Average loss: 0.7701, Accuracy: 119/178 (67%)\n",
            "Federated Round 247/1000\n",
            "Test set: Average loss: 0.7796, Accuracy: 117/178 (66%)\n",
            "Federated Round 248/1000\n",
            "Test set: Average loss: 0.7947, Accuracy: 116/178 (65%)\n",
            "Federated Round 249/1000\n",
            "Test set: Average loss: 0.7714, Accuracy: 125/178 (70%)\n",
            "Federated Round 250/1000\n",
            "Test set: Average loss: 0.7762, Accuracy: 119/178 (67%)\n",
            "Federated Round 251/1000\n",
            "Test set: Average loss: 0.7781, Accuracy: 119/178 (67%)\n",
            "Federated Round 252/1000\n",
            "Test set: Average loss: 0.7742, Accuracy: 122/178 (69%)\n",
            "Federated Round 253/1000\n",
            "Test set: Average loss: 0.8285, Accuracy: 115/178 (65%)\n",
            "Federated Round 254/1000\n",
            "Test set: Average loss: 0.8042, Accuracy: 115/178 (65%)\n",
            "Federated Round 255/1000\n",
            "Test set: Average loss: 0.7877, Accuracy: 118/178 (66%)\n",
            "Federated Round 256/1000\n",
            "Test set: Average loss: 0.8319, Accuracy: 115/178 (65%)\n",
            "Federated Round 257/1000\n",
            "Test set: Average loss: 0.7948, Accuracy: 117/178 (66%)\n",
            "Federated Round 258/1000\n",
            "Test set: Average loss: 0.8185, Accuracy: 115/178 (65%)\n",
            "Federated Round 259/1000\n",
            "Test set: Average loss: 0.8633, Accuracy: 117/178 (66%)\n",
            "Federated Round 260/1000\n",
            "Test set: Average loss: 0.7744, Accuracy: 122/178 (69%)\n",
            "Federated Round 261/1000\n",
            "Test set: Average loss: 0.7841, Accuracy: 119/178 (67%)\n",
            "Federated Round 262/1000\n",
            "Test set: Average loss: 0.7950, Accuracy: 117/178 (66%)\n",
            "Federated Round 263/1000\n",
            "Test set: Average loss: 0.7766, Accuracy: 120/178 (67%)\n",
            "Federated Round 264/1000\n",
            "Test set: Average loss: 0.7665, Accuracy: 122/178 (69%)\n",
            "Federated Round 265/1000\n",
            "Test set: Average loss: 0.8203, Accuracy: 115/178 (65%)\n",
            "Federated Round 266/1000\n",
            "Test set: Average loss: 0.7967, Accuracy: 118/178 (66%)\n",
            "Federated Round 267/1000\n",
            "Test set: Average loss: 0.8577, Accuracy: 117/178 (66%)\n",
            "Federated Round 268/1000\n",
            "Test set: Average loss: 0.8866, Accuracy: 115/178 (65%)\n",
            "Federated Round 269/1000\n",
            "Test set: Average loss: 0.8711, Accuracy: 117/178 (66%)\n",
            "Federated Round 270/1000\n",
            "Test set: Average loss: 0.9006, Accuracy: 113/178 (63%)\n",
            "Federated Round 271/1000\n",
            "Test set: Average loss: 0.8405, Accuracy: 117/178 (66%)\n",
            "Federated Round 272/1000\n",
            "Test set: Average loss: 0.7970, Accuracy: 119/178 (67%)\n",
            "Federated Round 273/1000\n",
            "Test set: Average loss: 0.7783, Accuracy: 124/178 (70%)\n",
            "Federated Round 274/1000\n",
            "Test set: Average loss: 0.7804, Accuracy: 122/178 (69%)\n",
            "Federated Round 275/1000\n",
            "Test set: Average loss: 0.7711, Accuracy: 124/178 (70%)\n",
            "Federated Round 276/1000\n",
            "Test set: Average loss: 0.7805, Accuracy: 124/178 (70%)\n",
            "Federated Round 277/1000\n",
            "Test set: Average loss: 0.7948, Accuracy: 120/178 (67%)\n",
            "Federated Round 278/1000\n",
            "Test set: Average loss: 0.7845, Accuracy: 124/178 (70%)\n",
            "Federated Round 279/1000\n",
            "Test set: Average loss: 0.8146, Accuracy: 116/178 (65%)\n",
            "Federated Round 280/1000\n",
            "Test set: Average loss: 0.8704, Accuracy: 117/178 (66%)\n",
            "Federated Round 281/1000\n",
            "Test set: Average loss: 0.7936, Accuracy: 120/178 (67%)\n",
            "Federated Round 282/1000\n",
            "Test set: Average loss: 0.7789, Accuracy: 124/178 (70%)\n",
            "Federated Round 283/1000\n",
            "Test set: Average loss: 0.7803, Accuracy: 122/178 (69%)\n",
            "Federated Round 284/1000\n",
            "Test set: Average loss: 0.7664, Accuracy: 118/178 (66%)\n",
            "Federated Round 285/1000\n",
            "Test set: Average loss: 0.7860, Accuracy: 120/178 (67%)\n",
            "Federated Round 286/1000\n",
            "Test set: Average loss: 0.7677, Accuracy: 119/178 (67%)\n",
            "Federated Round 287/1000\n",
            "Test set: Average loss: 0.7877, Accuracy: 119/178 (67%)\n",
            "Federated Round 288/1000\n",
            "Test set: Average loss: 0.8111, Accuracy: 117/178 (66%)\n",
            "Federated Round 289/1000\n",
            "Test set: Average loss: 0.8685, Accuracy: 117/178 (66%)\n",
            "Federated Round 290/1000\n",
            "Test set: Average loss: 0.9223, Accuracy: 114/178 (64%)\n",
            "Federated Round 291/1000\n",
            "Test set: Average loss: 0.9218, Accuracy: 112/178 (63%)\n",
            "Federated Round 292/1000\n",
            "Test set: Average loss: 0.8243, Accuracy: 117/178 (66%)\n",
            "Federated Round 293/1000\n",
            "Test set: Average loss: 0.8331, Accuracy: 116/178 (65%)\n",
            "Federated Round 294/1000\n",
            "Test set: Average loss: 0.7946, Accuracy: 121/178 (68%)\n",
            "Federated Round 295/1000\n",
            "Test set: Average loss: 0.7873, Accuracy: 124/178 (70%)\n",
            "Federated Round 296/1000\n",
            "Test set: Average loss: 0.8434, Accuracy: 117/178 (66%)\n",
            "Federated Round 297/1000\n",
            "Test set: Average loss: 0.8490, Accuracy: 116/178 (65%)\n",
            "Federated Round 298/1000\n",
            "Test set: Average loss: 0.7964, Accuracy: 121/178 (68%)\n",
            "Federated Round 299/1000\n",
            "Test set: Average loss: 0.7786, Accuracy: 123/178 (69%)\n",
            "Federated Round 300/1000\n",
            "Test set: Average loss: 0.8002, Accuracy: 121/178 (68%)\n",
            "Federated Round 301/1000\n",
            "Test set: Average loss: 0.8511, Accuracy: 116/178 (65%)\n",
            "Federated Round 302/1000\n",
            "Test set: Average loss: 0.7926, Accuracy: 124/178 (70%)\n",
            "Federated Round 303/1000\n",
            "Test set: Average loss: 0.7868, Accuracy: 123/178 (69%)\n",
            "Federated Round 304/1000\n",
            "Test set: Average loss: 0.8486, Accuracy: 116/178 (65%)\n",
            "Federated Round 305/1000\n",
            "Test set: Average loss: 0.8851, Accuracy: 115/178 (65%)\n",
            "Federated Round 306/1000\n",
            "Test set: Average loss: 0.8026, Accuracy: 119/178 (67%)\n",
            "Federated Round 307/1000\n",
            "Test set: Average loss: 0.7866, Accuracy: 121/178 (68%)\n",
            "Federated Round 308/1000\n",
            "Test set: Average loss: 0.9571, Accuracy: 115/178 (65%)\n",
            "Federated Round 309/1000\n",
            "Test set: Average loss: 0.8931, Accuracy: 115/178 (65%)\n",
            "Federated Round 310/1000\n",
            "Test set: Average loss: 0.7973, Accuracy: 122/178 (69%)\n",
            "Federated Round 311/1000\n",
            "Test set: Average loss: 0.7986, Accuracy: 120/178 (67%)\n",
            "Federated Round 312/1000\n",
            "Test set: Average loss: 0.8419, Accuracy: 117/178 (66%)\n",
            "Federated Round 313/1000\n",
            "Test set: Average loss: 0.8288, Accuracy: 119/178 (67%)\n",
            "Federated Round 314/1000\n",
            "Test set: Average loss: 0.8521, Accuracy: 117/178 (66%)\n",
            "Federated Round 315/1000\n",
            "Test set: Average loss: 0.8082, Accuracy: 121/178 (68%)\n",
            "Federated Round 316/1000\n",
            "Test set: Average loss: 0.8168, Accuracy: 119/178 (67%)\n",
            "Federated Round 317/1000\n",
            "Test set: Average loss: 0.8017, Accuracy: 121/178 (68%)\n",
            "Federated Round 318/1000\n",
            "Test set: Average loss: 0.8344, Accuracy: 119/178 (67%)\n",
            "Federated Round 319/1000\n",
            "Test set: Average loss: 0.8811, Accuracy: 114/178 (64%)\n",
            "Federated Round 320/1000\n",
            "Test set: Average loss: 0.8733, Accuracy: 115/178 (65%)\n",
            "Federated Round 321/1000\n",
            "Test set: Average loss: 0.8357, Accuracy: 119/178 (67%)\n",
            "Federated Round 322/1000\n",
            "Test set: Average loss: 0.8298, Accuracy: 120/178 (67%)\n",
            "Federated Round 323/1000\n",
            "Test set: Average loss: 0.8362, Accuracy: 120/178 (67%)\n",
            "Federated Round 324/1000\n",
            "Test set: Average loss: 0.8066, Accuracy: 121/178 (68%)\n",
            "Federated Round 325/1000\n",
            "Test set: Average loss: 0.7986, Accuracy: 121/178 (68%)\n",
            "Federated Round 326/1000\n",
            "Test set: Average loss: 0.7902, Accuracy: 123/178 (69%)\n",
            "Federated Round 327/1000\n",
            "Test set: Average loss: 0.7977, Accuracy: 120/178 (67%)\n",
            "Federated Round 328/1000\n",
            "Test set: Average loss: 0.7871, Accuracy: 123/178 (69%)\n",
            "Federated Round 329/1000\n",
            "Test set: Average loss: 0.7939, Accuracy: 120/178 (67%)\n",
            "Federated Round 330/1000\n",
            "Test set: Average loss: 0.7812, Accuracy: 123/178 (69%)\n",
            "Federated Round 331/1000\n",
            "Test set: Average loss: 0.7963, Accuracy: 121/178 (68%)\n",
            "Federated Round 332/1000\n",
            "Test set: Average loss: 0.8289, Accuracy: 117/178 (66%)\n",
            "Federated Round 333/1000\n",
            "Test set: Average loss: 0.7780, Accuracy: 119/178 (67%)\n",
            "Federated Round 334/1000\n",
            "Test set: Average loss: 0.7807, Accuracy: 123/178 (69%)\n",
            "Federated Round 335/1000\n",
            "Test set: Average loss: 0.7835, Accuracy: 117/178 (66%)\n",
            "Federated Round 336/1000\n",
            "Test set: Average loss: 0.7936, Accuracy: 123/178 (69%)\n",
            "Federated Round 337/1000\n",
            "Test set: Average loss: 0.8216, Accuracy: 117/178 (66%)\n",
            "Federated Round 338/1000\n",
            "Test set: Average loss: 0.8138, Accuracy: 120/178 (67%)\n",
            "Federated Round 339/1000\n",
            "Test set: Average loss: 0.8049, Accuracy: 121/178 (68%)\n",
            "Federated Round 340/1000\n",
            "Test set: Average loss: 0.8315, Accuracy: 116/178 (65%)\n",
            "Federated Round 341/1000\n",
            "Test set: Average loss: 0.8694, Accuracy: 115/178 (65%)\n",
            "Federated Round 342/1000\n",
            "Test set: Average loss: 0.8285, Accuracy: 118/178 (66%)\n",
            "Federated Round 343/1000\n",
            "Test set: Average loss: 0.7915, Accuracy: 121/178 (68%)\n",
            "Federated Round 344/1000\n",
            "Test set: Average loss: 0.7982, Accuracy: 119/178 (67%)\n",
            "Federated Round 345/1000\n",
            "Test set: Average loss: 0.8146, Accuracy: 119/178 (67%)\n",
            "Federated Round 346/1000\n",
            "Test set: Average loss: 0.8218, Accuracy: 118/178 (66%)\n",
            "Federated Round 347/1000\n",
            "Test set: Average loss: 0.7901, Accuracy: 119/178 (67%)\n",
            "Federated Round 348/1000\n",
            "Test set: Average loss: 0.7943, Accuracy: 122/178 (69%)\n",
            "Federated Round 349/1000\n",
            "Test set: Average loss: 0.8821, Accuracy: 114/178 (64%)\n",
            "Federated Round 350/1000\n",
            "Test set: Average loss: 0.9638, Accuracy: 110/178 (62%)\n",
            "Federated Round 351/1000\n",
            "Test set: Average loss: 0.9673, Accuracy: 109/178 (61%)\n",
            "Federated Round 352/1000\n",
            "Test set: Average loss: 0.9163, Accuracy: 110/178 (62%)\n",
            "Federated Round 353/1000\n",
            "Test set: Average loss: 0.9275, Accuracy: 109/178 (61%)\n",
            "Federated Round 354/1000\n",
            "Test set: Average loss: 0.8465, Accuracy: 113/178 (63%)\n",
            "Federated Round 355/1000\n",
            "Test set: Average loss: 0.8732, Accuracy: 110/178 (62%)\n",
            "Federated Round 356/1000\n",
            "Test set: Average loss: 0.9097, Accuracy: 109/178 (61%)\n",
            "Federated Round 357/1000\n",
            "Test set: Average loss: 0.7997, Accuracy: 115/178 (65%)\n",
            "Federated Round 358/1000\n",
            "Test set: Average loss: 0.8034, Accuracy: 122/178 (69%)\n",
            "Federated Round 359/1000\n",
            "Test set: Average loss: 0.8118, Accuracy: 121/178 (68%)\n",
            "Federated Round 360/1000\n",
            "Test set: Average loss: 0.7874, Accuracy: 119/178 (67%)\n",
            "Federated Round 361/1000\n",
            "Test set: Average loss: 0.7926, Accuracy: 119/178 (67%)\n",
            "Federated Round 362/1000\n",
            "Test set: Average loss: 0.8510, Accuracy: 109/178 (61%)\n",
            "Federated Round 363/1000\n",
            "Test set: Average loss: 0.8032, Accuracy: 119/178 (67%)\n",
            "Federated Round 364/1000\n",
            "Test set: Average loss: 0.7951, Accuracy: 119/178 (67%)\n",
            "Federated Round 365/1000\n",
            "Test set: Average loss: 0.8618, Accuracy: 111/178 (62%)\n",
            "Federated Round 366/1000\n",
            "Test set: Average loss: 0.8807, Accuracy: 110/178 (62%)\n",
            "Federated Round 367/1000\n",
            "Test set: Average loss: 0.8526, Accuracy: 109/178 (61%)\n",
            "Federated Round 368/1000\n",
            "Test set: Average loss: 0.8189, Accuracy: 114/178 (64%)\n",
            "Federated Round 369/1000\n",
            "Test set: Average loss: 0.8037, Accuracy: 117/178 (66%)\n",
            "Federated Round 370/1000\n",
            "Test set: Average loss: 0.8238, Accuracy: 115/178 (65%)\n",
            "Federated Round 371/1000\n",
            "Test set: Average loss: 0.8284, Accuracy: 116/178 (65%)\n",
            "Federated Round 372/1000\n",
            "Test set: Average loss: 0.8075, Accuracy: 120/178 (67%)\n",
            "Federated Round 373/1000\n",
            "Test set: Average loss: 0.8348, Accuracy: 116/178 (65%)\n",
            "Federated Round 374/1000\n",
            "Test set: Average loss: 0.8456, Accuracy: 112/178 (63%)\n",
            "Federated Round 375/1000\n",
            "Test set: Average loss: 0.8206, Accuracy: 117/178 (66%)\n",
            "Federated Round 376/1000\n",
            "Test set: Average loss: 0.8274, Accuracy: 115/178 (65%)\n",
            "Federated Round 377/1000\n",
            "Test set: Average loss: 0.8203, Accuracy: 116/178 (65%)\n",
            "Federated Round 378/1000\n",
            "Test set: Average loss: 0.8108, Accuracy: 113/178 (63%)\n",
            "Federated Round 379/1000\n",
            "Test set: Average loss: 0.9324, Accuracy: 110/178 (62%)\n",
            "Federated Round 380/1000\n",
            "Test set: Average loss: 0.9251, Accuracy: 110/178 (62%)\n",
            "Federated Round 381/1000\n",
            "Test set: Average loss: 0.8417, Accuracy: 111/178 (62%)\n",
            "Federated Round 382/1000\n",
            "Test set: Average loss: 0.8873, Accuracy: 110/178 (62%)\n",
            "Federated Round 383/1000\n",
            "Test set: Average loss: 0.8913, Accuracy: 109/178 (61%)\n",
            "Federated Round 384/1000\n",
            "Test set: Average loss: 0.8174, Accuracy: 120/178 (67%)\n",
            "Federated Round 385/1000\n",
            "Test set: Average loss: 0.8637, Accuracy: 112/178 (63%)\n",
            "Federated Round 386/1000\n",
            "Test set: Average loss: 0.9091, Accuracy: 109/178 (61%)\n",
            "Federated Round 387/1000\n",
            "Test set: Average loss: 0.8222, Accuracy: 122/178 (69%)\n",
            "Federated Round 388/1000\n",
            "Test set: Average loss: 0.8341, Accuracy: 117/178 (66%)\n",
            "Federated Round 389/1000\n",
            "Test set: Average loss: 0.8087, Accuracy: 123/178 (69%)\n",
            "Federated Round 390/1000\n",
            "Test set: Average loss: 0.8044, Accuracy: 116/178 (65%)\n",
            "Federated Round 391/1000\n",
            "Test set: Average loss: 0.8750, Accuracy: 111/178 (62%)\n",
            "Federated Round 392/1000\n",
            "Test set: Average loss: 0.8160, Accuracy: 114/178 (64%)\n",
            "Federated Round 393/1000\n",
            "Test set: Average loss: 0.8100, Accuracy: 122/178 (69%)\n",
            "Federated Round 394/1000\n",
            "Test set: Average loss: 0.8082, Accuracy: 122/178 (69%)\n",
            "Federated Round 395/1000\n",
            "Test set: Average loss: 0.8158, Accuracy: 111/178 (62%)\n",
            "Federated Round 396/1000\n",
            "Test set: Average loss: 0.8632, Accuracy: 106/178 (60%)\n",
            "Federated Round 397/1000\n",
            "Test set: Average loss: 0.8134, Accuracy: 112/178 (63%)\n",
            "Federated Round 398/1000\n",
            "Test set: Average loss: 0.8162, Accuracy: 110/178 (62%)\n",
            "Federated Round 399/1000\n",
            "Test set: Average loss: 0.8173, Accuracy: 113/178 (63%)\n",
            "Federated Round 400/1000\n",
            "Test set: Average loss: 0.9408, Accuracy: 110/178 (62%)\n",
            "Federated Round 401/1000\n",
            "Test set: Average loss: 0.8937, Accuracy: 109/178 (61%)\n",
            "Federated Round 402/1000\n",
            "Test set: Average loss: 0.8136, Accuracy: 114/178 (64%)\n",
            "Federated Round 403/1000\n",
            "Test set: Average loss: 0.8193, Accuracy: 116/178 (65%)\n",
            "Federated Round 404/1000\n",
            "Test set: Average loss: 0.8571, Accuracy: 112/178 (63%)\n",
            "Federated Round 405/1000\n",
            "Test set: Average loss: 0.8434, Accuracy: 113/178 (63%)\n",
            "Federated Round 406/1000\n",
            "Test set: Average loss: 0.8334, Accuracy: 115/178 (65%)\n",
            "Federated Round 407/1000\n",
            "Test set: Average loss: 0.8164, Accuracy: 115/178 (65%)\n",
            "Federated Round 408/1000\n",
            "Test set: Average loss: 0.9079, Accuracy: 110/178 (62%)\n",
            "Federated Round 409/1000\n",
            "Test set: Average loss: 0.8402, Accuracy: 115/178 (65%)\n",
            "Federated Round 410/1000\n",
            "Test set: Average loss: 0.8216, Accuracy: 114/178 (64%)\n",
            "Federated Round 411/1000\n",
            "Test set: Average loss: 0.8780, Accuracy: 106/178 (60%)\n",
            "Federated Round 412/1000\n",
            "Test set: Average loss: 0.8252, Accuracy: 111/178 (62%)\n",
            "Federated Round 413/1000\n",
            "Test set: Average loss: 0.8407, Accuracy: 109/178 (61%)\n",
            "Federated Round 414/1000\n",
            "Test set: Average loss: 0.8364, Accuracy: 112/178 (63%)\n",
            "Federated Round 415/1000\n",
            "Test set: Average loss: 0.8387, Accuracy: 117/178 (66%)\n",
            "Federated Round 416/1000\n",
            "Test set: Average loss: 0.8722, Accuracy: 106/178 (60%)\n",
            "Federated Round 417/1000\n",
            "Test set: Average loss: 0.8312, Accuracy: 115/178 (65%)\n",
            "Federated Round 418/1000\n",
            "Test set: Average loss: 0.8447, Accuracy: 115/178 (65%)\n",
            "Federated Round 419/1000\n",
            "Test set: Average loss: 0.8730, Accuracy: 111/178 (62%)\n",
            "Federated Round 420/1000\n",
            "Test set: Average loss: 0.8360, Accuracy: 115/178 (65%)\n",
            "Federated Round 421/1000\n",
            "Test set: Average loss: 0.8238, Accuracy: 117/178 (66%)\n",
            "Federated Round 422/1000\n",
            "Test set: Average loss: 0.8143, Accuracy: 117/178 (66%)\n",
            "Federated Round 423/1000\n",
            "Test set: Average loss: 0.8467, Accuracy: 116/178 (65%)\n",
            "Federated Round 424/1000\n",
            "Test set: Average loss: 0.8479, Accuracy: 116/178 (65%)\n",
            "Federated Round 425/1000\n",
            "Test set: Average loss: 0.8334, Accuracy: 116/178 (65%)\n",
            "Federated Round 426/1000\n",
            "Test set: Average loss: 0.8435, Accuracy: 115/178 (65%)\n",
            "Federated Round 427/1000\n",
            "Test set: Average loss: 0.8722, Accuracy: 107/178 (60%)\n",
            "Federated Round 428/1000\n",
            "Test set: Average loss: 0.9194, Accuracy: 111/178 (62%)\n",
            "Federated Round 429/1000\n",
            "Test set: Average loss: 0.8596, Accuracy: 115/178 (65%)\n",
            "Federated Round 430/1000\n",
            "Test set: Average loss: 0.8392, Accuracy: 112/178 (63%)\n",
            "Federated Round 431/1000\n",
            "Test set: Average loss: 0.8864, Accuracy: 112/178 (63%)\n",
            "Federated Round 432/1000\n",
            "Test set: Average loss: 0.8573, Accuracy: 114/178 (64%)\n",
            "Federated Round 433/1000\n",
            "Test set: Average loss: 0.8553, Accuracy: 114/178 (64%)\n",
            "Federated Round 434/1000\n",
            "Test set: Average loss: 0.8767, Accuracy: 112/178 (63%)\n",
            "Federated Round 435/1000\n",
            "Test set: Average loss: 0.8980, Accuracy: 112/178 (63%)\n",
            "Federated Round 436/1000\n",
            "Test set: Average loss: 0.9315, Accuracy: 111/178 (62%)\n",
            "Federated Round 437/1000\n",
            "Test set: Average loss: 0.8765, Accuracy: 112/178 (63%)\n",
            "Federated Round 438/1000\n",
            "Test set: Average loss: 0.8796, Accuracy: 112/178 (63%)\n",
            "Federated Round 439/1000\n",
            "Test set: Average loss: 0.8652, Accuracy: 112/178 (63%)\n",
            "Federated Round 440/1000\n",
            "Test set: Average loss: 0.8613, Accuracy: 114/178 (64%)\n",
            "Federated Round 441/1000\n",
            "Test set: Average loss: 0.9064, Accuracy: 111/178 (62%)\n",
            "Federated Round 442/1000\n",
            "Test set: Average loss: 0.8780, Accuracy: 112/178 (63%)\n",
            "Federated Round 443/1000\n",
            "Test set: Average loss: 0.8782, Accuracy: 110/178 (62%)\n",
            "Federated Round 444/1000\n",
            "Test set: Average loss: 0.8480, Accuracy: 116/178 (65%)\n",
            "Federated Round 445/1000\n",
            "Test set: Average loss: 0.8474, Accuracy: 110/178 (62%)\n",
            "Federated Round 446/1000\n",
            "Test set: Average loss: 0.8550, Accuracy: 112/178 (63%)\n",
            "Federated Round 447/1000\n",
            "Test set: Average loss: 0.8784, Accuracy: 113/178 (63%)\n",
            "Federated Round 448/1000\n",
            "Test set: Average loss: 0.8857, Accuracy: 113/178 (63%)\n",
            "Federated Round 449/1000\n",
            "Test set: Average loss: 0.8940, Accuracy: 111/178 (62%)\n",
            "Federated Round 450/1000\n",
            "Test set: Average loss: 0.8920, Accuracy: 112/178 (63%)\n",
            "Federated Round 451/1000\n",
            "Test set: Average loss: 0.8704, Accuracy: 114/178 (64%)\n",
            "Federated Round 452/1000\n",
            "Test set: Average loss: 0.9032, Accuracy: 110/178 (62%)\n",
            "Federated Round 453/1000\n",
            "Test set: Average loss: 0.9493, Accuracy: 110/178 (62%)\n",
            "Federated Round 454/1000\n",
            "Test set: Average loss: 0.8599, Accuracy: 113/178 (63%)\n",
            "Federated Round 455/1000\n",
            "Test set: Average loss: 0.8476, Accuracy: 113/178 (63%)\n",
            "Federated Round 456/1000\n",
            "Test set: Average loss: 0.8546, Accuracy: 111/178 (62%)\n",
            "Federated Round 457/1000\n",
            "Test set: Average loss: 0.9084, Accuracy: 110/178 (62%)\n",
            "Federated Round 458/1000\n",
            "Test set: Average loss: 0.9027, Accuracy: 110/178 (62%)\n",
            "Federated Round 459/1000\n",
            "Test set: Average loss: 0.8505, Accuracy: 111/178 (62%)\n",
            "Federated Round 460/1000\n",
            "Test set: Average loss: 0.8619, Accuracy: 114/178 (64%)\n",
            "Federated Round 461/1000\n",
            "Test set: Average loss: 0.9110, Accuracy: 110/178 (62%)\n",
            "Federated Round 462/1000\n",
            "Test set: Average loss: 0.8863, Accuracy: 113/178 (63%)\n",
            "Federated Round 463/1000\n",
            "Test set: Average loss: 0.8836, Accuracy: 113/178 (63%)\n",
            "Federated Round 464/1000\n",
            "Test set: Average loss: 0.9141, Accuracy: 111/178 (62%)\n",
            "Federated Round 465/1000\n",
            "Test set: Average loss: 0.9418, Accuracy: 110/178 (62%)\n",
            "Federated Round 466/1000\n",
            "Test set: Average loss: 0.8869, Accuracy: 112/178 (63%)\n",
            "Federated Round 467/1000\n",
            "Test set: Average loss: 0.8591, Accuracy: 113/178 (63%)\n",
            "Federated Round 468/1000\n",
            "Test set: Average loss: 0.8884, Accuracy: 110/178 (62%)\n",
            "Federated Round 469/1000\n",
            "Test set: Average loss: 0.8428, Accuracy: 115/178 (65%)\n",
            "Federated Round 470/1000\n",
            "Test set: Average loss: 0.8673, Accuracy: 112/178 (63%)\n",
            "Federated Round 471/1000\n",
            "Test set: Average loss: 0.8622, Accuracy: 106/178 (60%)\n",
            "Federated Round 472/1000\n",
            "Test set: Average loss: 0.8841, Accuracy: 113/178 (63%)\n",
            "Federated Round 473/1000\n",
            "Test set: Average loss: 0.8575, Accuracy: 112/178 (63%)\n",
            "Federated Round 474/1000\n",
            "Test set: Average loss: 0.8666, Accuracy: 114/178 (64%)\n",
            "Federated Round 475/1000\n",
            "Test set: Average loss: 0.8730, Accuracy: 115/178 (65%)\n",
            "Federated Round 476/1000\n",
            "Test set: Average loss: 0.8723, Accuracy: 115/178 (65%)\n",
            "Federated Round 477/1000\n",
            "Test set: Average loss: 0.8599, Accuracy: 112/178 (63%)\n",
            "Federated Round 478/1000\n",
            "Test set: Average loss: 0.8763, Accuracy: 106/178 (60%)\n",
            "Federated Round 479/1000\n",
            "Test set: Average loss: 0.8553, Accuracy: 110/178 (62%)\n",
            "Federated Round 480/1000\n",
            "Test set: Average loss: 0.9227, Accuracy: 105/178 (59%)\n",
            "Federated Round 481/1000\n",
            "Test set: Average loss: 0.8635, Accuracy: 110/178 (62%)\n",
            "Federated Round 482/1000\n",
            "Test set: Average loss: 0.8907, Accuracy: 117/178 (66%)\n",
            "Federated Round 483/1000\n",
            "Test set: Average loss: 0.8866, Accuracy: 115/178 (65%)\n",
            "Federated Round 484/1000\n",
            "Test set: Average loss: 0.9296, Accuracy: 111/178 (62%)\n",
            "Federated Round 485/1000\n",
            "Test set: Average loss: 0.9235, Accuracy: 111/178 (62%)\n",
            "Federated Round 486/1000\n",
            "Test set: Average loss: 0.9908, Accuracy: 109/178 (61%)\n",
            "Federated Round 487/1000\n",
            "Test set: Average loss: 0.8812, Accuracy: 113/178 (63%)\n",
            "Federated Round 488/1000\n",
            "Test set: Average loss: 0.8946, Accuracy: 104/178 (58%)\n",
            "Federated Round 489/1000\n",
            "Test set: Average loss: 0.8972, Accuracy: 112/178 (63%)\n",
            "Federated Round 490/1000\n",
            "Test set: Average loss: 0.8536, Accuracy: 112/178 (63%)\n",
            "Federated Round 491/1000\n",
            "Test set: Average loss: 0.8656, Accuracy: 116/178 (65%)\n",
            "Federated Round 492/1000\n",
            "Test set: Average loss: 0.9896, Accuracy: 109/178 (61%)\n",
            "Federated Round 493/1000\n",
            "Test set: Average loss: 0.9176, Accuracy: 105/178 (59%)\n",
            "Federated Round 494/1000\n",
            "Test set: Average loss: 0.8720, Accuracy: 105/178 (59%)\n",
            "Federated Round 495/1000\n",
            "Test set: Average loss: 0.8652, Accuracy: 107/178 (60%)\n",
            "Federated Round 496/1000\n",
            "Test set: Average loss: 0.8666, Accuracy: 107/178 (60%)\n",
            "Federated Round 497/1000\n",
            "Test set: Average loss: 0.8726, Accuracy: 107/178 (60%)\n",
            "Federated Round 498/1000\n",
            "Test set: Average loss: 0.8712, Accuracy: 110/178 (62%)\n",
            "Federated Round 499/1000\n",
            "Test set: Average loss: 0.9056, Accuracy: 112/178 (63%)\n",
            "Federated Round 500/1000\n",
            "Test set: Average loss: 0.9535, Accuracy: 109/178 (61%)\n",
            "Federated Round 501/1000\n",
            "Test set: Average loss: 0.8737, Accuracy: 111/178 (62%)\n",
            "Federated Round 502/1000\n",
            "Test set: Average loss: 0.8759, Accuracy: 111/178 (62%)\n",
            "Federated Round 503/1000\n",
            "Test set: Average loss: 0.8823, Accuracy: 107/178 (60%)\n",
            "Federated Round 504/1000\n",
            "Test set: Average loss: 0.8730, Accuracy: 111/178 (62%)\n",
            "Federated Round 505/1000\n",
            "Test set: Average loss: 0.8874, Accuracy: 113/178 (63%)\n",
            "Federated Round 506/1000\n",
            "Test set: Average loss: 0.8736, Accuracy: 116/178 (65%)\n",
            "Federated Round 507/1000\n",
            "Test set: Average loss: 0.8966, Accuracy: 106/178 (60%)\n",
            "Federated Round 508/1000\n",
            "Test set: Average loss: 0.8554, Accuracy: 112/178 (63%)\n",
            "Federated Round 509/1000\n",
            "Test set: Average loss: 0.8897, Accuracy: 113/178 (63%)\n",
            "Federated Round 510/1000\n",
            "Test set: Average loss: 0.9352, Accuracy: 110/178 (62%)\n",
            "Federated Round 511/1000\n",
            "Test set: Average loss: 0.9328, Accuracy: 110/178 (62%)\n",
            "Federated Round 512/1000\n",
            "Test set: Average loss: 0.9248, Accuracy: 110/178 (62%)\n",
            "Federated Round 513/1000\n",
            "Test set: Average loss: 0.8775, Accuracy: 109/178 (61%)\n",
            "Federated Round 514/1000\n",
            "Test set: Average loss: 0.8687, Accuracy: 110/178 (62%)\n",
            "Federated Round 515/1000\n",
            "Test set: Average loss: 0.8961, Accuracy: 112/178 (63%)\n",
            "Federated Round 516/1000\n",
            "Test set: Average loss: 0.8682, Accuracy: 112/178 (63%)\n",
            "Federated Round 517/1000\n",
            "Test set: Average loss: 0.8621, Accuracy: 110/178 (62%)\n",
            "Federated Round 518/1000\n",
            "Test set: Average loss: 0.9208, Accuracy: 109/178 (61%)\n",
            "Federated Round 519/1000\n",
            "Test set: Average loss: 0.8659, Accuracy: 111/178 (62%)\n",
            "Federated Round 520/1000\n",
            "Test set: Average loss: 0.8741, Accuracy: 111/178 (62%)\n",
            "Federated Round 521/1000\n",
            "Test set: Average loss: 0.8714, Accuracy: 110/178 (62%)\n",
            "Federated Round 522/1000\n",
            "Test set: Average loss: 0.8769, Accuracy: 110/178 (62%)\n",
            "Federated Round 523/1000\n",
            "Test set: Average loss: 0.9180, Accuracy: 111/178 (62%)\n",
            "Federated Round 524/1000\n",
            "Test set: Average loss: 0.9166, Accuracy: 111/178 (62%)\n",
            "Federated Round 525/1000\n",
            "Test set: Average loss: 0.8774, Accuracy: 115/178 (65%)\n",
            "Federated Round 526/1000\n",
            "Test set: Average loss: 0.8844, Accuracy: 115/178 (65%)\n",
            "Federated Round 527/1000\n",
            "Test set: Average loss: 0.8806, Accuracy: 113/178 (63%)\n",
            "Federated Round 528/1000\n",
            "Test set: Average loss: 0.8801, Accuracy: 110/178 (62%)\n",
            "Federated Round 529/1000\n",
            "Test set: Average loss: 0.8845, Accuracy: 107/178 (60%)\n",
            "Federated Round 530/1000\n",
            "Test set: Average loss: 0.8686, Accuracy: 110/178 (62%)\n",
            "Federated Round 531/1000\n",
            "Test set: Average loss: 0.8777, Accuracy: 112/178 (63%)\n",
            "Federated Round 532/1000\n",
            "Test set: Average loss: 0.8808, Accuracy: 110/178 (62%)\n",
            "Federated Round 533/1000\n",
            "Test set: Average loss: 0.8937, Accuracy: 114/178 (64%)\n",
            "Federated Round 534/1000\n",
            "Test set: Average loss: 0.9505, Accuracy: 108/178 (61%)\n",
            "Federated Round 535/1000\n",
            "Test set: Average loss: 0.9104, Accuracy: 112/178 (63%)\n",
            "Federated Round 536/1000\n",
            "Test set: Average loss: 0.8839, Accuracy: 114/178 (64%)\n",
            "Federated Round 537/1000\n",
            "Test set: Average loss: 0.8762, Accuracy: 107/178 (60%)\n",
            "Federated Round 538/1000\n",
            "Test set: Average loss: 0.8779, Accuracy: 107/178 (60%)\n",
            "Federated Round 539/1000\n",
            "Test set: Average loss: 0.8743, Accuracy: 110/178 (62%)\n",
            "Federated Round 540/1000\n",
            "Test set: Average loss: 0.8896, Accuracy: 106/178 (60%)\n",
            "Federated Round 541/1000\n",
            "Test set: Average loss: 0.9340, Accuracy: 104/178 (58%)\n",
            "Federated Round 542/1000\n",
            "Test set: Average loss: 0.8951, Accuracy: 106/178 (60%)\n",
            "Federated Round 543/1000\n",
            "Test set: Average loss: 0.8917, Accuracy: 113/178 (63%)\n",
            "Federated Round 544/1000\n",
            "Test set: Average loss: 0.8936, Accuracy: 114/178 (64%)\n",
            "Federated Round 545/1000\n",
            "Test set: Average loss: 0.8897, Accuracy: 107/178 (60%)\n",
            "Federated Round 546/1000\n",
            "Test set: Average loss: 0.9837, Accuracy: 104/178 (58%)\n",
            "Federated Round 547/1000\n",
            "Test set: Average loss: 0.9587, Accuracy: 104/178 (58%)\n",
            "Federated Round 548/1000\n",
            "Test set: Average loss: 1.0198, Accuracy: 105/178 (59%)\n",
            "Federated Round 549/1000\n",
            "Test set: Average loss: 0.9735, Accuracy: 104/178 (58%)\n",
            "Federated Round 550/1000\n",
            "Test set: Average loss: 0.9104, Accuracy: 105/178 (59%)\n",
            "Federated Round 551/1000\n",
            "Test set: Average loss: 0.8746, Accuracy: 110/178 (62%)\n",
            "Federated Round 552/1000\n",
            "Test set: Average loss: 0.8663, Accuracy: 110/178 (62%)\n",
            "Federated Round 553/1000\n",
            "Test set: Average loss: 0.8836, Accuracy: 106/178 (60%)\n",
            "Federated Round 554/1000\n",
            "Test set: Average loss: 1.0147, Accuracy: 109/178 (61%)\n",
            "Federated Round 555/1000\n",
            "Test set: Average loss: 0.8830, Accuracy: 106/178 (60%)\n",
            "Federated Round 556/1000\n",
            "Test set: Average loss: 0.9414, Accuracy: 105/178 (59%)\n",
            "Federated Round 557/1000\n",
            "Test set: Average loss: 0.9814, Accuracy: 103/178 (58%)\n",
            "Federated Round 558/1000\n",
            "Test set: Average loss: 0.9194, Accuracy: 104/178 (58%)\n",
            "Federated Round 559/1000\n",
            "Test set: Average loss: 0.8824, Accuracy: 107/178 (60%)\n",
            "Federated Round 560/1000\n",
            "Test set: Average loss: 0.9434, Accuracy: 105/178 (59%)\n",
            "Federated Round 561/1000\n",
            "Test set: Average loss: 0.9254, Accuracy: 105/178 (59%)\n",
            "Federated Round 562/1000\n",
            "Test set: Average loss: 0.8796, Accuracy: 110/178 (62%)\n",
            "Federated Round 563/1000\n",
            "Test set: Average loss: 0.8770, Accuracy: 111/178 (62%)\n",
            "Federated Round 564/1000\n",
            "Test set: Average loss: 1.0195, Accuracy: 104/178 (58%)\n",
            "Federated Round 565/1000\n",
            "Test set: Average loss: 0.8917, Accuracy: 107/178 (60%)\n",
            "Federated Round 566/1000\n",
            "Test set: Average loss: 0.8796, Accuracy: 110/178 (62%)\n",
            "Federated Round 567/1000\n",
            "Test set: Average loss: 0.8858, Accuracy: 108/178 (61%)\n",
            "Federated Round 568/1000\n",
            "Test set: Average loss: 0.9144, Accuracy: 105/178 (59%)\n",
            "Federated Round 569/1000\n",
            "Test set: Average loss: 0.9625, Accuracy: 103/178 (58%)\n",
            "Federated Round 570/1000\n",
            "Test set: Average loss: 0.9166, Accuracy: 105/178 (59%)\n",
            "Federated Round 571/1000\n",
            "Test set: Average loss: 0.9012, Accuracy: 115/178 (65%)\n",
            "Federated Round 572/1000\n",
            "Test set: Average loss: 0.8845, Accuracy: 114/178 (64%)\n",
            "Federated Round 573/1000\n",
            "Test set: Average loss: 0.8813, Accuracy: 111/178 (62%)\n",
            "Federated Round 574/1000\n",
            "Test set: Average loss: 0.8834, Accuracy: 108/178 (61%)\n",
            "Federated Round 575/1000\n",
            "Test set: Average loss: 0.8765, Accuracy: 111/178 (62%)\n",
            "Federated Round 576/1000\n",
            "Test set: Average loss: 0.8788, Accuracy: 115/178 (65%)\n",
            "Federated Round 577/1000\n",
            "Test set: Average loss: 0.9052, Accuracy: 114/178 (64%)\n",
            "Federated Round 578/1000\n",
            "Test set: Average loss: 0.9149, Accuracy: 110/178 (62%)\n",
            "Federated Round 579/1000\n",
            "Test set: Average loss: 0.8764, Accuracy: 116/178 (65%)\n",
            "Federated Round 580/1000\n",
            "Test set: Average loss: 0.9109, Accuracy: 111/178 (62%)\n",
            "Federated Round 581/1000\n",
            "Test set: Average loss: 0.9002, Accuracy: 111/178 (62%)\n",
            "Federated Round 582/1000\n",
            "Test set: Average loss: 0.8939, Accuracy: 117/178 (66%)\n",
            "Federated Round 583/1000\n",
            "Test set: Average loss: 0.9051, Accuracy: 111/178 (62%)\n",
            "Federated Round 584/1000\n",
            "Test set: Average loss: 0.9308, Accuracy: 108/178 (61%)\n",
            "Federated Round 585/1000\n",
            "Test set: Average loss: 0.8836, Accuracy: 115/178 (65%)\n",
            "Federated Round 586/1000\n",
            "Test set: Average loss: 0.8989, Accuracy: 115/178 (65%)\n",
            "Federated Round 587/1000\n",
            "Test set: Average loss: 0.8776, Accuracy: 112/178 (63%)\n",
            "Federated Round 588/1000\n",
            "Test set: Average loss: 0.8867, Accuracy: 109/178 (61%)\n",
            "Federated Round 589/1000\n",
            "Test set: Average loss: 0.8847, Accuracy: 109/178 (61%)\n",
            "Federated Round 590/1000\n",
            "Test set: Average loss: 0.9140, Accuracy: 111/178 (62%)\n",
            "Federated Round 591/1000\n",
            "Test set: Average loss: 0.8944, Accuracy: 115/178 (65%)\n",
            "Federated Round 592/1000\n",
            "Test set: Average loss: 0.8987, Accuracy: 116/178 (65%)\n",
            "Federated Round 593/1000\n",
            "Test set: Average loss: 0.9140, Accuracy: 111/178 (62%)\n",
            "Federated Round 594/1000\n",
            "Test set: Average loss: 0.8777, Accuracy: 112/178 (63%)\n",
            "Federated Round 595/1000\n",
            "Test set: Average loss: 0.8821, Accuracy: 114/178 (64%)\n",
            "Federated Round 596/1000\n",
            "Test set: Average loss: 0.8767, Accuracy: 114/178 (64%)\n",
            "Federated Round 597/1000\n",
            "Test set: Average loss: 0.8717, Accuracy: 111/178 (62%)\n",
            "Federated Round 598/1000\n",
            "Test set: Average loss: 0.9124, Accuracy: 104/178 (58%)\n",
            "Federated Round 599/1000\n",
            "Test set: Average loss: 0.8932, Accuracy: 115/178 (65%)\n",
            "Federated Round 600/1000\n",
            "Test set: Average loss: 0.8948, Accuracy: 115/178 (65%)\n",
            "Federated Round 601/1000\n",
            "Test set: Average loss: 0.8813, Accuracy: 111/178 (62%)\n",
            "Federated Round 602/1000\n",
            "Test set: Average loss: 0.8830, Accuracy: 107/178 (60%)\n",
            "Federated Round 603/1000\n",
            "Test set: Average loss: 0.9210, Accuracy: 111/178 (62%)\n",
            "Federated Round 604/1000\n",
            "Test set: Average loss: 0.8838, Accuracy: 115/178 (65%)\n",
            "Federated Round 605/1000\n",
            "Test set: Average loss: 0.8758, Accuracy: 111/178 (62%)\n",
            "Federated Round 606/1000\n",
            "Test set: Average loss: 0.8768, Accuracy: 109/178 (61%)\n",
            "Federated Round 607/1000\n",
            "Test set: Average loss: 0.8799, Accuracy: 110/178 (62%)\n",
            "Federated Round 608/1000\n",
            "Test set: Average loss: 0.8822, Accuracy: 109/178 (61%)\n",
            "Federated Round 609/1000\n",
            "Test set: Average loss: 0.8769, Accuracy: 114/178 (64%)\n",
            "Federated Round 610/1000\n",
            "Test set: Average loss: 0.8831, Accuracy: 114/178 (64%)\n",
            "Federated Round 611/1000\n",
            "Test set: Average loss: 0.8774, Accuracy: 114/178 (64%)\n",
            "Federated Round 612/1000\n",
            "Test set: Average loss: 0.8728, Accuracy: 109/178 (61%)\n",
            "Federated Round 613/1000\n",
            "Test set: Average loss: 0.9376, Accuracy: 104/178 (58%)\n",
            "Federated Round 614/1000\n",
            "Test set: Average loss: 1.0458, Accuracy: 108/178 (61%)\n",
            "Federated Round 615/1000\n",
            "Test set: Average loss: 0.9421, Accuracy: 104/178 (58%)\n",
            "Federated Round 616/1000\n",
            "Test set: Average loss: 0.8999, Accuracy: 104/178 (58%)\n",
            "Federated Round 617/1000\n",
            "Test set: Average loss: 0.9477, Accuracy: 105/178 (59%)\n",
            "Federated Round 618/1000\n",
            "Test set: Average loss: 0.8918, Accuracy: 113/178 (63%)\n",
            "Federated Round 619/1000\n",
            "Test set: Average loss: 0.8901, Accuracy: 114/178 (64%)\n",
            "Federated Round 620/1000\n",
            "Test set: Average loss: 0.9298, Accuracy: 111/178 (62%)\n",
            "Federated Round 621/1000\n",
            "Test set: Average loss: 0.8805, Accuracy: 110/178 (62%)\n",
            "Federated Round 622/1000\n",
            "Test set: Average loss: 0.9559, Accuracy: 104/178 (58%)\n",
            "Federated Round 623/1000\n",
            "Test set: Average loss: 0.9366, Accuracy: 111/178 (62%)\n",
            "Federated Round 624/1000\n",
            "Test set: Average loss: 0.8756, Accuracy: 113/178 (63%)\n",
            "Federated Round 625/1000\n",
            "Test set: Average loss: 0.8801, Accuracy: 112/178 (63%)\n",
            "Federated Round 626/1000\n",
            "Test set: Average loss: 0.8807, Accuracy: 111/178 (62%)\n",
            "Federated Round 627/1000\n",
            "Test set: Average loss: 0.8778, Accuracy: 113/178 (63%)\n",
            "Federated Round 628/1000\n",
            "Test set: Average loss: 0.8881, Accuracy: 106/178 (60%)\n",
            "Federated Round 629/1000\n",
            "Test set: Average loss: 0.8786, Accuracy: 110/178 (62%)\n",
            "Federated Round 630/1000\n",
            "Test set: Average loss: 0.8723, Accuracy: 112/178 (63%)\n",
            "Federated Round 631/1000\n",
            "Test set: Average loss: 0.8993, Accuracy: 113/178 (63%)\n",
            "Federated Round 632/1000\n",
            "Test set: Average loss: 0.9487, Accuracy: 109/178 (61%)\n",
            "Federated Round 633/1000\n",
            "Test set: Average loss: 0.8943, Accuracy: 105/178 (59%)\n",
            "Federated Round 634/1000\n",
            "Test set: Average loss: 0.8817, Accuracy: 112/178 (63%)\n",
            "Federated Round 635/1000\n",
            "Test set: Average loss: 0.8798, Accuracy: 112/178 (63%)\n",
            "Federated Round 636/1000\n",
            "Test set: Average loss: 0.8763, Accuracy: 112/178 (63%)\n",
            "Federated Round 637/1000\n",
            "Test set: Average loss: 0.9184, Accuracy: 112/178 (63%)\n",
            "Federated Round 638/1000\n",
            "Test set: Average loss: 0.9275, Accuracy: 104/178 (58%)\n",
            "Federated Round 639/1000\n",
            "Test set: Average loss: 1.0405, Accuracy: 111/178 (62%)\n",
            "Federated Round 640/1000\n",
            "Test set: Average loss: 1.0695, Accuracy: 110/178 (62%)\n",
            "Federated Round 641/1000\n",
            "Test set: Average loss: 0.9662, Accuracy: 104/178 (58%)\n",
            "Federated Round 642/1000\n",
            "Test set: Average loss: 0.9651, Accuracy: 104/178 (58%)\n",
            "Federated Round 643/1000\n",
            "Test set: Average loss: 0.9190, Accuracy: 105/178 (59%)\n",
            "Federated Round 644/1000\n",
            "Test set: Average loss: 1.0589, Accuracy: 106/178 (60%)\n",
            "Federated Round 645/1000\n",
            "Test set: Average loss: 0.9557, Accuracy: 105/178 (59%)\n",
            "Federated Round 646/1000\n",
            "Test set: Average loss: 0.8806, Accuracy: 113/178 (63%)\n",
            "Federated Round 647/1000\n",
            "Test set: Average loss: 0.8814, Accuracy: 111/178 (62%)\n",
            "Federated Round 648/1000\n",
            "Test set: Average loss: 0.9799, Accuracy: 113/178 (63%)\n",
            "Federated Round 649/1000\n",
            "Test set: Average loss: 0.9648, Accuracy: 110/178 (62%)\n",
            "Federated Round 650/1000\n",
            "Test set: Average loss: 0.9702, Accuracy: 110/178 (62%)\n",
            "Federated Round 651/1000\n",
            "Test set: Average loss: 0.9176, Accuracy: 111/178 (62%)\n",
            "Federated Round 652/1000\n",
            "Test set: Average loss: 0.8957, Accuracy: 117/178 (66%)\n",
            "Federated Round 653/1000\n",
            "Test set: Average loss: 0.8811, Accuracy: 114/178 (64%)\n",
            "Federated Round 654/1000\n",
            "Test set: Average loss: 0.9077, Accuracy: 105/178 (59%)\n",
            "Federated Round 655/1000\n",
            "Test set: Average loss: 0.9016, Accuracy: 112/178 (63%)\n",
            "Federated Round 656/1000\n",
            "Test set: Average loss: 0.9050, Accuracy: 115/178 (65%)\n",
            "Federated Round 657/1000\n",
            "Test set: Average loss: 0.8606, Accuracy: 116/178 (65%)\n",
            "Federated Round 658/1000\n",
            "Test set: Average loss: 0.9355, Accuracy: 110/178 (62%)\n",
            "Federated Round 659/1000\n",
            "Test set: Average loss: 1.2226, Accuracy: 109/178 (61%)\n",
            "Federated Round 660/1000\n",
            "Test set: Average loss: 1.2086, Accuracy: 109/178 (61%)\n",
            "Federated Round 661/1000\n",
            "Test set: Average loss: 0.9252, Accuracy: 111/178 (62%)\n",
            "Federated Round 662/1000\n",
            "Test set: Average loss: 0.8812, Accuracy: 116/178 (65%)\n",
            "Federated Round 663/1000\n",
            "Test set: Average loss: 0.8840, Accuracy: 115/178 (65%)\n",
            "Federated Round 664/1000\n",
            "Test set: Average loss: 1.1161, Accuracy: 108/178 (61%)\n",
            "Federated Round 665/1000\n",
            "Test set: Average loss: 1.0713, Accuracy: 113/178 (63%)\n",
            "Federated Round 666/1000\n",
            "Test set: Average loss: 0.9353, Accuracy: 112/178 (63%)\n",
            "Federated Round 667/1000\n",
            "Test set: Average loss: 1.0637, Accuracy: 110/178 (62%)\n",
            "Federated Round 668/1000\n",
            "Test set: Average loss: 1.1583, Accuracy: 108/178 (61%)\n",
            "Federated Round 669/1000\n",
            "Test set: Average loss: 0.9851, Accuracy: 110/178 (62%)\n",
            "Federated Round 670/1000\n",
            "Test set: Average loss: 0.8860, Accuracy: 114/178 (64%)\n",
            "Federated Round 671/1000\n",
            "Test set: Average loss: 0.8878, Accuracy: 114/178 (64%)\n",
            "Federated Round 672/1000\n",
            "Test set: Average loss: 0.9116, Accuracy: 108/178 (61%)\n",
            "Federated Round 673/1000\n",
            "Test set: Average loss: 0.8860, Accuracy: 111/178 (62%)\n",
            "Federated Round 674/1000\n",
            "Test set: Average loss: 0.8779, Accuracy: 114/178 (64%)\n",
            "Federated Round 675/1000\n",
            "Test set: Average loss: 0.8834, Accuracy: 111/178 (62%)\n",
            "Federated Round 676/1000\n",
            "Test set: Average loss: 0.8856, Accuracy: 113/178 (63%)\n",
            "Federated Round 677/1000\n",
            "Test set: Average loss: 0.9125, Accuracy: 106/178 (60%)\n",
            "Federated Round 678/1000\n",
            "Test set: Average loss: 0.8928, Accuracy: 113/178 (63%)\n",
            "Federated Round 679/1000\n",
            "Test set: Average loss: 0.8901, Accuracy: 113/178 (63%)\n",
            "Federated Round 680/1000\n",
            "Test set: Average loss: 0.8847, Accuracy: 113/178 (63%)\n",
            "Federated Round 681/1000\n",
            "Test set: Average loss: 0.8906, Accuracy: 108/178 (61%)\n",
            "Federated Round 682/1000\n",
            "Test set: Average loss: 0.8868, Accuracy: 114/178 (64%)\n",
            "Federated Round 683/1000\n",
            "Test set: Average loss: 0.8758, Accuracy: 115/178 (65%)\n",
            "Federated Round 684/1000\n",
            "Test set: Average loss: 0.9096, Accuracy: 106/178 (60%)\n",
            "Federated Round 685/1000\n",
            "Test set: Average loss: 1.1175, Accuracy: 111/178 (62%)\n",
            "Federated Round 686/1000\n",
            "Test set: Average loss: 0.9270, Accuracy: 106/178 (60%)\n",
            "Federated Round 687/1000\n",
            "Test set: Average loss: 0.8792, Accuracy: 115/178 (65%)\n",
            "Federated Round 688/1000\n",
            "Test set: Average loss: 0.8765, Accuracy: 114/178 (64%)\n",
            "Federated Round 689/1000\n",
            "Test set: Average loss: 0.9088, Accuracy: 111/178 (62%)\n",
            "Federated Round 690/1000\n",
            "Test set: Average loss: 0.9191, Accuracy: 112/178 (63%)\n",
            "Federated Round 691/1000\n",
            "Test set: Average loss: 0.8851, Accuracy: 114/178 (64%)\n",
            "Federated Round 692/1000\n",
            "Test set: Average loss: 0.9100, Accuracy: 114/178 (64%)\n",
            "Federated Round 693/1000\n",
            "Test set: Average loss: 0.9180, Accuracy: 113/178 (63%)\n",
            "Federated Round 694/1000\n",
            "Test set: Average loss: 0.8762, Accuracy: 119/178 (67%)\n",
            "Federated Round 695/1000\n",
            "Test set: Average loss: 0.8768, Accuracy: 113/178 (63%)\n",
            "Federated Round 696/1000\n",
            "Test set: Average loss: 0.9242, Accuracy: 111/178 (62%)\n",
            "Federated Round 697/1000\n",
            "Test set: Average loss: 0.8890, Accuracy: 114/178 (64%)\n",
            "Federated Round 698/1000\n",
            "Test set: Average loss: 0.8935, Accuracy: 113/178 (63%)\n",
            "Federated Round 699/1000\n",
            "Test set: Average loss: 0.8792, Accuracy: 114/178 (64%)\n",
            "Federated Round 700/1000\n",
            "Test set: Average loss: 0.8846, Accuracy: 115/178 (65%)\n",
            "Federated Round 701/1000\n",
            "Test set: Average loss: 0.8782, Accuracy: 113/178 (63%)\n",
            "Federated Round 702/1000\n",
            "Test set: Average loss: 0.8733, Accuracy: 114/178 (64%)\n",
            "Federated Round 703/1000\n",
            "Test set: Average loss: 0.8860, Accuracy: 115/178 (65%)\n",
            "Federated Round 704/1000\n",
            "Test set: Average loss: 0.8954, Accuracy: 116/178 (65%)\n",
            "Federated Round 705/1000\n",
            "Test set: Average loss: 0.8912, Accuracy: 115/178 (65%)\n",
            "Federated Round 706/1000\n",
            "Test set: Average loss: 0.8967, Accuracy: 109/178 (61%)\n",
            "Federated Round 707/1000\n",
            "Test set: Average loss: 0.9142, Accuracy: 112/178 (63%)\n",
            "Federated Round 708/1000\n",
            "Test set: Average loss: 0.8951, Accuracy: 115/178 (65%)\n",
            "Federated Round 709/1000\n",
            "Test set: Average loss: 0.9129, Accuracy: 117/178 (66%)\n",
            "Federated Round 710/1000\n",
            "Test set: Average loss: 0.8842, Accuracy: 115/178 (65%)\n",
            "Federated Round 711/1000\n",
            "Test set: Average loss: 0.8880, Accuracy: 116/178 (65%)\n",
            "Federated Round 712/1000\n",
            "Test set: Average loss: 0.8734, Accuracy: 123/178 (69%)\n",
            "Federated Round 713/1000\n",
            "Test set: Average loss: 0.8813, Accuracy: 117/178 (66%)\n",
            "Federated Round 714/1000\n",
            "Test set: Average loss: 0.8687, Accuracy: 113/178 (63%)\n",
            "Federated Round 715/1000\n",
            "Test set: Average loss: 0.9130, Accuracy: 112/178 (63%)\n",
            "Federated Round 716/1000\n",
            "Test set: Average loss: 0.9301, Accuracy: 111/178 (62%)\n",
            "Federated Round 717/1000\n",
            "Test set: Average loss: 0.8663, Accuracy: 114/178 (64%)\n",
            "Federated Round 718/1000\n",
            "Test set: Average loss: 0.9125, Accuracy: 111/178 (62%)\n",
            "Federated Round 719/1000\n",
            "Test set: Average loss: 0.9329, Accuracy: 111/178 (62%)\n",
            "Federated Round 720/1000\n",
            "Test set: Average loss: 0.9501, Accuracy: 111/178 (62%)\n",
            "Federated Round 721/1000\n",
            "Test set: Average loss: 0.9057, Accuracy: 111/178 (62%)\n",
            "Federated Round 722/1000\n",
            "Test set: Average loss: 0.9842, Accuracy: 110/178 (62%)\n",
            "Federated Round 723/1000\n",
            "Test set: Average loss: 0.9026, Accuracy: 111/178 (62%)\n",
            "Federated Round 724/1000\n",
            "Test set: Average loss: 0.9168, Accuracy: 110/178 (62%)\n",
            "Federated Round 725/1000\n",
            "Test set: Average loss: 0.9842, Accuracy: 110/178 (62%)\n",
            "Federated Round 726/1000\n",
            "Test set: Average loss: 1.0065, Accuracy: 110/178 (62%)\n",
            "Federated Round 727/1000\n",
            "Test set: Average loss: 0.9522, Accuracy: 109/178 (61%)\n",
            "Federated Round 728/1000\n",
            "Test set: Average loss: 1.0396, Accuracy: 110/178 (62%)\n",
            "Federated Round 729/1000\n",
            "Test set: Average loss: 0.9784, Accuracy: 109/178 (61%)\n",
            "Federated Round 730/1000\n",
            "Test set: Average loss: 1.0572, Accuracy: 114/178 (64%)\n",
            "Federated Round 731/1000\n",
            "Test set: Average loss: 0.9789, Accuracy: 113/178 (63%)\n",
            "Federated Round 732/1000\n",
            "Test set: Average loss: 1.1241, Accuracy: 111/178 (62%)\n",
            "Federated Round 733/1000\n",
            "Test set: Average loss: 1.0233, Accuracy: 114/178 (64%)\n",
            "Federated Round 734/1000\n",
            "Test set: Average loss: 1.0027, Accuracy: 114/178 (64%)\n",
            "Federated Round 735/1000\n",
            "Test set: Average loss: 0.9244, Accuracy: 114/178 (64%)\n",
            "Federated Round 736/1000\n",
            "Test set: Average loss: 0.8555, Accuracy: 121/178 (68%)\n",
            "Federated Round 737/1000\n",
            "Test set: Average loss: 0.8688, Accuracy: 116/178 (65%)\n",
            "Federated Round 738/1000\n",
            "Test set: Average loss: 0.9267, Accuracy: 115/178 (65%)\n",
            "Federated Round 739/1000\n",
            "Test set: Average loss: 0.8752, Accuracy: 118/178 (66%)\n",
            "Federated Round 740/1000\n",
            "Test set: Average loss: 0.8679, Accuracy: 123/178 (69%)\n",
            "Federated Round 741/1000\n",
            "Test set: Average loss: 1.0832, Accuracy: 113/178 (63%)\n",
            "Federated Round 742/1000\n",
            "Test set: Average loss: 0.9863, Accuracy: 113/178 (63%)\n",
            "Federated Round 743/1000\n",
            "Test set: Average loss: 1.0160, Accuracy: 114/178 (64%)\n",
            "Federated Round 744/1000\n",
            "Test set: Average loss: 1.2274, Accuracy: 109/178 (61%)\n",
            "Federated Round 745/1000\n",
            "Test set: Average loss: 1.2366, Accuracy: 109/178 (61%)\n",
            "Federated Round 746/1000\n",
            "Test set: Average loss: 0.8840, Accuracy: 117/178 (66%)\n",
            "Federated Round 747/1000\n",
            "Test set: Average loss: 0.9106, Accuracy: 114/178 (64%)\n",
            "Federated Round 748/1000\n",
            "Test set: Average loss: 0.8856, Accuracy: 114/178 (64%)\n",
            "Federated Round 749/1000\n",
            "Test set: Average loss: 0.9609, Accuracy: 109/178 (61%)\n",
            "Federated Round 750/1000\n",
            "Test set: Average loss: 0.9425, Accuracy: 110/178 (62%)\n",
            "Federated Round 751/1000\n",
            "Test set: Average loss: 0.9151, Accuracy: 112/178 (63%)\n",
            "Federated Round 752/1000\n",
            "Test set: Average loss: 0.9034, Accuracy: 114/178 (64%)\n",
            "Federated Round 753/1000\n",
            "Test set: Average loss: 0.8979, Accuracy: 113/178 (63%)\n",
            "Federated Round 754/1000\n",
            "Test set: Average loss: 0.9819, Accuracy: 110/178 (62%)\n",
            "Federated Round 755/1000\n",
            "Test set: Average loss: 1.0298, Accuracy: 111/178 (62%)\n",
            "Federated Round 756/1000\n",
            "Test set: Average loss: 1.1304, Accuracy: 112/178 (63%)\n",
            "Federated Round 757/1000\n",
            "Test set: Average loss: 0.9136, Accuracy: 107/178 (60%)\n",
            "Federated Round 758/1000\n",
            "Test set: Average loss: 0.9182, Accuracy: 110/178 (62%)\n",
            "Federated Round 759/1000\n",
            "Test set: Average loss: 0.9750, Accuracy: 110/178 (62%)\n",
            "Federated Round 760/1000\n",
            "Test set: Average loss: 1.0499, Accuracy: 114/178 (64%)\n",
            "Federated Round 761/1000\n",
            "Test set: Average loss: 0.9064, Accuracy: 113/178 (63%)\n",
            "Federated Round 762/1000\n",
            "Test set: Average loss: 0.9887, Accuracy: 113/178 (63%)\n",
            "Federated Round 763/1000\n",
            "Test set: Average loss: 1.0003, Accuracy: 112/178 (63%)\n",
            "Federated Round 764/1000\n",
            "Test set: Average loss: 1.0166, Accuracy: 113/178 (63%)\n",
            "Federated Round 765/1000\n",
            "Test set: Average loss: 1.0137, Accuracy: 109/178 (61%)\n",
            "Federated Round 766/1000\n",
            "Test set: Average loss: 0.9261, Accuracy: 112/178 (63%)\n",
            "Federated Round 767/1000\n",
            "Test set: Average loss: 0.9069, Accuracy: 108/178 (61%)\n",
            "Federated Round 768/1000\n",
            "Test set: Average loss: 1.1200, Accuracy: 113/178 (63%)\n",
            "Federated Round 769/1000\n",
            "Test set: Average loss: 1.0480, Accuracy: 110/178 (62%)\n",
            "Federated Round 770/1000\n",
            "Test set: Average loss: 0.9398, Accuracy: 112/178 (63%)\n",
            "Federated Round 771/1000\n",
            "Test set: Average loss: 1.0627, Accuracy: 114/178 (64%)\n",
            "Federated Round 772/1000\n",
            "Test set: Average loss: 1.0590, Accuracy: 111/178 (62%)\n",
            "Federated Round 773/1000\n",
            "Test set: Average loss: 1.1138, Accuracy: 109/178 (61%)\n",
            "Federated Round 774/1000\n",
            "Test set: Average loss: 1.1470, Accuracy: 108/178 (61%)\n",
            "Federated Round 775/1000\n",
            "Test set: Average loss: 1.0951, Accuracy: 112/178 (63%)\n",
            "Federated Round 776/1000\n",
            "Test set: Average loss: 0.9230, Accuracy: 107/178 (60%)\n",
            "Federated Round 777/1000\n",
            "Test set: Average loss: 0.9190, Accuracy: 113/178 (63%)\n",
            "Federated Round 778/1000\n",
            "Test set: Average loss: 1.0507, Accuracy: 113/178 (63%)\n",
            "Federated Round 779/1000\n",
            "Test set: Average loss: 1.0145, Accuracy: 112/178 (63%)\n",
            "Federated Round 780/1000\n",
            "Test set: Average loss: 0.9428, Accuracy: 111/178 (62%)\n",
            "Federated Round 781/1000\n",
            "Test set: Average loss: 1.0150, Accuracy: 113/178 (63%)\n",
            "Federated Round 782/1000\n",
            "Test set: Average loss: 1.0076, Accuracy: 112/178 (63%)\n",
            "Federated Round 783/1000\n",
            "Test set: Average loss: 1.0298, Accuracy: 112/178 (63%)\n",
            "Federated Round 784/1000\n",
            "Test set: Average loss: 0.9752, Accuracy: 113/178 (63%)\n",
            "Federated Round 785/1000\n",
            "Test set: Average loss: 1.0119, Accuracy: 112/178 (63%)\n",
            "Federated Round 786/1000\n",
            "Test set: Average loss: 0.9484, Accuracy: 114/178 (64%)\n",
            "Federated Round 787/1000\n",
            "Test set: Average loss: 0.9530, Accuracy: 113/178 (63%)\n",
            "Federated Round 788/1000\n",
            "Test set: Average loss: 0.9627, Accuracy: 114/178 (64%)\n",
            "Federated Round 789/1000\n",
            "Test set: Average loss: 0.9419, Accuracy: 115/178 (65%)\n",
            "Federated Round 790/1000\n",
            "Test set: Average loss: 0.9535, Accuracy: 114/178 (64%)\n",
            "Federated Round 791/1000\n",
            "Test set: Average loss: 0.9276, Accuracy: 113/178 (63%)\n",
            "Federated Round 792/1000\n",
            "Test set: Average loss: 1.0149, Accuracy: 112/178 (63%)\n",
            "Federated Round 793/1000\n",
            "Test set: Average loss: 0.9674, Accuracy: 111/178 (62%)\n",
            "Federated Round 794/1000\n",
            "Test set: Average loss: 1.0009, Accuracy: 112/178 (63%)\n",
            "Federated Round 795/1000\n",
            "Test set: Average loss: 0.9540, Accuracy: 113/178 (63%)\n",
            "Federated Round 796/1000\n",
            "Test set: Average loss: 0.8851, Accuracy: 119/178 (67%)\n",
            "Federated Round 797/1000\n",
            "Test set: Average loss: 0.8826, Accuracy: 119/178 (67%)\n",
            "Federated Round 798/1000\n",
            "Test set: Average loss: 0.9488, Accuracy: 113/178 (63%)\n",
            "Federated Round 799/1000\n",
            "Test set: Average loss: 0.9867, Accuracy: 112/178 (63%)\n",
            "Federated Round 800/1000\n",
            "Test set: Average loss: 0.9595, Accuracy: 113/178 (63%)\n",
            "Federated Round 801/1000\n",
            "Test set: Average loss: 0.9568, Accuracy: 113/178 (63%)\n",
            "Federated Round 802/1000\n",
            "Test set: Average loss: 0.9919, Accuracy: 113/178 (63%)\n",
            "Federated Round 803/1000\n",
            "Test set: Average loss: 0.9142, Accuracy: 116/178 (65%)\n",
            "Federated Round 804/1000\n",
            "Test set: Average loss: 0.9142, Accuracy: 116/178 (65%)\n",
            "Federated Round 805/1000\n",
            "Test set: Average loss: 0.9171, Accuracy: 117/178 (66%)\n",
            "Federated Round 806/1000\n",
            "Test set: Average loss: 0.9148, Accuracy: 117/178 (66%)\n",
            "Federated Round 807/1000\n",
            "Test set: Average loss: 0.8738, Accuracy: 125/178 (70%)\n",
            "Federated Round 808/1000\n",
            "Test set: Average loss: 0.8779, Accuracy: 123/178 (69%)\n",
            "Federated Round 809/1000\n",
            "Test set: Average loss: 0.8927, Accuracy: 116/178 (65%)\n",
            "Federated Round 810/1000\n",
            "Test set: Average loss: 1.0043, Accuracy: 113/178 (63%)\n",
            "Federated Round 811/1000\n",
            "Test set: Average loss: 1.0514, Accuracy: 114/178 (64%)\n",
            "Federated Round 812/1000\n",
            "Test set: Average loss: 1.0388, Accuracy: 114/178 (64%)\n",
            "Federated Round 813/1000\n",
            "Test set: Average loss: 1.1673, Accuracy: 111/178 (62%)\n",
            "Federated Round 814/1000\n",
            "Test set: Average loss: 1.0255, Accuracy: 113/178 (63%)\n",
            "Federated Round 815/1000\n",
            "Test set: Average loss: 0.9871, Accuracy: 112/178 (63%)\n",
            "Federated Round 816/1000\n",
            "Test set: Average loss: 1.0421, Accuracy: 113/178 (63%)\n",
            "Federated Round 817/1000\n",
            "Test set: Average loss: 0.9592, Accuracy: 114/178 (64%)\n",
            "Federated Round 818/1000\n",
            "Test set: Average loss: 1.1204, Accuracy: 113/178 (63%)\n",
            "Federated Round 819/1000\n",
            "Test set: Average loss: 1.0574, Accuracy: 114/178 (64%)\n",
            "Federated Round 820/1000\n",
            "Test set: Average loss: 1.2184, Accuracy: 111/178 (62%)\n",
            "Federated Round 821/1000\n",
            "Test set: Average loss: 1.3309, Accuracy: 111/178 (62%)\n",
            "Federated Round 822/1000\n",
            "Test set: Average loss: 1.1098, Accuracy: 113/178 (63%)\n",
            "Federated Round 823/1000\n",
            "Test set: Average loss: 1.1679, Accuracy: 111/178 (62%)\n",
            "Federated Round 824/1000\n",
            "Test set: Average loss: 1.0255, Accuracy: 112/178 (63%)\n",
            "Federated Round 825/1000\n",
            "Test set: Average loss: 1.0507, Accuracy: 112/178 (63%)\n",
            "Federated Round 826/1000\n",
            "Test set: Average loss: 1.1331, Accuracy: 113/178 (63%)\n",
            "Federated Round 827/1000\n",
            "Test set: Average loss: 0.9111, Accuracy: 118/178 (66%)\n",
            "Federated Round 828/1000\n",
            "Test set: Average loss: 0.8790, Accuracy: 118/178 (66%)\n",
            "Federated Round 829/1000\n",
            "Test set: Average loss: 0.8796, Accuracy: 113/178 (63%)\n",
            "Federated Round 830/1000\n",
            "Test set: Average loss: 0.8780, Accuracy: 122/178 (69%)\n",
            "Federated Round 831/1000\n",
            "Test set: Average loss: 0.8948, Accuracy: 118/178 (66%)\n",
            "Federated Round 832/1000\n",
            "Test set: Average loss: 0.8839, Accuracy: 119/178 (67%)\n",
            "Federated Round 833/1000\n",
            "Test set: Average loss: 0.8856, Accuracy: 120/178 (67%)\n",
            "Federated Round 834/1000\n",
            "Test set: Average loss: 0.9030, Accuracy: 118/178 (66%)\n",
            "Federated Round 835/1000\n",
            "Test set: Average loss: 0.8751, Accuracy: 124/178 (70%)\n",
            "Federated Round 836/1000\n",
            "Test set: Average loss: 0.8726, Accuracy: 123/178 (69%)\n",
            "Federated Round 837/1000\n",
            "Test set: Average loss: 0.8726, Accuracy: 124/178 (70%)\n",
            "Federated Round 838/1000\n",
            "Test set: Average loss: 0.8888, Accuracy: 117/178 (66%)\n",
            "Federated Round 839/1000\n",
            "Test set: Average loss: 0.9304, Accuracy: 115/178 (65%)\n",
            "Federated Round 840/1000\n",
            "Test set: Average loss: 0.9653, Accuracy: 112/178 (63%)\n",
            "Federated Round 841/1000\n",
            "Test set: Average loss: 0.9259, Accuracy: 115/178 (65%)\n",
            "Federated Round 842/1000\n",
            "Test set: Average loss: 0.8695, Accuracy: 122/178 (69%)\n",
            "Federated Round 843/1000\n",
            "Test set: Average loss: 0.8895, Accuracy: 118/178 (66%)\n",
            "Federated Round 844/1000\n",
            "Test set: Average loss: 0.9574, Accuracy: 113/178 (63%)\n",
            "Federated Round 845/1000\n",
            "Test set: Average loss: 0.8952, Accuracy: 117/178 (66%)\n",
            "Federated Round 846/1000\n",
            "Test set: Average loss: 0.9019, Accuracy: 118/178 (66%)\n",
            "Federated Round 847/1000\n",
            "Test set: Average loss: 0.9447, Accuracy: 115/178 (65%)\n",
            "Federated Round 848/1000\n",
            "Test set: Average loss: 0.9142, Accuracy: 118/178 (66%)\n",
            "Federated Round 849/1000\n",
            "Test set: Average loss: 0.8934, Accuracy: 119/178 (67%)\n",
            "Federated Round 850/1000\n",
            "Test set: Average loss: 0.9279, Accuracy: 116/178 (65%)\n",
            "Federated Round 851/1000\n",
            "Test set: Average loss: 1.0838, Accuracy: 114/178 (64%)\n",
            "Federated Round 852/1000\n",
            "Test set: Average loss: 1.0985, Accuracy: 113/178 (63%)\n",
            "Federated Round 853/1000\n",
            "Test set: Average loss: 1.3034, Accuracy: 110/178 (62%)\n",
            "Federated Round 854/1000\n",
            "Test set: Average loss: 1.1676, Accuracy: 111/178 (62%)\n",
            "Federated Round 855/1000\n",
            "Test set: Average loss: 1.1183, Accuracy: 113/178 (63%)\n",
            "Federated Round 856/1000\n",
            "Test set: Average loss: 1.0734, Accuracy: 114/178 (64%)\n",
            "Federated Round 857/1000\n",
            "Test set: Average loss: 1.0949, Accuracy: 114/178 (64%)\n",
            "Federated Round 858/1000\n",
            "Test set: Average loss: 0.9950, Accuracy: 113/178 (63%)\n",
            "Federated Round 859/1000\n",
            "Test set: Average loss: 0.8920, Accuracy: 118/178 (66%)\n",
            "Federated Round 860/1000\n",
            "Test set: Average loss: 0.9508, Accuracy: 114/178 (64%)\n",
            "Federated Round 861/1000\n",
            "Test set: Average loss: 0.9424, Accuracy: 114/178 (64%)\n",
            "Federated Round 862/1000\n",
            "Test set: Average loss: 0.9265, Accuracy: 115/178 (65%)\n",
            "Federated Round 863/1000\n",
            "Test set: Average loss: 0.8857, Accuracy: 122/178 (69%)\n",
            "Federated Round 864/1000\n",
            "Test set: Average loss: 0.8928, Accuracy: 121/178 (68%)\n",
            "Federated Round 865/1000\n",
            "Test set: Average loss: 0.9047, Accuracy: 119/178 (67%)\n",
            "Federated Round 866/1000\n",
            "Test set: Average loss: 0.9061, Accuracy: 119/178 (67%)\n",
            "Federated Round 867/1000\n",
            "Test set: Average loss: 0.9307, Accuracy: 117/178 (66%)\n",
            "Federated Round 868/1000\n",
            "Test set: Average loss: 0.9119, Accuracy: 117/178 (66%)\n",
            "Federated Round 869/1000\n",
            "Test set: Average loss: 0.8977, Accuracy: 119/178 (67%)\n",
            "Federated Round 870/1000\n",
            "Test set: Average loss: 0.9942, Accuracy: 112/178 (63%)\n",
            "Federated Round 871/1000\n",
            "Test set: Average loss: 1.1079, Accuracy: 114/178 (64%)\n",
            "Federated Round 872/1000\n",
            "Test set: Average loss: 1.0973, Accuracy: 114/178 (64%)\n",
            "Federated Round 873/1000\n",
            "Test set: Average loss: 0.9192, Accuracy: 118/178 (66%)\n",
            "Federated Round 874/1000\n",
            "Test set: Average loss: 0.9423, Accuracy: 115/178 (65%)\n",
            "Federated Round 875/1000\n",
            "Test set: Average loss: 0.9195, Accuracy: 119/178 (67%)\n",
            "Federated Round 876/1000\n",
            "Test set: Average loss: 0.9895, Accuracy: 113/178 (63%)\n",
            "Federated Round 877/1000\n",
            "Test set: Average loss: 0.9963, Accuracy: 112/178 (63%)\n",
            "Federated Round 878/1000\n",
            "Test set: Average loss: 0.9147, Accuracy: 118/178 (66%)\n",
            "Federated Round 879/1000\n",
            "Test set: Average loss: 0.9247, Accuracy: 118/178 (66%)\n",
            "Federated Round 880/1000\n",
            "Test set: Average loss: 0.9770, Accuracy: 114/178 (64%)\n",
            "Federated Round 881/1000\n",
            "Test set: Average loss: 0.9328, Accuracy: 118/178 (66%)\n",
            "Federated Round 882/1000\n",
            "Test set: Average loss: 0.9543, Accuracy: 115/178 (65%)\n",
            "Federated Round 883/1000\n",
            "Test set: Average loss: 0.9832, Accuracy: 113/178 (63%)\n",
            "Federated Round 884/1000\n",
            "Test set: Average loss: 0.9167, Accuracy: 118/178 (66%)\n",
            "Federated Round 885/1000\n",
            "Test set: Average loss: 0.9093, Accuracy: 119/178 (67%)\n",
            "Federated Round 886/1000\n",
            "Test set: Average loss: 0.9261, Accuracy: 118/178 (66%)\n",
            "Federated Round 887/1000\n",
            "Test set: Average loss: 0.9931, Accuracy: 113/178 (63%)\n",
            "Federated Round 888/1000\n",
            "Test set: Average loss: 0.9107, Accuracy: 118/178 (66%)\n",
            "Federated Round 889/1000\n",
            "Test set: Average loss: 0.9460, Accuracy: 116/178 (65%)\n",
            "Federated Round 890/1000\n",
            "Test set: Average loss: 1.0804, Accuracy: 113/178 (63%)\n",
            "Federated Round 891/1000\n",
            "Test set: Average loss: 0.9214, Accuracy: 118/178 (66%)\n",
            "Federated Round 892/1000\n",
            "Test set: Average loss: 0.9190, Accuracy: 118/178 (66%)\n",
            "Federated Round 893/1000\n",
            "Test set: Average loss: 1.0375, Accuracy: 113/178 (63%)\n",
            "Federated Round 894/1000\n",
            "Test set: Average loss: 1.0493, Accuracy: 112/178 (63%)\n",
            "Federated Round 895/1000\n",
            "Test set: Average loss: 1.0876, Accuracy: 114/178 (64%)\n",
            "Federated Round 896/1000\n",
            "Test set: Average loss: 1.2687, Accuracy: 110/178 (62%)\n",
            "Federated Round 897/1000\n",
            "Test set: Average loss: 1.2215, Accuracy: 109/178 (61%)\n",
            "Federated Round 898/1000\n",
            "Test set: Average loss: 1.1922, Accuracy: 111/178 (62%)\n",
            "Federated Round 899/1000\n",
            "Test set: Average loss: 1.0280, Accuracy: 112/178 (63%)\n",
            "Federated Round 900/1000\n",
            "Test set: Average loss: 1.0851, Accuracy: 114/178 (64%)\n",
            "Federated Round 901/1000\n",
            "Test set: Average loss: 1.0925, Accuracy: 113/178 (63%)\n",
            "Federated Round 902/1000\n",
            "Test set: Average loss: 1.0328, Accuracy: 112/178 (63%)\n",
            "Federated Round 903/1000\n",
            "Test set: Average loss: 1.0779, Accuracy: 113/178 (63%)\n",
            "Federated Round 904/1000\n",
            "Test set: Average loss: 1.1106, Accuracy: 114/178 (64%)\n",
            "Federated Round 905/1000\n",
            "Test set: Average loss: 1.0789, Accuracy: 113/178 (63%)\n",
            "Federated Round 906/1000\n",
            "Test set: Average loss: 1.0184, Accuracy: 113/178 (63%)\n",
            "Federated Round 907/1000\n",
            "Test set: Average loss: 1.0935, Accuracy: 114/178 (64%)\n",
            "Federated Round 908/1000\n",
            "Test set: Average loss: 1.2235, Accuracy: 111/178 (62%)\n",
            "Federated Round 909/1000\n",
            "Test set: Average loss: 1.2356, Accuracy: 110/178 (62%)\n",
            "Federated Round 910/1000\n",
            "Test set: Average loss: 1.0137, Accuracy: 113/178 (63%)\n",
            "Federated Round 911/1000\n",
            "Test set: Average loss: 0.9781, Accuracy: 114/178 (64%)\n",
            "Federated Round 912/1000\n",
            "Test set: Average loss: 0.9474, Accuracy: 115/178 (65%)\n",
            "Federated Round 913/1000\n",
            "Test set: Average loss: 0.9752, Accuracy: 114/178 (64%)\n",
            "Federated Round 914/1000\n",
            "Test set: Average loss: 0.9333, Accuracy: 116/178 (65%)\n",
            "Federated Round 915/1000\n",
            "Test set: Average loss: 0.9099, Accuracy: 117/178 (66%)\n",
            "Federated Round 916/1000\n",
            "Test set: Average loss: 1.0115, Accuracy: 113/178 (63%)\n",
            "Federated Round 917/1000\n",
            "Test set: Average loss: 1.0138, Accuracy: 113/178 (63%)\n",
            "Federated Round 918/1000\n",
            "Test set: Average loss: 0.9755, Accuracy: 114/178 (64%)\n",
            "Federated Round 919/1000\n",
            "Test set: Average loss: 0.9390, Accuracy: 116/178 (65%)\n",
            "Federated Round 920/1000\n",
            "Test set: Average loss: 1.0011, Accuracy: 114/178 (64%)\n",
            "Federated Round 921/1000\n",
            "Test set: Average loss: 1.1516, Accuracy: 113/178 (63%)\n",
            "Federated Round 922/1000\n",
            "Test set: Average loss: 1.1979, Accuracy: 110/178 (62%)\n",
            "Federated Round 923/1000\n",
            "Test set: Average loss: 1.0337, Accuracy: 113/178 (63%)\n",
            "Federated Round 924/1000\n",
            "Test set: Average loss: 1.0531, Accuracy: 112/178 (63%)\n",
            "Federated Round 925/1000\n",
            "Test set: Average loss: 1.1051, Accuracy: 114/178 (64%)\n",
            "Federated Round 926/1000\n",
            "Test set: Average loss: 1.0200, Accuracy: 113/178 (63%)\n",
            "Federated Round 927/1000\n",
            "Test set: Average loss: 0.9416, Accuracy: 115/178 (65%)\n",
            "Federated Round 928/1000\n",
            "Test set: Average loss: 0.9603, Accuracy: 114/178 (64%)\n",
            "Federated Round 929/1000\n",
            "Test set: Average loss: 0.9434, Accuracy: 115/178 (65%)\n",
            "Federated Round 930/1000\n",
            "Test set: Average loss: 1.0096, Accuracy: 114/178 (64%)\n",
            "Federated Round 931/1000\n",
            "Test set: Average loss: 1.1472, Accuracy: 112/178 (63%)\n",
            "Federated Round 932/1000\n",
            "Test set: Average loss: 1.0714, Accuracy: 114/178 (64%)\n",
            "Federated Round 933/1000\n",
            "Test set: Average loss: 1.0919, Accuracy: 114/178 (64%)\n",
            "Federated Round 934/1000\n",
            "Test set: Average loss: 1.0324, Accuracy: 112/178 (63%)\n",
            "Federated Round 935/1000\n",
            "Test set: Average loss: 0.9856, Accuracy: 114/178 (64%)\n",
            "Federated Round 936/1000\n",
            "Test set: Average loss: 0.9024, Accuracy: 119/178 (67%)\n",
            "Federated Round 937/1000\n",
            "Test set: Average loss: 0.8866, Accuracy: 125/178 (70%)\n",
            "Federated Round 938/1000\n",
            "Test set: Average loss: 0.8869, Accuracy: 127/178 (71%)\n",
            "Federated Round 939/1000\n",
            "Test set: Average loss: 0.8894, Accuracy: 121/178 (68%)\n",
            "Federated Round 940/1000\n",
            "Test set: Average loss: 1.0740, Accuracy: 113/178 (63%)\n",
            "Federated Round 941/1000\n",
            "Test set: Average loss: 0.9288, Accuracy: 116/178 (65%)\n",
            "Federated Round 942/1000\n",
            "Test set: Average loss: 0.8939, Accuracy: 120/178 (67%)\n",
            "Federated Round 943/1000\n",
            "Test set: Average loss: 0.9511, Accuracy: 114/178 (64%)\n",
            "Federated Round 944/1000\n",
            "Test set: Average loss: 0.9284, Accuracy: 115/178 (65%)\n",
            "Federated Round 945/1000\n",
            "Test set: Average loss: 0.9695, Accuracy: 114/178 (64%)\n",
            "Federated Round 946/1000\n",
            "Test set: Average loss: 0.9625, Accuracy: 115/178 (65%)\n",
            "Federated Round 947/1000\n",
            "Test set: Average loss: 1.0581, Accuracy: 112/178 (63%)\n",
            "Federated Round 948/1000\n",
            "Test set: Average loss: 1.0516, Accuracy: 113/178 (63%)\n",
            "Federated Round 949/1000\n",
            "Test set: Average loss: 0.9951, Accuracy: 113/178 (63%)\n",
            "Federated Round 950/1000\n",
            "Test set: Average loss: 0.9349, Accuracy: 115/178 (65%)\n",
            "Federated Round 951/1000\n",
            "Test set: Average loss: 0.9039, Accuracy: 118/178 (66%)\n",
            "Federated Round 952/1000\n",
            "Test set: Average loss: 0.8963, Accuracy: 120/178 (67%)\n",
            "Federated Round 953/1000\n",
            "Test set: Average loss: 0.9151, Accuracy: 117/178 (66%)\n",
            "Federated Round 954/1000\n",
            "Test set: Average loss: 0.8812, Accuracy: 121/178 (68%)\n",
            "Federated Round 955/1000\n",
            "Test set: Average loss: 0.9823, Accuracy: 113/178 (63%)\n",
            "Federated Round 956/1000\n",
            "Test set: Average loss: 0.9020, Accuracy: 118/178 (66%)\n",
            "Federated Round 957/1000\n",
            "Test set: Average loss: 0.8898, Accuracy: 121/178 (68%)\n",
            "Federated Round 958/1000\n",
            "Test set: Average loss: 0.8834, Accuracy: 124/178 (70%)\n",
            "Federated Round 959/1000\n",
            "Test set: Average loss: 0.8824, Accuracy: 119/178 (67%)\n",
            "Federated Round 960/1000\n",
            "Test set: Average loss: 0.9311, Accuracy: 117/178 (66%)\n",
            "Federated Round 961/1000\n",
            "Test set: Average loss: 0.9652, Accuracy: 116/178 (65%)\n",
            "Federated Round 962/1000\n",
            "Test set: Average loss: 1.0825, Accuracy: 114/178 (64%)\n",
            "Federated Round 963/1000\n",
            "Test set: Average loss: 0.9930, Accuracy: 113/178 (63%)\n",
            "Federated Round 964/1000\n",
            "Test set: Average loss: 0.9965, Accuracy: 113/178 (63%)\n",
            "Federated Round 965/1000\n",
            "Test set: Average loss: 1.0619, Accuracy: 113/178 (63%)\n",
            "Federated Round 966/1000\n",
            "Test set: Average loss: 1.0666, Accuracy: 113/178 (63%)\n",
            "Federated Round 967/1000\n",
            "Test set: Average loss: 1.0395, Accuracy: 112/178 (63%)\n",
            "Federated Round 968/1000\n",
            "Test set: Average loss: 1.0032, Accuracy: 113/178 (63%)\n",
            "Federated Round 969/1000\n",
            "Test set: Average loss: 0.9892, Accuracy: 113/178 (63%)\n",
            "Federated Round 970/1000\n",
            "Test set: Average loss: 0.9792, Accuracy: 113/178 (63%)\n",
            "Federated Round 971/1000\n",
            "Test set: Average loss: 1.0847, Accuracy: 114/178 (64%)\n",
            "Federated Round 972/1000\n",
            "Test set: Average loss: 1.2748, Accuracy: 109/178 (61%)\n",
            "Federated Round 973/1000\n",
            "Test set: Average loss: 1.1539, Accuracy: 112/178 (63%)\n",
            "Federated Round 974/1000\n",
            "Test set: Average loss: 0.9701, Accuracy: 114/178 (64%)\n",
            "Federated Round 975/1000\n",
            "Test set: Average loss: 0.9153, Accuracy: 118/178 (66%)\n",
            "Federated Round 976/1000\n",
            "Test set: Average loss: 0.9576, Accuracy: 114/178 (64%)\n",
            "Federated Round 977/1000\n",
            "Test set: Average loss: 1.0804, Accuracy: 114/178 (64%)\n",
            "Federated Round 978/1000\n",
            "Test set: Average loss: 1.2653, Accuracy: 108/178 (61%)\n",
            "Federated Round 979/1000\n",
            "Test set: Average loss: 1.0031, Accuracy: 112/178 (63%)\n",
            "Federated Round 980/1000\n",
            "Test set: Average loss: 0.9645, Accuracy: 114/178 (64%)\n",
            "Federated Round 981/1000\n",
            "Test set: Average loss: 1.1453, Accuracy: 113/178 (63%)\n",
            "Federated Round 982/1000\n",
            "Test set: Average loss: 1.2180, Accuracy: 111/178 (62%)\n",
            "Federated Round 983/1000\n",
            "Test set: Average loss: 1.2248, Accuracy: 110/178 (62%)\n",
            "Federated Round 984/1000\n",
            "Test set: Average loss: 1.1877, Accuracy: 112/178 (63%)\n",
            "Federated Round 985/1000\n",
            "Test set: Average loss: 1.1697, Accuracy: 113/178 (63%)\n",
            "Federated Round 986/1000\n",
            "Test set: Average loss: 1.1181, Accuracy: 113/178 (63%)\n",
            "Federated Round 987/1000\n",
            "Test set: Average loss: 0.9627, Accuracy: 115/178 (65%)\n",
            "Federated Round 988/1000\n",
            "Test set: Average loss: 0.9833, Accuracy: 114/178 (64%)\n",
            "Federated Round 989/1000\n",
            "Test set: Average loss: 1.0485, Accuracy: 113/178 (63%)\n",
            "Federated Round 990/1000\n",
            "Test set: Average loss: 0.9723, Accuracy: 115/178 (65%)\n",
            "Federated Round 991/1000\n",
            "Test set: Average loss: 0.9249, Accuracy: 118/178 (66%)\n",
            "Federated Round 992/1000\n",
            "Test set: Average loss: 0.9987, Accuracy: 114/178 (64%)\n",
            "Federated Round 993/1000\n",
            "Test set: Average loss: 0.9911, Accuracy: 114/178 (64%)\n",
            "Federated Round 994/1000\n",
            "Test set: Average loss: 1.0091, Accuracy: 113/178 (63%)\n",
            "Federated Round 995/1000\n",
            "Test set: Average loss: 0.9357, Accuracy: 116/178 (65%)\n",
            "Federated Round 996/1000\n",
            "Test set: Average loss: 0.9814, Accuracy: 112/178 (63%)\n",
            "Federated Round 997/1000\n",
            "Test set: Average loss: 0.9375, Accuracy: 113/178 (63%)\n",
            "Federated Round 998/1000\n",
            "Test set: Average loss: 0.9247, Accuracy: 117/178 (66%)\n",
            "Federated Round 999/1000\n",
            "Test set: Average loss: 0.9650, Accuracy: 115/178 (65%)\n",
            "Federated Round 1000/1000\n",
            "Test set: Average loss: 1.0895, Accuracy: 112/178 (63%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7d5f4d72ada0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPh88GSdDZai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
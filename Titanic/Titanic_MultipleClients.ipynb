{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Prototype 1 with 5 seeds\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "# For random seeds-\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# Load data\n",
        "def load_titanic_data(num_participants):\n",
        "    def _bin_age(age_series):\n",
        "        bins = [-np.inf, 10, 40, np.inf]\n",
        "        labels = [\"Child\", \"Adult\", \"Elderly\"]\n",
        "        return pd.cut(age_series, bins=bins, labels=labels, right=True).astype(str).replace(\"nan\", \"Unknown\")\n",
        "\n",
        "    def _extract_title(name_series):\n",
        "        titles = name_series.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
        "        rare_titles = {\n",
        "            \"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"\n",
        "        }\n",
        "        titles = titles.replace(list(rare_titles), \"Rare\")\n",
        "        titles = titles.replace({\"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\"})\n",
        "        return titles\n",
        "\n",
        "    def _create_features(df):\n",
        "        df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\")\n",
        "        df[\"Age\"] = _bin_age(df[\"Age\"])\n",
        "        df[\"Cabin\"] = df[\"Cabin\"].str[0].fillna(\"Unknown\")\n",
        "        df[\"Title\"] = _extract_title(df[\"Name\"])\n",
        "        df.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"], inplace=True)\n",
        "        return df\n",
        "\n",
        "    def vertical_partition_rotating(df, num_participants):\n",
        "        partitions = [[] for _ in range(num_participants)]\n",
        "        num_features = df.shape[1]\n",
        "\n",
        "        for i, feature in enumerate(df.columns):\n",
        "            participant = i % num_participants\n",
        "            partitions[participant].append(feature)\n",
        "\n",
        "        partitioned_dfs = [pd.get_dummies(df[features]) for features in partitions]\n",
        "        return partitioned_dfs\n",
        "\n",
        "    def get_partitions_and_label():\n",
        "        df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "        processed_df = df.dropna(subset=[\"Embarked\", \"Fare\"]).copy()\n",
        "        processed_df = _create_features(processed_df)\n",
        "        labels = processed_df[\"Survived\"].values\n",
        "\n",
        "        train_df, test_df, y_train, y_test = train_test_split(processed_df, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "        train_partitions = vertical_partition_rotating(train_df.drop(columns=[\"Survived\"]), num_participants)\n",
        "        test_partitions = vertical_partition_rotating(test_df.drop(columns=[\"Survived\"]), num_participants)\n",
        "\n",
        "        for i in range(len(train_partitions)):\n",
        "            train_partitions[i]['Survived'] = y_train\n",
        "\n",
        "        for i in range(len(test_partitions)):\n",
        "            test_partitions[i]['Survived'] = y_test\n",
        "\n",
        "        return train_partitions, test_partitions, y_train, y_test\n",
        "\n",
        "    train_partitions, test_partitions, y_train, y_test = get_partitions_and_label()\n",
        "\n",
        "    def create_tensor_datasets(partitions):\n",
        "        tensor_partitions = []\n",
        "        for partition in partitions:\n",
        "            partition = partition.apply(pd.to_numeric, errors='coerce')\n",
        "            partition = partition.fillna(0)\n",
        "\n",
        "            for col in partition.select_dtypes(include=['bool']).columns:\n",
        "                partition[col] = partition[col].astype(int)\n",
        "\n",
        "            features = partition.drop(columns=[\"Survived\"]).values\n",
        "            labels = partition[\"Survived\"].values.astype(np.int64)\n",
        "\n",
        "            tensor_partition = TensorDataset(torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long))\n",
        "            tensor_partitions.append(tensor_partition)\n",
        "        return tensor_partitions\n",
        "\n",
        "    train_tensor_partitions = create_tensor_datasets(train_partitions)\n",
        "    test_tensor_partitions = create_tensor_datasets(test_partitions)\n",
        "\n",
        "    return train_tensor_partitions, test_tensor_partitions, train_partitions, y_train, y_test\n",
        "\n",
        "\n",
        "class GlobalModel(nn.Module):\n",
        "    def __init__(self, input_sizes, hidden_sizes, output_size):\n",
        "        super(GlobalModel, self).__init__()\n",
        "        self.segments = nn.ModuleList()\n",
        "        for input_size, hidden_size in zip(input_sizes, hidden_sizes):\n",
        "            layers = [nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, output_size)]\n",
        "            self.segments.append(nn.Sequential(*layers))\n",
        "\n",
        "    def forward(self, x, active_segments):\n",
        "        segment_outputs = []\n",
        "        start_index = 0\n",
        "        for i, segment in enumerate(self.segments):\n",
        "            end_index = start_index + input_sizes[i]\n",
        "            if i in active_segments:\n",
        "                segment_input = x[:, start_index:end_index]\n",
        "                segment_output = segment(segment_input)\n",
        "                segment_outputs.append(segment_output)\n",
        "            else:\n",
        "                segment_outputs.append(torch.zeros(x.size(0), output_size, device=x.device))\n",
        "            start_index = end_index\n",
        "        combined_output = torch.mean(torch.stack(segment_outputs), dim=0)\n",
        "        return combined_output\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, input_sizes, participant_id):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        data = data.view(data.size(0), -1)\n",
        "        padded_data = torch.zeros(data.size(0), sum(input_sizes)).to(device)\n",
        "        start_index = sum(input_sizes[:participant_id])\n",
        "        end_index = start_index + input_sizes[participant_id]\n",
        "        padded_data[:, start_index:end_index] = data\n",
        "        optimizer.zero_grad()\n",
        "        output = model(padded_data, active_segments=[participant_id])\n",
        "        loss = nn.CrossEntropyLoss(weight=class_weights)(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def selective_exchange_gradients(models, input_sizes, hidden_sizes):\n",
        "    num_models = len(models)\n",
        "    param_indices = [0]\n",
        "    cumulative_index = 0\n",
        "    for i in range(len(hidden_sizes)):\n",
        "        cumulative_index += (input_sizes[i] * hidden_sizes[i]) + hidden_sizes[i]\n",
        "        param_indices.append(cumulative_index)\n",
        "\n",
        "    for seg in range(len(hidden_sizes)):\n",
        "        start = param_indices[seg]\n",
        "        end = param_indices[seg + 1]\n",
        "        for param_idx in range(start, end):\n",
        "            grads = []\n",
        "            for model in models:\n",
        "                model_params = list(model.parameters())\n",
        "                if param_idx < len(model_params) and model_params[param_idx].grad is not None:\n",
        "                    grads.append(model_params[param_idx].grad)\n",
        "            if grads:\n",
        "                avg_grad = torch.stack(grads).mean(dim=0)\n",
        "                for model in models:\n",
        "                    model_params = list(model.parameters())\n",
        "                    if param_idx < len(model_params):\n",
        "                        model_params[param_idx].grad = avg_grad.clone()\n",
        "\n",
        "\n",
        "def evaluate(models, device, test_loaders):\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        for batch_data in zip(*test_loaders):\n",
        "            data_list = []\n",
        "            target_list = []\n",
        "            for participant_id, (data, target) in enumerate(batch_data):\n",
        "                data_list.append(data)\n",
        "                target_list.append(target)\n",
        "\n",
        "            target = target_list[0].to(device)\n",
        "            for t in target_list:\n",
        "                assert torch.equal(t, target), \"Targets are not consistent across participants\"\n",
        "\n",
        "            data_combined = torch.cat(data_list, dim=1).to(device)\n",
        "\n",
        "            padded_data = torch.zeros(data_combined.size(0), sum(input_sizes)).to(device)\n",
        "            start_index = 0\n",
        "            for participant_id in range(len(input_sizes)):\n",
        "                end_index = start_index + input_sizes[participant_id]\n",
        "                if end_index <= data_combined.size(1):\n",
        "                    padded_data[:, start_index:end_index] = data_combined[:, start_index:end_index]\n",
        "                else:\n",
        "                    adjusted_end_index = data_combined.size(1)\n",
        "                    padded_data[:, start_index:adjusted_end_index] = data_combined[:, start_index:adjusted_end_index]\n",
        "                start_index = end_index\n",
        "\n",
        "            outputs = torch.zeros(data_combined.size(0), 2, device=device)\n",
        "            for model in models:\n",
        "                output = model(padded_data, active_segments=list(range(len(model.segments))))\n",
        "                outputs += output\n",
        "            outputs /= len(models)\n",
        "            test_loss += nn.CrossEntropyLoss(reduction='sum')(outputs, target).item()\n",
        "            pred = outputs.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            all_preds.extend(pred.view(-1).cpu().numpy())\n",
        "            all_targets.extend(target.view(-1).cpu().numpy())\n",
        "\n",
        "        test_loss /= len(test_loaders[0].dataset)\n",
        "        accuracy = 100. * correct / len(test_loaders[0].dataset)\n",
        "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
        "        #print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loaders[0].dataset)} ({accuracy:.0f}%), F1-score: {f1:.4f}')\n",
        "\n",
        "        return accuracy, f1\n",
        "\n",
        "\n",
        "# Running the experiment for multiple seeds\n",
        "federated_rounds = 1000\n",
        "epochs_per_round = 1\n",
        "hidden_size = 10  # Single hidden layer\n",
        "\n",
        "seeds = [42, 55, 77, 101, 123]  # List of 5 different seeds\n",
        "final_accuracies = []\n",
        "final_f1_scores = []\n",
        "\n",
        "for seed in seeds:\n",
        "    set_random_seed(seed)\n",
        "    print(f\"\\nRunning VFL with seed {seed}...\")\n",
        "\n",
        "    avg_accuracy = 0\n",
        "    avg_f1 = 0\n",
        "\n",
        "    for num_participants in range(2, 10):\n",
        "      print(f\"Running VFL with {num_participants} participants...\")\n",
        "\n",
        "      train_tensor_partitions, test_tensor_partitions, feature_partitions, y_train, y_test = load_titanic_data(num_participants)\n",
        "\n",
        "      input_sizes = [partition.shape[1] - 1 for partition in feature_partitions]\n",
        "      hidden_sizes = [10] * num_participants\n",
        "      output_size = 2\n",
        "\n",
        "      models = [GlobalModel(input_sizes, hidden_sizes, output_size).to(device) for _ in range(num_participants)]\n",
        "      optimizers = [optim.Adam(model.parameters(), lr=0.01) for model in models]\n",
        "\n",
        "      class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "      class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "      best_accuracy = 0\n",
        "      best_f1 = 0\n",
        "\n",
        "      for federated_round in range(federated_rounds):\n",
        "          #print(f\"Federated Round {federated_round + 1}/{federated_rounds}\")\n",
        "          for participant_id in range(num_participants):\n",
        "              train_loader = DataLoader(train_tensor_partitions[participant_id], batch_size=64, shuffle=True)\n",
        "              for epoch in range(1, epochs_per_round + 1):\n",
        "                train(models[participant_id], device, train_loader, optimizers[participant_id], epoch, input_sizes, participant_id)\n",
        "\n",
        "          selective_exchange_gradients(models, input_sizes, hidden_sizes)\n",
        "\n",
        "          test_loaders = [DataLoader(test_tensor_partitions[i], batch_size=32, shuffle=False) for i in range(num_participants)]\n",
        "          accuracy, f1 = evaluate(models, device, test_loaders)\n",
        "\n",
        "          if accuracy > best_accuracy:\n",
        "              best_accuracy = accuracy\n",
        "          if f1 > best_f1:\n",
        "              best_f1 = f1\n",
        "\n",
        "      print(f'Best Accuracy with {num_participants} participants: {best_accuracy:.2f}%')\n",
        "      print(f'Best F1-Score with {num_participants} participants: {best_f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTKjBkI1k5eE",
        "outputId": "fd9bd24a-6ad2-46ff-99aa-e89d5bb18010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running VFL with seed 42...\n",
            "Running VFL with 2 participants...\n",
            "Best Accuracy with 2 participants: 65.73%\n",
            "Best F1-Score with 2 participants: 0.5431\n",
            "Running VFL with 3 participants...\n",
            "Best Accuracy with 3 participants: 71.35%\n",
            "Best F1-Score with 3 participants: 0.6694\n",
            "Running VFL with 4 participants...\n",
            "Best Accuracy with 4 participants: 81.46%\n",
            "Best F1-Score with 4 participants: 0.8042\n",
            "Running VFL with 5 participants...\n",
            "Best Accuracy with 5 participants: 77.53%\n",
            "Best F1-Score with 5 participants: 0.7543\n",
            "Running VFL with 6 participants...\n",
            "Best Accuracy with 6 participants: 61.24%\n",
            "Best F1-Score with 6 participants: 0.3810\n",
            "Running VFL with 7 participants...\n",
            "Best Accuracy with 7 participants: 81.46%\n",
            "Best F1-Score with 7 participants: 0.8096\n",
            "Running VFL with 8 participants...\n",
            "Best Accuracy with 8 participants: 74.72%\n",
            "Best F1-Score with 8 participants: 0.7382\n",
            "Running VFL with 9 participants...\n",
            "Best Accuracy with 9 participants: 74.72%\n",
            "Best F1-Score with 9 participants: 0.7282\n",
            "\n",
            "Running VFL with seed 55...\n",
            "Running VFL with 2 participants...\n",
            "Best Accuracy with 2 participants: 63.48%\n",
            "Best F1-Score with 2 participants: 0.4400\n",
            "Running VFL with 3 participants...\n",
            "Best Accuracy with 3 participants: 61.24%\n",
            "Best F1-Score with 3 participants: 0.3798\n",
            "Running VFL with 4 participants...\n",
            "Best Accuracy with 4 participants: 82.02%\n",
            "Best F1-Score with 4 participants: 0.8126\n",
            "Running VFL with 5 participants...\n",
            "Best Accuracy with 5 participants: 72.47%\n",
            "Best F1-Score with 5 participants: 0.7093\n",
            "Running VFL with 6 participants...\n",
            "Best Accuracy with 6 participants: 61.24%\n",
            "Best F1-Score with 6 participants: 0.3798\n",
            "Running VFL with 7 participants...\n",
            "Best Accuracy with 7 participants: 78.65%\n",
            "Best F1-Score with 7 participants: 0.7774\n",
            "Running VFL with 8 participants...\n",
            "Best Accuracy with 8 participants: 79.78%\n",
            "Best F1-Score with 8 participants: 0.7891\n",
            "Running VFL with 9 participants...\n",
            "Best Accuracy with 9 participants: 72.47%\n",
            "Best F1-Score with 9 participants: 0.7136\n",
            "\n",
            "Running VFL with seed 77...\n",
            "Running VFL with 2 participants...\n",
            "Best Accuracy with 2 participants: 69.10%\n",
            "Best F1-Score with 2 participants: 0.6584\n",
            "Running VFL with 3 participants...\n",
            "Best Accuracy with 3 participants: 63.48%\n",
            "Best F1-Score with 3 participants: 0.5205\n",
            "Running VFL with 4 participants...\n",
            "Best Accuracy with 4 participants: 83.15%\n",
            "Best F1-Score with 4 participants: 0.8234\n",
            "Running VFL with 5 participants...\n",
            "Best Accuracy with 5 participants: 71.91%\n",
            "Best F1-Score with 5 participants: 0.6642\n",
            "Running VFL with 6 participants...\n",
            "Best Accuracy with 6 participants: 61.80%\n",
            "Best F1-Score with 6 participants: 0.5692\n",
            "Running VFL with 7 participants...\n",
            "Best Accuracy with 7 participants: 64.04%\n",
            "Best F1-Score with 7 participants: 0.4841\n",
            "Running VFL with 8 participants...\n",
            "Best Accuracy with 8 participants: 80.34%\n",
            "Best F1-Score with 8 participants: 0.7987\n",
            "Running VFL with 9 participants...\n",
            "Best Accuracy with 9 participants: 72.47%\n",
            "Best F1-Score with 9 participants: 0.6929\n",
            "\n",
            "Running VFL with seed 101...\n",
            "Running VFL with 2 participants...\n",
            "Best Accuracy with 2 participants: 65.17%\n",
            "Best F1-Score with 2 participants: 0.5002\n",
            "Running VFL with 3 participants...\n",
            "Best Accuracy with 3 participants: 62.92%\n",
            "Best F1-Score with 3 participants: 0.5666\n",
            "Running VFL with 4 participants...\n",
            "Best Accuracy with 4 participants: 79.21%\n",
            "Best F1-Score with 4 participants: 0.7751\n",
            "Running VFL with 5 participants...\n",
            "Best Accuracy with 5 participants: 76.97%\n",
            "Best F1-Score with 5 participants: 0.7593\n",
            "Running VFL with 6 participants...\n",
            "Best Accuracy with 6 participants: 61.80%\n",
            "Best F1-Score with 6 participants: 0.4050\n",
            "Running VFL with 7 participants...\n",
            "Best Accuracy with 7 participants: 76.40%\n",
            "Best F1-Score with 7 participants: 0.7562\n",
            "Running VFL with 8 participants...\n",
            "Best Accuracy with 8 participants: 66.29%\n",
            "Best F1-Score with 8 participants: 0.5334\n",
            "Running VFL with 9 participants...\n",
            "Best Accuracy with 9 participants: 74.72%\n",
            "Best F1-Score with 9 participants: 0.7231\n",
            "\n",
            "Running VFL with seed 123...\n",
            "Running VFL with 2 participants...\n",
            "Best Accuracy with 2 participants: 62.92%\n",
            "Best F1-Score with 2 participants: 0.4370\n",
            "Running VFL with 3 participants...\n",
            "Best Accuracy with 3 participants: 64.04%\n",
            "Best F1-Score with 3 participants: 0.5140\n",
            "Running VFL with 4 participants...\n",
            "Best Accuracy with 4 participants: 74.72%\n",
            "Best F1-Score with 4 participants: 0.7449\n",
            "Running VFL with 5 participants...\n",
            "Best Accuracy with 5 participants: 70.79%\n",
            "Best F1-Score with 5 participants: 0.6507\n",
            "Running VFL with 6 participants...\n",
            "Best Accuracy with 6 participants: 66.29%\n",
            "Best F1-Score with 6 participants: 0.6181\n",
            "Running VFL with 7 participants...\n",
            "Best Accuracy with 7 participants: 75.84%\n",
            "Best F1-Score with 7 participants: 0.7351\n",
            "Running VFL with 8 participants...\n",
            "Best Accuracy with 8 participants: 79.78%\n",
            "Best F1-Score with 8 participants: 0.7819\n",
            "Running VFL with 9 participants...\n",
            "Best Accuracy with 9 participants: 70.79%\n",
            "Best F1-Score with 9 participants: 0.6758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prototype 2 with 5 seeds\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# For random seeds-\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Preprocessing Titanic dataset (same as before)\n",
        "def load_titanic_data(num_participants):\n",
        "    def _bin_age(age_series):\n",
        "        bins = [-np.inf, 10, 40, np.inf]\n",
        "        labels = [\"Child\", \"Adult\", \"Elderly\"]\n",
        "        return pd.cut(age_series, bins=bins, labels=labels, right=True).astype(str).replace(\"nan\", \"Unknown\")\n",
        "\n",
        "    def _extract_title(name_series):\n",
        "        titles = name_series.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
        "        rare_titles = {\n",
        "            \"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"\n",
        "        }\n",
        "        titles = titles.replace(list(rare_titles), \"Rare\")\n",
        "        titles = titles.replace({\"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\"})\n",
        "        return titles\n",
        "\n",
        "    def _create_features(df):\n",
        "        df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\")\n",
        "        df[\"Age\"] = _bin_age(df[\"Age\"])\n",
        "        df[\"Cabin\"] = df[\"Cabin\"].str[0].fillna(\"Unknown\")\n",
        "        df[\"Title\"] = _extract_title(df[\"Name\"])\n",
        "        df.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"], inplace=True)\n",
        "        return df\n",
        "\n",
        "    def vertical_partition_rotating(df, num_participants):\n",
        "        partitions = [[] for _ in range(num_participants)]\n",
        "        num_features = df.shape[1]\n",
        "\n",
        "        for i, feature in enumerate(df.columns):\n",
        "            participant = i % num_participants\n",
        "            partitions[participant].append(feature)\n",
        "\n",
        "        partitioned_dfs = [pd.get_dummies(df[features]) for features in partitions]\n",
        "        return partitioned_dfs\n",
        "\n",
        "\n",
        "    def align_train_test_partitions(train_partitions, test_partitions):\n",
        "        for i in range(len(train_partitions)):\n",
        "            train_partitions[i] = pd.get_dummies(train_partitions[i], drop_first=True)\n",
        "            test_partitions[i] = pd.get_dummies(test_partitions[i], drop_first=True)\n",
        "            train_partitions[i], test_partitions[i] = train_partitions[i].align(test_partitions[i], join='left', axis=1, fill_value=0)\n",
        "        return train_partitions, test_partitions\n",
        "\n",
        "    def get_partitions_and_label():\n",
        "        df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "        processed_df = df.dropna(subset=[\"Embarked\", \"Fare\"]).copy()\n",
        "        processed_df = _create_features(processed_df)\n",
        "        labels = processed_df[\"Survived\"].values\n",
        "\n",
        "        train_df, test_df, y_train, y_test = train_test_split(processed_df, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "        train_partitions = vertical_partition_rotating(train_df.drop(columns=[\"Survived\"]), num_participants)\n",
        "        test_partitions = vertical_partition_rotating(test_df.drop(columns=[\"Survived\"]), num_participants)\n",
        "\n",
        "        for i in range(len(train_partitions)):\n",
        "            train_partitions[i]['Survived'] = y_train\n",
        "            test_partitions[i]['Survived'] = y_test\n",
        "\n",
        "        train_partitions, test_partitions = align_train_test_partitions(train_partitions, test_partitions)\n",
        "\n",
        "        return train_partitions, test_partitions, y_train, y_test\n",
        "\n",
        "    train_partitions, test_partitions, y_train, y_test = get_partitions_and_label()\n",
        "\n",
        "    def create_tensor_datasets(partitions):\n",
        "        tensor_partitions = []\n",
        "        for partition in partitions:\n",
        "            partition = partition.apply(pd.to_numeric, errors='coerce')\n",
        "            partition = partition.fillna(0)\n",
        "\n",
        "            for col in partition.select_dtypes(include=['bool']).columns:\n",
        "                partition[col] = partition[col].astype(int)\n",
        "\n",
        "            features = partition.drop(columns=[\"Survived\"]).values\n",
        "            labels = partition[\"Survived\"].values.astype(np.int64)\n",
        "\n",
        "            tensor_partition = TensorDataset(torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long))\n",
        "            tensor_partitions.append(tensor_partition)\n",
        "        return tensor_partitions\n",
        "\n",
        "    train_tensor_partitions = create_tensor_datasets(train_partitions)\n",
        "    test_tensor_partitions = create_tensor_datasets(test_partitions)\n",
        "\n",
        "    return train_tensor_partitions, test_tensor_partitions, train_partitions, y_train, y_test\n",
        "\n",
        "\n",
        "# New Model Class\n",
        "class GlobalModel(nn.Module):\n",
        "    def __init__(self, input_sizes, hidden_sizes, output_size):\n",
        "        super(GlobalModel, self).__init__()\n",
        "        self.segments = nn.ModuleList()\n",
        "        for input_size, hidden_size in zip(input_sizes, hidden_sizes):\n",
        "            layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
        "            layers.append(nn.Linear(hidden_size, output_size))  # Each participant has its own output layer\n",
        "            self.segments.append(nn.Sequential(*layers))\n",
        "\n",
        "    def forward(self, x, active_segments):\n",
        "        segment_outputs = []\n",
        "        hidden_outputs = []  # Store hidden layer outputs for sharing\n",
        "        start_index = 0\n",
        "        for i, segment in enumerate(self.segments):\n",
        "            end_index = start_index + input_sizes[i]\n",
        "            if i in active_segments:\n",
        "                segment_input = x[:, start_index:end_index]\n",
        "                hidden_output = segment[:2](segment_input)  # Pass through hidden layer\n",
        "                final_output = segment[2](hidden_output)    # Get final output after applying last layers\n",
        "                hidden_outputs.append(hidden_output)\n",
        "                segment_outputs.append(final_output)\n",
        "            else:\n",
        "                hidden_outputs.append(torch.zeros(x.size(0), hidden_sizes[i], device=x.device))\n",
        "                segment_outputs.append(torch.zeros(x.size(0), output_size, device=x.device))\n",
        "            start_index = end_index\n",
        "        combined_output = torch.mean(torch.stack(segment_outputs), dim=0)  # Average the outputs from all participants\n",
        "        return combined_output, hidden_outputs\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training function\n",
        "def train(model, device, train_loader, optimizer, epoch, input_sizes, participant_id, hidden_outputs_shared):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        data = data.view(data.size(0), -1)  # Flatten input\n",
        "        padded_data = torch.zeros(data.size(0), sum(input_sizes)).to(device)\n",
        "        start_index = sum(input_sizes[:participant_id])\n",
        "        end_index = start_index + input_sizes[participant_id]\n",
        "        padded_data[:, start_index:end_index] = data\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden_outputs = model(padded_data, active_segments=[participant_id])\n",
        "\n",
        "\n",
        "        if hidden_outputs_shared is not None:\n",
        "            for layer_id, hidden_output in enumerate(hidden_outputs):\n",
        "                if layer_id != participant_id:\n",
        "                    hidden_outputs[layer_id] = hidden_outputs_shared[layer_id]\n",
        "\n",
        "        loss = nn.CrossEntropyLoss(weight=class_weights)(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        hidden_outputs_shared[:] = hidden_outputs  # Update shared hidden outputs\n",
        "\n",
        "# Gradients exchange and hidden layer outputs sahring\n",
        "def selective_exchange_gradients_and_hidden(models, input_sizes, hidden_sizes):\n",
        "    num_models = len(models)\n",
        "    param_indices = [0]\n",
        "    cumulative_index = 0\n",
        "    for i in range(len(hidden_sizes)):\n",
        "        cumulative_index += (input_sizes[i] * hidden_sizes[i]) + hidden_sizes[i]\n",
        "        param_indices.append(cumulative_index)\n",
        "\n",
        "    for seg in range(len(hidden_sizes)):\n",
        "        start = param_indices[seg]\n",
        "        end = param_indices[seg + 1]\n",
        "        for param_idx in range(start, end):\n",
        "            grads = []\n",
        "            for model in models:\n",
        "                model_params = list(model.parameters())\n",
        "                if param_idx < len(model_params) and model_params[param_idx].grad is not None:\n",
        "                    grads.append(model_params[param_idx].grad)\n",
        "            if grads:\n",
        "                avg_grad = torch.stack(grads).mean(dim=0)\n",
        "                for model in models:\n",
        "                    model_params = list(model.parameters())\n",
        "                    if param_idx < len(model_params):\n",
        "                        model_params[param_idx].grad = avg_grad.clone()\n",
        "\n",
        "    # Hidden output exchange\n",
        "    hidden_outputs_shared = [None for _ in hidden_sizes]\n",
        "    for model in models:\n",
        "        _, hidden_outputs = model.forward(torch.zeros(1, sum(input_sizes)), active_segments=list(range(len(hidden_sizes))))\n",
        "        for layer_id, hidden_output in enumerate(hidden_outputs):\n",
        "            if hidden_outputs_shared[layer_id] is None:\n",
        "                hidden_outputs_shared[layer_id] = hidden_output.clone()\n",
        "            else:\n",
        "                hidden_outputs_shared[layer_id] += hidden_output.clone()\n",
        "        for layer_id in range(len(hidden_outputs_shared)):\n",
        "            hidden_outputs_shared[layer_id] /= num_models\n",
        "    return hidden_outputs_shared\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(models, device, test_loaders, input_sizes):\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in zip(*test_loaders):\n",
        "            data_list = []\n",
        "            target_list = []\n",
        "            for participant_id, (data, target) in enumerate(batch_data):\n",
        "                data_list.append(data)\n",
        "                target_list.append(target)\n",
        "\n",
        "            target = target_list[0].to(device)\n",
        "            for t in target_list:\n",
        "                assert torch.equal(t, target), \"Targets are not consistent across participants\"\n",
        "\n",
        "            data_combined = torch.cat(data_list, dim=1).to(device)\n",
        "\n",
        "            padded_data = torch.zeros(data_combined.size(0), sum(input_sizes)).to(device)\n",
        "            start_index = 0\n",
        "            for participant_id in range(len(input_sizes)):\n",
        "                end_index = start_index + input_sizes[participant_id]\n",
        "                padded_data[:, start_index:end_index] = data_combined[:, start_index:end_index]\n",
        "                start_index = end_index\n",
        "\n",
        "            outputs = torch.zeros(data_combined.size(0), 2, device=device)\n",
        "            for model in models:\n",
        "                output, _ = model(padded_data, active_segments=list(range(len(model.segments))))\n",
        "                outputs += output\n",
        "            outputs /= len(models)\n",
        "\n",
        "            pred = outputs.argmax(dim=1, keepdim=True)\n",
        "            all_preds.extend(pred.view(-1).cpu().numpy())\n",
        "            all_targets.extend(target.view(-1).cpu().numpy())\n",
        "\n",
        "    accuracy = np.mean(np.array(all_preds) == np.array(all_targets))\n",
        "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
        "    return accuracy, f1\n",
        "\n",
        "# Running the experiment for multiple seeds\n",
        "federated_rounds = 1000\n",
        "epochs_per_round = 1\n",
        "hidden_size = 10  # Single hidden layer\n",
        "\n",
        "seeds = [42, 55, 77, 101, 123]\n",
        "final_accuracies = []\n",
        "final_f1_scores = []\n",
        "\n",
        "for seed in seeds:\n",
        "    set_random_seed(seed)\n",
        "    print(f\"\\nRunning VFL with seed {seed}...\")\n",
        "\n",
        "    avg_accuracy = 0\n",
        "    avg_f1 = 0\n",
        "\n",
        "    for num_participants in range(2, 10):\n",
        "        print(f\"Running VFL with {num_participants} participants...\")\n",
        "\n",
        "        train_tensor_partitions, test_tensor_partitions, feature_partitions, y_train, y_test = load_titanic_data(num_participants)\n",
        "\n",
        "        input_sizes = [partition.shape[1] - 1 for partition in feature_partitions]\n",
        "        hidden_sizes = [10] * num_participants\n",
        "        output_size = 2\n",
        "\n",
        "        models = [GlobalModel(input_sizes, hidden_sizes, output_size).to(device) for _ in range(num_participants)]\n",
        "        optimizers = [optim.Adam(model.parameters(), lr=0.01) for model in models]\n",
        "\n",
        "        class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "        best_accuracy = 0\n",
        "        best_f1 = 0\n",
        "\n",
        "        hidden_outputs_shared = [None for _ in hidden_sizes]\n",
        "\n",
        "        for federated_round in range(federated_rounds):\n",
        "            #print(f\"Federated Round {federated_round + 1}/{federated_rounds}\")\n",
        "            for participant_id in range(num_participants):\n",
        "                train_loader = DataLoader(train_tensor_partitions[participant_id], batch_size=64, shuffle=True)\n",
        "                for epoch in range(1, epochs_per_round + 1):\n",
        "                    train(models[participant_id], device, train_loader, optimizers[participant_id], epoch, input_sizes, participant_id, hidden_outputs_shared)\n",
        "\n",
        "            hidden_outputs_shared = selective_exchange_gradients_and_hidden(models, input_sizes, hidden_sizes)\n",
        "\n",
        "            test_loaders = [DataLoader(test_tensor_partitions[i], batch_size=32, shuffle=False) for i in range(num_participants)]\n",
        "            accuracy, f1 = evaluate(models, device, test_loaders, input_sizes)\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "\n",
        "        print(f'Best Accuracy with {num_participants} participants: {best_accuracy:.2f}%')\n",
        "        print(f'Best F1-Score with {num_participants} participants: {best_f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiS-WeihSKaP",
        "outputId": "0c4409a4-6110-4a8b-eefd-d2f43e01280a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running VFL with seed 42...\n",
            "Running VFL with 2 participants...\n",
            "Best Accuracy with 2 participants: 0.76%\n",
            "Best F1-Score with 2 participants: 0.7604\n",
            "Running VFL with 3 participants...\n",
            "Best Accuracy with 3 participants: 0.82%\n",
            "Best F1-Score with 3 participants: 0.8142\n",
            "Running VFL with 4 participants...\n",
            "Best Accuracy with 4 participants: 0.81%\n",
            "Best F1-Score with 4 participants: 0.7988\n",
            "Running VFL with 5 participants...\n",
            "Best Accuracy with 5 participants: 0.83%\n",
            "Best F1-Score with 5 participants: 0.8211\n",
            "Running VFL with 6 participants...\n",
            "Best Accuracy with 6 participants: 0.81%\n",
            "Best F1-Score with 6 participants: 0.8088\n",
            "Running VFL with 7 participants...\n",
            "Best Accuracy with 7 participants: 0.81%\n",
            "Best F1-Score with 7 participants: 0.8034\n",
            "Running VFL with 8 participants...\n",
            "Best Accuracy with 8 participants: 0.70%\n",
            "Best F1-Score with 8 participants: 0.6966\n",
            "Running VFL with 9 participants...\n",
            "Best Accuracy with 9 participants: 0.75%\n",
            "Best F1-Score with 9 participants: 0.7465\n",
            "\n",
            "Running VFL with seed 55...\n",
            "Running VFL with 2 participants...\n",
            "Best Accuracy with 2 participants: 0.75%\n",
            "Best F1-Score with 2 participants: 0.7458\n",
            "Running VFL with 3 participants...\n",
            "Best Accuracy with 3 participants: 0.75%\n",
            "Best F1-Score with 3 participants: 0.7520\n",
            "Running VFL with 4 participants...\n",
            "Best Accuracy with 4 participants: 0.81%\n",
            "Best F1-Score with 4 participants: 0.8034\n",
            "Running VFL with 5 participants...\n",
            "Best Accuracy with 5 participants: 0.81%\n",
            "Best F1-Score with 5 participants: 0.8088\n",
            "Running VFL with 6 participants...\n",
            "Best Accuracy with 6 participants: 0.68%\n",
            "Best F1-Score with 6 participants: 0.6797\n",
            "Running VFL with 7 participants...\n",
            "Best Accuracy with 7 participants: 0.78%\n",
            "Best F1-Score with 7 participants: 0.7730\n",
            "Running VFL with 8 participants...\n",
            "Best Accuracy with 8 participants: 0.75%\n",
            "Best F1-Score with 8 participants: 0.7465\n",
            "Running VFL with 9 participants...\n",
            "Best Accuracy with 9 participants: 0.70%\n",
            "Best F1-Score with 9 participants: 0.6966\n",
            "\n",
            "Running VFL with seed 77...\n",
            "Running VFL with 2 participants...\n",
            "Best Accuracy with 2 participants: 0.81%\n",
            "Best F1-Score with 2 participants: 0.8062\n",
            "Running VFL with 3 participants...\n",
            "Best Accuracy with 3 participants: 0.81%\n",
            "Best F1-Score with 3 participants: 0.8102\n",
            "Running VFL with 4 participants...\n",
            "Best Accuracy with 4 participants: 0.82%\n",
            "Best F1-Score with 4 participants: 0.8126\n",
            "Running VFL with 5 participants...\n",
            "Best Accuracy with 5 participants: 0.81%\n",
            "Best F1-Score with 5 participants: 0.8072\n",
            "Running VFL with 6 participants...\n",
            "Best Accuracy with 6 participants: 0.67%\n",
            "Best F1-Score with 6 participants: 0.6099\n",
            "Running VFL with 7 participants...\n",
            "Best Accuracy with 7 participants: 0.66%\n",
            "Best F1-Score with 7 participants: 0.5621\n",
            "Running VFL with 8 participants...\n",
            "Best Accuracy with 8 participants: 0.79%\n",
            "Best F1-Score with 8 participants: 0.7880\n",
            "Running VFL with 9 participants...\n",
            "Best Accuracy with 9 participants: 0.81%\n",
            "Best F1-Score with 9 participants: 0.7977\n",
            "\n",
            "Running VFL with seed 101...\n",
            "Running VFL with 2 participants...\n",
            "Best Accuracy with 2 participants: 0.79%\n",
            "Best F1-Score with 2 participants: 0.7826\n",
            "Running VFL with 3 participants...\n",
            "Best Accuracy with 3 participants: 0.74%\n",
            "Best F1-Score with 3 participants: 0.7132\n",
            "Running VFL with 4 participants...\n",
            "Best Accuracy with 5 participants: 0.82%\n",
            "Best F1-Score with 5 participants: 0.8142\n",
            "Running VFL with 6 participants...\n",
            "Best Accuracy with 6 participants: 0.70%\n",
            "Best F1-Score with 6 participants: 0.7022\n",
            "Running VFL with 7 participants...\n",
            "Best Accuracy with 7 participants: 0.75%\n",
            "Best F1-Score with 7 participants: 0.7421\n",
            "Running VFL with 8 participants...\n",
            "Best Accuracy with 8 participants: 0.69%\n",
            "Best F1-Score with 8 participants: 0.6199\n",
            "Running VFL with 9 participants...\n",
            "Best Accuracy with 9 participants: 0.78%\n",
            "Best F1-Score with 9 participants: 0.7735\n",
            "\n",
            "Running VFL with seed 123...\n",
            "Running VFL with 2 participants...\n",
            "Best Accuracy with 2 participants: 0.76%\n",
            "Best F1-Score with 2 participants: 0.7562\n",
            "Running VFL with 3 participants...\n",
            "Best Accuracy with 3 participants: 0.75%\n",
            "Best F1-Score with 3 participants: 0.7205\n",
            "Running VFL with 4 participants...\n",
            "Best Accuracy with 4 participants: 0.70%\n",
            "Best F1-Score with 4 participants: 0.7022\n",
            "Running VFL with 5 participants...\n",
            "Best Accuracy with 5 participants: 0.71%\n",
            "Best F1-Score with 5 participants: 0.6564\n",
            "Running VFL with 6 participants...\n",
            "Best Accuracy with 6 participants: 0.77%\n",
            "Best F1-Score with 6 participants: 0.7664\n",
            "Running VFL with 7 participants...\n",
            "Best Accuracy with 7 participants: 0.69%\n",
            "Best F1-Score with 7 participants: 0.6910\n",
            "Running VFL with 8 participants...\n",
            "Best Accuracy with 8 participants: 0.75%\n",
            "Best F1-Score with 8 participants: 0.7465\n",
            "Running VFL with 9 participants...\n",
            "Best Accuracy with 9 participants: 0.80%\n",
            "Best F1-Score with 9 participants: 0.7881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Titanic- No federation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# For random seeds-\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Preprocessing Titanic dataset (same as before)\n",
        "def load_titanic_data(num_participants):\n",
        "    def _bin_age(age_series):\n",
        "        bins = [-np.inf, 10, 40, np.inf]\n",
        "        labels = [\"Child\", \"Adult\", \"Elderly\"]\n",
        "        return pd.cut(age_series, bins=bins, labels=labels, right=True).astype(str).replace(\"nan\", \"Unknown\")\n",
        "\n",
        "    def _extract_title(name_series):\n",
        "        titles = name_series.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
        "        rare_titles = {\n",
        "            \"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"\n",
        "        }\n",
        "        titles = titles.replace(list(rare_titles), \"Rare\")\n",
        "        titles = titles.replace({\"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\"})\n",
        "        return titles\n",
        "\n",
        "    def _create_features(df):\n",
        "        df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\")\n",
        "        df[\"Age\"] = _bin_age(df[\"Age\"])\n",
        "        df[\"Cabin\"] = df[\"Cabin\"].str[0].fillna(\"Unknown\")\n",
        "        df[\"Title\"] = _extract_title(df[\"Name\"])\n",
        "        df.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"], inplace=True)\n",
        "        return df\n",
        "\n",
        "    def vertical_partition_rotating(df, num_participants):\n",
        "        partitions = [[] for _ in range(num_participants)]\n",
        "        num_features = df.shape[1]\n",
        "\n",
        "        for i, feature in enumerate(df.columns):\n",
        "            participant = i % num_participants\n",
        "            partitions[participant].append(feature)\n",
        "\n",
        "        partitioned_dfs = [pd.get_dummies(df[features]) for features in partitions]\n",
        "        return partitioned_dfs\n",
        "\n",
        "\n",
        "    def align_train_test_partitions(train_partitions, test_partitions):\n",
        "        for i in range(len(train_partitions)):\n",
        "            train_partitions[i] = pd.get_dummies(train_partitions[i], drop_first=True)\n",
        "            test_partitions[i] = pd.get_dummies(test_partitions[i], drop_first=True)\n",
        "            train_partitions[i], test_partitions[i] = train_partitions[i].align(test_partitions[i], join='left', axis=1, fill_value=0)\n",
        "        return train_partitions, test_partitions\n",
        "\n",
        "    def get_partitions_and_label():\n",
        "        df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "        processed_df = df.dropna(subset=[\"Embarked\", \"Fare\"]).copy()\n",
        "        processed_df = _create_features(processed_df)\n",
        "        labels = processed_df[\"Survived\"].values\n",
        "\n",
        "        train_df, test_df, y_train, y_test = train_test_split(processed_df, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "        train_partitions = vertical_partition_rotating(train_df.drop(columns=[\"Survived\"]), num_participants)\n",
        "        test_partitions = vertical_partition_rotating(test_df.drop(columns=[\"Survived\"]), num_participants)\n",
        "\n",
        "        for i in range(len(train_partitions)):\n",
        "            train_partitions[i]['Survived'] = y_train\n",
        "            test_partitions[i]['Survived'] = y_test\n",
        "\n",
        "        train_partitions, test_partitions = align_train_test_partitions(train_partitions, test_partitions)\n",
        "\n",
        "        return train_partitions, test_partitions, y_train, y_test\n",
        "\n",
        "    train_partitions, test_partitions, y_train, y_test = get_partitions_and_label()\n",
        "\n",
        "    def create_tensor_datasets(partitions):\n",
        "        tensor_partitions = []\n",
        "        for partition in partitions:\n",
        "            partition = partition.apply(pd.to_numeric, errors='coerce')\n",
        "            partition = partition.fillna(0)\n",
        "\n",
        "            for col in partition.select_dtypes(include=['bool']).columns:\n",
        "                partition[col] = partition[col].astype(int)\n",
        "\n",
        "            features = partition.drop(columns=[\"Survived\"]).values\n",
        "            labels = partition[\"Survived\"].values.astype(np.int64)\n",
        "\n",
        "            tensor_partition = TensorDataset(torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.long))\n",
        "            tensor_partitions.append(tensor_partition)\n",
        "        return tensor_partitions\n",
        "\n",
        "    train_tensor_partitions = create_tensor_datasets(train_partitions)\n",
        "    test_tensor_partitions = create_tensor_datasets(test_partitions)\n",
        "\n",
        "    return train_tensor_partitions, test_tensor_partitions, train_partitions, y_train, y_test\n",
        "\n",
        "\n",
        "class GlobalModel(nn.Module):\n",
        "    def __init__(self, input_sizes, hidden_sizes, output_size):\n",
        "        super(GlobalModel, self).__init__()\n",
        "        self.segments = nn.ModuleList()\n",
        "        for input_size, hidden_size in zip(input_sizes, hidden_sizes):\n",
        "            layers = [nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, output_size)]\n",
        "            self.segments.append(nn.Sequential(*layers))\n",
        "\n",
        "    def forward(self, x, active_segments):\n",
        "        segment_outputs = []\n",
        "        start_index = 0\n",
        "        for i, segment in enumerate(self.segments):\n",
        "            end_index = start_index + input_sizes[i]\n",
        "            if i in active_segments:\n",
        "                segment_input = x[:, start_index:end_index]\n",
        "                segment_output = segment(segment_input)\n",
        "                segment_outputs.append(segment_output)\n",
        "            else:\n",
        "                segment_outputs.append(torch.zeros(x.size(0), output_size, device=x.device))\n",
        "            start_index = end_index\n",
        "        combined_output = torch.mean(torch.stack(segment_outputs), dim=0)\n",
        "        return combined_output  # Only return the combined output\n",
        "\n",
        "# Training function (here training is independent with zero-padding)\n",
        "def train_independent(model, device, train_loader, optimizer, epoch, input_sizes, participant_id):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        padded_data = torch.zeros(data.size(0), sum(input_sizes)).to(device)\n",
        "        start_index = sum(input_sizes[:participant_id])\n",
        "        end_index = start_index + input_sizes[participant_id]\n",
        "        padded_data[:, start_index:end_index] = data\n",
        "        optimizer.zero_grad()\n",
        "        output = model(padded_data, active_segments=[participant_id])\n",
        "        loss = nn.CrossEntropyLoss()(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Evaluation function (evaluating independently for each participant)\n",
        "def evaluate_independent(model, device, test_loader, input_sizes, participant_id):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            padded_data = torch.zeros(data.size(0), sum(input_sizes)).to(device)  # Create zero-padded input\n",
        "            start_index = sum(input_sizes[:participant_id])\n",
        "            end_index = start_index + input_sizes[participant_id]\n",
        "            padded_data[:, start_index:end_index] = data  # Insert the active participant's data\n",
        "            output = model(padded_data, active_segments=[participant_id])  # Only return the output\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            all_preds.extend(pred.view(-1).cpu().numpy())\n",
        "            all_targets.extend(target.view(-1).cpu().numpy())\n",
        "\n",
        "    accuracy = np.mean(np.array(all_preds) == np.array(all_targets))\n",
        "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
        "    return accuracy, f1\n",
        "\n",
        "# Running the experiment for multiple seeds (no collaboration)\n",
        "federated_rounds = 1000\n",
        "epochs_per_round = 1\n",
        "hidden_size = 10  # Single hidden layer\n",
        "\n",
        "seeds = [1,2,3,4,5]  # List of 5 different seeds\n",
        "final_accuracies = []\n",
        "final_f1_scores = []\n",
        "\n",
        "for seed in seeds:\n",
        "    set_random_seed(seed)\n",
        "    print(f\"\\nRunning independent training, seed {seed}...\")\n",
        "\n",
        "    avg_accuracy = 0\n",
        "    avg_f1 = 0\n",
        "\n",
        "    for num_participants in range(2, 10):\n",
        "        print(f\"Running with {num_participants} independent participants...\")\n",
        "\n",
        "        train_tensor_partitions, test_tensor_partitions, feature_partitions, y_train, y_test = load_titanic_data(num_participants)\n",
        "\n",
        "        input_sizes = [partition.shape[1] - 1 for partition in feature_partitions]  # Minus 1 for the 'Survived' label\n",
        "        output_size = 2\n",
        "\n",
        "\n",
        "        models = [GlobalModel(input_sizes, [hidden_size] * num_participants, output_size).to(device) for _ in range(num_participants)]\n",
        "        optimizers = [optim.Adam(model.parameters(), lr=0.01) for model in models]\n",
        "\n",
        "\n",
        "        for participant_id in range(num_participants):\n",
        "            print(f\"Training participant {participant_id + 1} independently...\")\n",
        "            train_loader = DataLoader(train_tensor_partitions[participant_id], batch_size=64, shuffle=True)\n",
        "            test_loader = DataLoader(test_tensor_partitions[participant_id], batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "            for epoch in range(1, epochs_per_round + 1):\n",
        "                train_independent(models[participant_id], device, train_loader, optimizers[participant_id], epoch, input_sizes, participant_id)\n",
        "\n",
        "            # Local model evaluation\n",
        "            accuracy, f1 = evaluate_independent(models[participant_id], device, test_loader, input_sizes, participant_id)\n",
        "            print(f'Participant {participant_id + 1}: Accuracy: {accuracy:.2f}, F1-Score: {f1:.4f}')\n",
        "            avg_accuracy += accuracy\n",
        "            avg_f1 += f1\n",
        "\n",
        "        avg_accuracy /= num_participants\n",
        "        avg_f1 /= num_participants\n",
        "\n"
      ],
      "metadata": {
        "id": "ksmYtmjFqUzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c1c40b-05fe-4a26-da39-320d5f8c2d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running independent training, seed 1...\n",
            "Running with 2 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.62, F1-Score: 0.4312\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.69, F1-Score: 0.6800\n",
            "Running with 3 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.67, F1-Score: 0.6258\n",
            "Running with 4 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.60, F1-Score: 0.5903\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 5 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.74, F1-Score: 0.6626\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3930\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.69, F1-Score: 0.6435\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 6 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.80, F1-Score: 0.7891\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.79, F1-Score: 0.7865\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3930\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.39, F1-Score: 0.2794\n",
            "Running with 7 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.62, F1-Score: 0.3954\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.78, F1-Score: 0.7757\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.34, F1-Score: 0.3370\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 8 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.80, F1-Score: 0.7891\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.67, F1-Score: 0.6185\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 8 independently...\n",
            "Participant 8: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 9 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.80, F1-Score: 0.7891\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.68, F1-Score: 0.5665\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 8 independently...\n",
            "Participant 8: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 9 independently...\n",
            "Participant 9: Accuracy: 0.78, F1-Score: 0.7757\n",
            "\n",
            "Running independent training, seed 2...\n",
            "Running with 2 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.65, F1-Score: 0.4780\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.66, F1-Score: 0.5784\n",
            "Running with 3 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.67, F1-Score: 0.5581\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.63, F1-Score: 0.6345\n",
            "Running with 4 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.75, F1-Score: 0.7012\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.6067\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 5 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.67, F1-Score: 0.6276\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.63, F1-Score: 0.4582\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 6 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.39, F1-Score: 0.2896\n",
            "Running with 7 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.78, F1-Score: 0.7757\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.39, F1-Score: 0.2896\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 8 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.39, F1-Score: 0.2896\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.39, F1-Score: 0.2794\n",
            "Training participant 8 independently...\n",
            "Participant 8: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 9 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.39, F1-Score: 0.2794\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.71, F1-Score: 0.6628\n",
            "Training participant 8 independently...\n",
            "Participant 8: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 9 independently...\n",
            "Participant 9: Accuracy: 0.61, F1-Score: 0.3798\n",
            "\n",
            "Running independent training, seed 3...\n",
            "Running with 2 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.63, F1-Score: 0.4479\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.69, F1-Score: 0.6435\n",
            "Running with 3 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.80, F1-Score: 0.7901\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.75, F1-Score: 0.7344\n",
            "Running with 4 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.73, F1-Score: 0.7030\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.62, F1-Score: 0.4518\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 5 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.39, F1-Score: 0.2896\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 6 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.67, F1-Score: 0.5622\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.66, F1-Score: 0.6129\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.39, F1-Score: 0.2896\n",
            "Running with 7 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.64, F1-Score: 0.5871\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.66, F1-Score: 0.6410\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 8 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.80, F1-Score: 0.7891\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.39, F1-Score: 0.2896\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 8 independently...\n",
            "Participant 8: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 9 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.68, F1-Score: 0.5961\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.39, F1-Score: 0.2794\n",
            "Training participant 8 independently...\n",
            "Participant 8: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 9 independently...\n",
            "Participant 9: Accuracy: 0.61, F1-Score: 0.3798\n",
            "\n",
            "Running independent training, seed 4...\n",
            "Running with 2 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.62, F1-Score: 0.3954\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.66, F1-Score: 0.6316\n",
            "Running with 3 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.52, F1-Score: 0.4929\n",
            "Running with 4 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.67, F1-Score: 0.5201\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.80, F1-Score: 0.7891\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 5 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.71, F1-Score: 0.6832\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 6 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.63, F1-Score: 0.4510\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.58, F1-Score: 0.5764\n",
            "Running with 7 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.72, F1-Score: 0.6743\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.62, F1-Score: 0.6168\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 8 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.39, F1-Score: 0.2896\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 8 independently...\n",
            "Participant 8: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 9 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 8 independently...\n",
            "Participant 8: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 9 independently...\n",
            "Participant 9: Accuracy: 0.66, F1-Score: 0.5880\n",
            "\n",
            "Running independent training, seed 5...\n",
            "Running with 2 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.65, F1-Score: 0.5051\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.39, F1-Score: 0.2866\n",
            "Running with 3 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.67, F1-Score: 0.5622\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.47, F1-Score: 0.4100\n",
            "Running with 4 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.39, F1-Score: 0.2896\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 5 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.69, F1-Score: 0.6420\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.69, F1-Score: 0.5707\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 6 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.62, F1-Score: 0.5475\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.45, F1-Score: 0.3936\n",
            "Running with 7 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.80, F1-Score: 0.7891\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.65, F1-Score: 0.5876\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 8 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.76, F1-Score: 0.7438\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.39, F1-Score: 0.2794\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 8 independently...\n",
            "Participant 8: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Running with 9 independent participants...\n",
            "Training participant 1 independently...\n",
            "Participant 1: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 2 independently...\n",
            "Participant 2: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 3 independently...\n",
            "Participant 3: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 4 independently...\n",
            "Participant 4: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 5 independently...\n",
            "Participant 5: Accuracy: 0.36, F1-Score: 0.3477\n",
            "Training participant 6 independently...\n",
            "Participant 6: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 7 independently...\n",
            "Participant 7: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 8 independently...\n",
            "Participant 8: Accuracy: 0.61, F1-Score: 0.3798\n",
            "Training participant 9 independently...\n",
            "Participant 9: Accuracy: 0.71, F1-Score: 0.6427\n"
          ]
        }
      ]
    }
  ]
}